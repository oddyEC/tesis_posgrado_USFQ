project,branch,bug_report
accumulo,bugs-dot-jar_ACCUMULO-1044_9396979b,"{'BugID': 'ACCUMULO-1044', 'Summary': 'bulk imported files showing up in metadata after bulk import fails', 'Description': 'Bulk import fails.  The file is moved to the failures directory.\n\nBut references in the !METADATA table remain.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1044_ea2f9856,"{'BugID': 'ACCUMULO-1044', 'Summary': 'bulk imported files showing up in metadata after bulk import fails', 'Description': 'Bulk import fails.  The file is moved to the failures directory.\n\nBut references in the !METADATA table remain.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1051_25cf3ccd,"{'BugID': 'ACCUMULO-1051', 'Summary': 'Authorizations has inconsistent serialization', 'Description': 'The same set of authorizations may not serialize to the same value each time, if specified in a different order when constructed (like new Authorizations(""a"", ""b"") and new Authorizations(""b"", ""a"")), because serialization reproducibility depends on the insert order in the underlying HashSet.\n\nSo, one could get the following to happen:\n{code:java}\ntrue == auths1.equals(auths2) && !auths1.serialize().equals(auths2.serialize());\n{code}'}"
accumulo,bugs-dot-jar_ACCUMULO-1120_474b2577,"{'BugID': 'ACCUMULO-1120', 'Summary': ""stop-all doesn't work: Error BAD_CREDENTIALS for user root"", 'Description': '{noformat}\n$ bin/accumulo admin stopAll\n2013-02-27 14:56:14,072 [util.Admin] ERROR: org.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid\norg.apache.accumulo.core.client.AccumuloSecurityException: Error BAD_CREDENTIALS for user root - Username or Password is Invalid\n\tat org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:119)\n\tat org.apache.accumulo.server.util.Admin.stopServer(Admin.java:107)\n\tat org.apache.accumulo.server.util.Admin.main(Admin.java:95)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.accumulo.start.Main$1.run(Main.java:97)\n\tat java.lang.Thread.run(Thread.java:662)\nCaused by: ThriftSecurityException(user:root, code:BAD_CREDENTIALS)\n\tat org.apache.accumulo.core.master.thrift.MasterClientService$shutdown_result$shutdown_resultStandardScheme.read(MasterClientService.java:8424)\n\tat org.apache.accumulo.core.master.thrift.MasterClientService$shutdown_result$shutdown_resultStandardScheme.read(MasterClientService.java:8410)\n\tat org.apache.accumulo.core.master.thrift.MasterClientService$shutdown_result.read(MasterClientService.java:8360)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\n\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.recv_shutdown(MasterClientService.java:312)\n\tat org.apache.accumulo.core.master.thrift.MasterClientService$Client.shutdown(MasterClientService.java:297)\n\tat org.apache.accumulo.server.util.Admin$1.execute(Admin.java:110)\n\tat org.apache.accumulo.server.util.Admin$1.execute(Admin.java:107)\n\tat org.apache.accumulo.core.client.impl.MasterClient.execute(MasterClient.java:113)\n\t... 8 more\n\n{noformat}\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1183_742960f1,"{'BugID': 'ACCUMULO-1183', 'Summary': 'ProxyServer does not set column information on BatchScanner', 'Description': 'The createScanner method uses the options from the thrift request to call fetchColumn() and fetchColumnFamily(). The createBatchScanner should be doing have the same feature, though the statements are absent from the code.'}"
accumulo,bugs-dot-jar_ACCUMULO-1183_cfbf5999,"{'BugID': 'ACCUMULO-1183', 'Summary': 'ProxyServer does not set column information on BatchScanner', 'Description': 'The createScanner method uses the options from the thrift request to call fetchColumn() and fetchColumnFamily(). The createBatchScanner should be doing have the same feature, though the statements are absent from the code.'}"
accumulo,bugs-dot-jar_ACCUMULO-1190_e29dc4f5,"{'BugID': 'ACCUMULO-1190', 'Summary': 'The update() method on the ProxyServer should throw a MutationsRejectedException', 'Description': None}"
accumulo,bugs-dot-jar_ACCUMULO-1192_9476b877,"{'BugID': 'ACCUMULO-1192', 'Summary': '""du"" on a table without files does not report', 'Description': '{noformat}\nshell> createtable t\nshell> du t\nshell>\n{noformat}\n\nexpected:\n\n{noformat}\nshell> du t\n             0 t\nshell>\n{noformat}\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1192_c489d866,"{'BugID': 'ACCUMULO-1192', 'Summary': '""du"" on a table without files does not report', 'Description': '{noformat}\nshell> createtable t\nshell> du t\nshell>\n{noformat}\n\nexpected:\n\n{noformat}\nshell> du t\n             0 t\nshell>\n{noformat}\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1199_813109d7,"{'BugID': 'ACCUMULO-1199', 'Summary': 'Verify all methods in the ProxyService that take table names actually throw TableNotFoundException when the table is missing.', 'Description': None}"
accumulo,bugs-dot-jar_ACCUMULO-1312_d9ab8449,"{'BugID': 'ACCUMULO-1312', 'Summary': ""Don't cache credentials in client-side Connector"", 'Description': 'AuthenticationToken objects are Destroyable. However, this cannot be exercised properly in the client code, because the Connector immediately serializes the credentials and stores them as long as the Connector lives.\n\nIt should be possible to destroy a token after creating a Connector, and thereby forcing any further RPC calls initiated by that Connector to fail to authenticate. This means that serialization on the client side to a TCredentials object needs to occur just before the RPC call.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1348_6ff92b12,"{'BugID': 'ACCUMULO-1348', 'Summary': ""Accumulo Shell does not respect 'exit' when executing file"", 'Description': 'If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.\n\nTo recreate:\n{noformat}\n[mike@home ~] cat bug.accumulo\nexit\nscan -np -t !METADATA\n[mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo\n{noformat}\n\nExpected output: None\nActual output: A full scan of the !METADATA'}"
accumulo,bugs-dot-jar_ACCUMULO-1348_ef0f6ddc,"{'BugID': 'ACCUMULO-1348', 'Summary': ""Accumulo Shell does not respect 'exit' when executing file"", 'Description': 'If there is an {{exit}} statement in the file given via {{accumulo shell -f file}}, the execution seems to skip it and go on to the next command instead of terminating.\n\nTo recreate:\n{noformat}\n[mike@home ~] cat bug.accumulo\nexit\nscan -np -t !METADATA\n[mike@home ~] bin/accumulo shell -f /home/mike/bug.accumulo\n{noformat}\n\nExpected output: None\nActual output: A full scan of the !METADATA'}"
accumulo,bugs-dot-jar_ACCUMULO-1358_4d10c92f,"{'BugID': 'ACCUMULO-1358', 'Summary': ""Shell's setiter is not informative when using a bad class name"", 'Description': ""In the shell, I did setiter using a class that wasn't found. Rather then a message about it not being found, I just get told that I have an invalid argument. Even turning on debug, I had to use the stack trace to figure out why it was erroring.""}"
accumulo,bugs-dot-jar_ACCUMULO-1358_6c565dfb,"{'BugID': 'ACCUMULO-1358', 'Summary': ""Shell's setiter is not informative when using a bad class name"", 'Description': ""In the shell, I did setiter using a class that wasn't found. Rather then a message about it not being found, I just get told that I have an invalid argument. Even turning on debug, I had to use the stack trace to figure out why it was erroring.""}"
accumulo,bugs-dot-jar_ACCUMULO-1505_994df698,"{'BugID': 'ACCUMULO-1505', 'Summary': ""MockTable's addMutation does not check for empty mutation"", 'Description': 'When calling addMutation or addMutations on a MockBatchWriter, the updates stored in the mutation are iterated over then committed in the MockTable class. \n\nWhen this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl), however, the mutation size is first checked and if the mutation size is 0, an IllegalArgumentException is thrown.\n\nIn practice, if you have code that tries to submit an empty mutation to a BatchWriter, it will fail and throw an exception in the real world, but this will not be caught in tests against MockAccumulo.'}"
accumulo,bugs-dot-jar_ACCUMULO-1505_b082fc1e,"{'BugID': 'ACCUMULO-1505', 'Summary': ""MockTable's addMutation does not check for empty mutation"", 'Description': 'When calling addMutation or addMutations on a MockBatchWriter, the updates stored in the mutation are iterated over then committed in the MockTable class. \n\nWhen this occurs in the TabletServerBatchWriter (eventually called from the BatchWriterImpl), however, the mutation size is first checked and if the mutation size is 0, an IllegalArgumentException is thrown.\n\nIn practice, if you have code that tries to submit an empty mutation to a BatchWriter, it will fail and throw an exception in the real world, but this will not be caught in tests against MockAccumulo.'}"
accumulo,bugs-dot-jar_ACCUMULO-1514_fb25913c,"{'BugID': 'ACCUMULO-1514', 'Summary': 'AccumuloVFSClassloader incorrectly treats folders as folders of jar files', 'Description': 'Specifying a directory of classes is incorrectly interpreted as a directory of jars in the general.dynamic.classpaths configuration property.\n\nExample: adding a path such as *_$ACCUMULO_HOME/core/target/classes_* gets incorrectly interpreted as *_$ACCUMULO_HOME/core/target/classes/\\*_* and evaluates to *_$ACCUMULO_HOME/core/target/classes/org_* and *_$ACCUMULO_HOME/core/target/classes/META-INF_*, but *NOT* to *_$ACCUMULO_HOME/core/target/classes_* as expected.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-1518_dc95cb69,"{'BugID': 'ACCUMULO-1518', 'Summary': 'FileOperations expects RFile filenames to contain only 1 dot.', 'Description': 'If I attempt to create or read an RFile that contains more than 1 dot in the filename, FileOperations throws an IllegalArgumentException(""File name "" + name + "" has no extension"").\nPlease allow creation/import of RFiles that have more than 1 dot in the filename.'}"
accumulo,bugs-dot-jar_ACCUMULO-1518_df4b1985,"{'BugID': 'ACCUMULO-1518', 'Summary': 'FileOperations expects RFile filenames to contain only 1 dot.', 'Description': 'If I attempt to create or read an RFile that contains more than 1 dot in the filename, FileOperations throws an IllegalArgumentException(""File name "" + name + "" has no extension"").\nPlease allow creation/import of RFiles that have more than 1 dot in the filename.'}"
accumulo,bugs-dot-jar_ACCUMULO-151_b007b22e,"{'BugID': 'ACCUMULO-151', 'Summary': 'Combiner default behavior is dangerous', 'Description': 'Currently if the users does not give the combiner any columns to work against, it will work against all columns.  This is dangerous, if a user accidentally forgets to specify columns then their data could be unintentionally corrupted.  Something different needs to be done.  \n\nAlso classes that extend combiner should call super.validateOptions(). '}"
accumulo,bugs-dot-jar_ACCUMULO-1544_0cf2ff72,"{'BugID': 'ACCUMULO-1544', 'Summary': 'Remove username from initialization', 'Description': ""This is an artifact from a brief transition area during the 1.5 development. We have a flag for the user to set what the root username is, except it's never used. We should remove both the variable and the flag for it.""}"
accumulo,bugs-dot-jar_ACCUMULO-1661_13eb19c2,"{'BugID': 'ACCUMULO-1661', 'Summary': 'AccumuloInputFormat cannot fetch empty column family', 'Description': 'The following fails:\n{code:java}\nJob job = new Job();\nHashSet<Pair<Text,Text>> cols = new HashSet<Pair<Text,Text>>();\ncols.add(new Pair<Text,Text>(new Text(""""), null));\nAccumuloInputFormat.fetchColumns(job, cols);\nSet<Pair<Text,Text>> setCols = AccumuloInputFormat.getFetchedColumns(job);\nassertEquals(cols.size(), setCols.size());\n{code}'}"
accumulo,bugs-dot-jar_ACCUMULO-1730_872b6db3,"{'BugID': 'ACCUMULO-1730', 'Summary': 'ColumnVisibility parse tree nodes do not have correct location offsets for AND and OR nodes', 'Description': 'Trying to do some transformations on visibility strings and running into issues working with the parse tree:\n\nClojure 1.5.1\nuser=> (import [org.apache.accumulo.core.security ColumnVisibility])\norg.apache.accumulo.core.security.ColumnVisibility\nuser=> (def vis (ColumnVisibility. ""(W)|(U|V)""))\n#\'user/vis\nuser=> (.getTermStart (first (.getChildren (.getParseTree vis))))\n1\nuser=> (.getTermEnd (first (.getChildren (.getParseTree vis))))\n2\nuser=> (.getTermStart (second (.getChildren (.getParseTree vis))))\n0\nuser=> (.getTermEnd (second (.getChildren (.getParseTree vis))))\n8\n\nShouldn\'t those last two be 5 and 8?'}"
accumulo,bugs-dot-jar_ACCUMULO-1732_941e3cb1,"{'BugID': 'ACCUMULO-1732', 'Summary': 'Resolve table name to table id once in Accumulo input format', 'Description': 'AccumuloInputFormat (and I suspect AccumuloOutputFormat) sends the table name to each mapper.  The mapper uses this table name to create a scanner.  In the case of the following events a map reduce job could read from two different table ids.   \n\n # start M/R job reading table A\n # rename table A (tableId=1) to table C\n # rename table B (tableId=2) to table A\n\nIf the input format passed table id 1 to the mappers, then the renames would not cause a problem.'}"
accumulo,bugs-dot-jar_ACCUMULO-178_2f0643a9,"{'BugID': 'ACCUMULO-178', 'Summary': 'Off-by-one error in FamilyIntersectingIterator', 'Description': ""In the buildDocKey() function within the FamilyIntersectingIterator there is a bug that shortens the docID by 1.  This causes the wrong doc's data to be returned in the results of a query using this Iterator.""}"
accumulo,bugs-dot-jar_ACCUMULO-178_efef09b0,"{'BugID': 'ACCUMULO-178', 'Summary': 'Off-by-one error in FamilyIntersectingIterator', 'Description': ""In the buildDocKey() function within the FamilyIntersectingIterator there is a bug that shortens the docID by 1.  This causes the wrong doc's data to be returned in the results of a query using this Iterator.""}"
accumulo,bugs-dot-jar_ACCUMULO-1800_3143b9c5,"{'BugID': 'ACCUMULO-1800', 'Summary': 'delete mutations not working through the Proxy', 'Description': ""Aru Sahni writes:\n\n{quote}\nI'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.\n\nRight now I'm trying to understand how Accumulo manages record (key-value pair) deletions.\n\nconn = Accumulo(host, port, user, password)\ntable = 'test_table'\nconn.create_table(table)\nwriter = conn.create_batch_writer(table)\nmut = Mutation('mut_01')\nmut.put(cf='item', cq='name', value='car')\nwriter.add_mutation(mut)\nwriter.close()\nconn.close()\n\nWill generate a record (found via a shell scan):\n\nmut_01 item:name []    car\n\nHowever the subsequent mutation...\n\nwriter = conn.create_batch_writer(table)\nmut = Mutation('mut_01')\nmut.put(cf='item', cq='name', is_delete=True)\nwriter.add_mutation(mut)\nwriter.close()\n\nResults in:\n\nmut_01 item:name []\n\nHow should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted.\n{quote}\n\n[~ecn] has confirmed the problem.\n""}"
accumulo,bugs-dot-jar_ACCUMULO-1800_8ec4cb84,"{'BugID': 'ACCUMULO-1800', 'Summary': 'delete mutations not working through the Proxy', 'Description': ""Aru Sahni writes:\n\n{quote}\nI'm new to Accumulo and am still trying to wrap my head around its ways. To further that challenge, I'm using Pyaccumulo, which doesn't present much in terms of available reference material.\n\nRight now I'm trying to understand how Accumulo manages record (key-value pair) deletions.\n\nconn = Accumulo(host, port, user, password)\ntable = 'test_table'\nconn.create_table(table)\nwriter = conn.create_batch_writer(table)\nmut = Mutation('mut_01')\nmut.put(cf='item', cq='name', value='car')\nwriter.add_mutation(mut)\nwriter.close()\nconn.close()\n\nWill generate a record (found via a shell scan):\n\nmut_01 item:name []    car\n\nHowever the subsequent mutation...\n\nwriter = conn.create_batch_writer(table)\nmut = Mutation('mut_01')\nmut.put(cf='item', cq='name', is_delete=True)\nwriter.add_mutation(mut)\nwriter.close()\n\nResults in:\n\nmut_01 item:name []\n\nHow should one expect the deleted row to be represented? That record sticks around even after I force a compaction of the table.  I was expecting it to not show up in any iterators, or at least provide an easy way to see if the cell has been deleted.\n{quote}\n\n[~ecn] has confirmed the problem.\n""}"
accumulo,bugs-dot-jar_ACCUMULO-189_6dbbdc21,"{'BugID': 'ACCUMULO-189', 'Summary': 'RegExFilter deepCopy NullPointerException', 'Description': 'If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-189_cd7feb4d,"{'BugID': 'ACCUMULO-189', 'Summary': 'RegExFilter deepCopy NullPointerException', 'Description': 'If any of the regex matcher objects are null (i.e. for example, if you only specify a regex for the column family), the deepCopy call will throw a NullPointerException.\n'}"
accumulo,bugs-dot-jar_ACCUMULO-193_8ad5a888,"{'BugID': 'ACCUMULO-193', 'Summary': 'key.followingKey(PartialKey.ROW_COLFAM_COLQUAL_COLVIS) can produce a key with an invalid COLVIS', 'Description': 'Need a new algorithm for calculating the next biggest column visibility, because tagging \\0 to the end creates an invalid column visibility. We might be able to minimize the timestamp for this (i.e. set timestamp to Long.MIN_VALUE, but keep column and row elements the same).'}"
accumulo,bugs-dot-jar_ACCUMULO-193_c831e44d,"{'BugID': 'ACCUMULO-193', 'Summary': 'key.followingKey(PartialKey.ROW_COLFAM_COLQUAL_COLVIS) can produce a key with an invalid COLVIS', 'Description': 'Need a new algorithm for calculating the next biggest column visibility, because tagging \\0 to the end creates an invalid column visibility. We might be able to minimize the timestamp for this (i.e. set timestamp to Long.MIN_VALUE, but keep column and row elements the same).'}"
accumulo,bugs-dot-jar_ACCUMULO-1986_2d97b875,"{'BugID': 'ACCUMULO-1986', 'Summary': 'Validity checks missing for readFields and Thrift deserialization', 'Description': ""Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.""}"
accumulo,bugs-dot-jar_ACCUMULO-1986_a5e3ed3b,"{'BugID': 'ACCUMULO-1986', 'Summary': 'Validity checks missing for readFields and Thrift deserialization', 'Description': ""Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.""}"
accumulo,bugs-dot-jar_ACCUMULO-1986_adee0f12,"{'BugID': 'ACCUMULO-1986', 'Summary': 'Validity checks missing for readFields and Thrift deserialization', 'Description': ""Classes in o.a.a.core.data (and potentially elsewhere) that support construction from a Thrift object and/or population from a {{DataInput}} (via a {{readFields()}} method) often lack data validity checks that the classes' constructors enforce. The missing checks make it possible for an attacker to create invalid objects by manipulating the bytes being read. The situation is analogous to the need to check objects deserialized from their Java serialized form within the {{readObject()}} method.""}"
accumulo,bugs-dot-jar_ACCUMULO-209_397f86f6,"{'BugID': 'ACCUMULO-209', 'Summary': 'RegExFilter does not properly regex when using multi-byte characters', 'Description': 'The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.\n\nLine 49 of ByteArrayBackedCharSequence.java is:\nreturn (char) (0xff & data[offset + index]);                                                                                              \n\nThis incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.\n\nA patch for the RegExFilter.java file has been created and will be submitted.'}"
accumulo,bugs-dot-jar_ACCUMULO-209_76d727f0,"{'BugID': 'ACCUMULO-209', 'Summary': 'RegExFilter does not properly regex when using multi-byte characters', 'Description': 'The current RegExFilter class uses a ByteArrayBackedCharSequence to set the data to match against. The ByteArrayBackedCharSequence contains a line of code that prevents the matcher from properly matching multi-byte characters.\n\nLine 49 of ByteArrayBackedCharSequence.java is:\nreturn (char) (0xff & data[offset + index]);                                                                                              \n\nThis incorrectly casts a single byte from the byte array to a char, which is 2 bytes in Java. This prevents the RegExFilter from properly performing Regular Expressions on multi-byte character encoded values.\n\nA patch for the RegExFilter.java file has been created and will be submitted.'}"
accumulo,bugs-dot-jar_ACCUMULO-217_46f62443,"{'BugID': 'ACCUMULO-217', 'Summary': ""MockAccumulo doesn't throw informative errors"", 'Description': 'Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.'}"
accumulo,bugs-dot-jar_ACCUMULO-217_add180fb,"{'BugID': 'ACCUMULO-217', 'Summary': ""MockAccumulo doesn't throw informative errors"", 'Description': 'Users are unable to tell if an error has occurred and whether it is due to unimplemented features in MockAccumulo.'}"
accumulo,bugs-dot-jar_ACCUMULO-218_15476a0d,"{'BugID': 'ACCUMULO-218', 'Summary': 'Mock Accumulo Inverts order of mutations w/ same timestamp', 'Description': 'Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.'}"
accumulo,bugs-dot-jar_ACCUMULO-218_3d55560a,"{'BugID': 'ACCUMULO-218', 'Summary': 'Mock Accumulo Inverts order of mutations w/ same timestamp', 'Description': 'Mock accumulo has different behavior than real accumulo when the same key is updated in the same millisecond.  The hidden in memory map counter in mock accumulo needs to sort descending.'}"
accumulo,bugs-dot-jar_ACCUMULO-2390_28294266,"{'BugID': 'ACCUMULO-2390', 'Summary': 'TraceProxy.trace should not throw InvocationTargetException', 'Description': ""In {{TraceProxy.trace}} there is the following code snippet:\n{code}\n        try {\n          return method.invoke(instance, args);\n        } catch (Throwable ex) {\n          ex.printStackTrace();\n          throw ex;\n        }\n{code}\nWhen this is an InvocationTargetException, it can really mess with the calling code's exception handling logic.""}"
accumulo,bugs-dot-jar_ACCUMULO-2487_f2920c26,"{'BugID': 'ACCUMULO-2487', 'Summary': 'Value implementation provides conflicting statements', 'Description': 'The javadoc for the no-arg constructor for {{Value}} states that it ""Creates a zero-size sequence."" However, the implementation of get will error in this case.\n{code}\npublic byte[] get() {\n    if (this.value == null) {\n      throw new IllegalStateException(""Uninitialized. Null constructor "" + ""called w/o accompanying readFields invocation"");\n    }\n{code}\n\nEither we need to change the javadoc to be more explicit or change the behaviour of various accessors in the class. I would consider both solutions to be breaking of the API contract since we are changing what clients can expect from us.'}"
accumulo,bugs-dot-jar_ACCUMULO-2494_0dc92ca1,"{'BugID': 'ACCUMULO-2494', 'Summary': 'Stat calculation of STDEV may be inaccurate', 'Description': 'The math is sound, but it is susceptible to rounding errors. We should address that.\n\nSee http://www.strchr.com/standard_deviation_in_one_pass and http://www.cs.berkeley.edu/~mhoemmen/cs194/Tutorials/variance.pdf'}"
accumulo,bugs-dot-jar_ACCUMULO-2520_a64151e6,"{'BugID': 'ACCUMULO-2520', 'Summary': 'Garbage collector deleted everything when given bad input', 'Description': 'Patch v3 of the upgrade from ACCUMULO-2145 had a test that did the following before upgrade.\n\n{noformat}\nroot@testUp> table !METADATA\nroot@testUp !METADATA> grant Table.WRITE -u root \nroot@testUp !METADATA> insert ~del testDel test valueTest\n{noformat}\n\nThis is a malformed delete entry.  Accumulo code should not delete such entries.  When the 1.5.1 garbage collector saw this it did the following.\n\n{noformat}\n2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables\n2014-03-20 18:20:05,359 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/default_tablet/F0000009.rf\n2014-03-20 18:20:05,360 [gc.SimpleGarbageCollector] DEBUG: Deleting /accumuloTest/tables/!0/table_info/F000000b.rf\n{noformat}\n\nGC should validate that delete entries are paths of the expected length.  I have confirmed this bug exist in 1.5.1.  I am assuming it exist in 1.4 and 1.6 branches.'}"
accumulo,bugs-dot-jar_ACCUMULO-2544_7ec60f1b,"{'BugID': 'ACCUMULO-2544', 'Summary': 'Incorrect boundry matching for MockTableOperations.deleteRows', 'Description': 'The api for deleteRows specifies: Delete rows between (start, end] but the current implementation for MockTableOperations.deleteRows is implemented as (start, end)\n\nHere is the failing test case\n\n{code:java}\npublic class TestDelete {\n  private static final String INSTANCE = ""mock"";\n  private static final String TABLE = ""foo"";\n  private static final String USER = ""user"";\n  private static final String PASS = ""password"";\n  private static final Authorizations AUTHS = new Authorizations();\n\n  @Test\n  public void testDelete() throws TableNotFoundException, AccumuloException,\n      AccumuloSecurityException, TableExistsException {\n\n    MockInstance mockAcc = new MockInstance(INSTANCE);\n    Connector conn = mockAcc.getConnector(USER, new PasswordToken(PASS));\n    conn.tableOperations().create(TABLE);\n    conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.READ);\n    conn.securityOperations().grantTablePermission(USER, TABLE, TablePermission.WRITE);\n\n    Mutation mut = new Mutation(""2"");\n    mut.put(""colfam"", ""colqual"", ""value"");\n    BatchWriter writer = conn.createBatchWriter(TABLE, new BatchWriterConfig());\n    writer.addMutation(mut);\n\n    Scanner scan = conn.createScanner(TABLE, AUTHS);\n    scan.setRange(new Range(""2"", ""2""));\n\n    assertEquals(1, countRecords(scan));\n    \n    // this should delete (1,2] \n    conn.tableOperations().deleteRows(TABLE, new Text(""1""), new Text(""2""));\n\n    scan = conn.createScanner(TABLE, AUTHS);\n    scan.setRange(new Range(""2"", ""2""));\n    \n    // this will fail if row 2 exists\n    assertEquals(0, countRecords(scan));\n  }\n\n  private int countRecords(Scanner scan) {\n    int cnt = 0;\n    for (Entry<Key, Value> entry : scan) {\n      cnt++;\n    }\n    scan.close();\n    return cnt;\n  }\n}\n{code}'}"
accumulo,bugs-dot-jar_ACCUMULO-2659_019edb16,"{'BugID': 'ACCUMULO-2659', 'Summary': 'Incompatible API changes in 1.6.0', 'Description': 'While examining API changes for 1.6.0 I noticed some non-deprecated methods were removed.  I am not sure how important these are, but technically these methods are in the public API.  Opening this issue to document what I found.\n\nI compared 1.6.0 to 1.5.0.\n\nIn ACCUMULO-1674 the following methods were removed\n\n{noformat}\npackage org.apache.accumulo.core.client.mapreduce.lib.util\nConfiguratorBase.getToken ( Class<?>, Configuration ) [static]  :  byte[ ]\nConfiguratorBase.getTokenClass ( Class<?> ,Configuration) [static]  :  String\n{noformat}\n\nIn ACCUMULO-391 the following method was removed\n\n{noformat}\npackage org.apache.accumulo.core.client.mapreduce.lib.util\nInputConfigurator.getTabletLocator ( Class<?>, Configuration ) [static]  : TabletLocator \n{noformat}\n\nIn ACCUMULO-391 the following method was removed and not properly fixed in ACCUMULO-2586\n\n{noformat}\naccumulo-core.jar, RangeInputSplit.class\npackage org.apache.accumulo.core.client.mapred\nInputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations )\npackage org.apache.accumulo.core.client.mapreduce\nInputFormatBase.RangeInputSplit.InputFormatBase.RangeInputSplit ( String table, Range range, String[ ] locations ) \n{noformat}\n\n It seems like the following were removed in ACCUMULO-1854 \n\n{noformat}\npackage org.apache.accumulo.core.client.mapred\nInputFormatBase.RecordReaderBase<K.setupIterators (JobConf job, Scanner scanner )  :  void\npackage org.apache.accumulo.core.client.mapreduce\nInputFormatBase.RecordReaderBase<K.setupIterators (TaskAttemptContext context, Scanner scanner)  :  void\n{noformat}\n\nIn ACCUMULO-1018 the following method was removed\n\n{noformat}\npackage org.apache.accumulo.core.client\nMutationsRejectedException.MutationsRejectedException ( List, HashMap, Set, Collection, int cause, Throwable cvsList ) \n{noformat}'}"
accumulo,bugs-dot-jar_ACCUMULO-2671_17344890,"{'BugID': 'ACCUMULO-2671', 'Summary': 'BlockedOutputStream can hit a StackOverflowError', 'Description': 'This issue mostly came up after a resolution to ACCUMULO-2668 that allows a byte[] to be passed directly to the underlying stream from the NoFlushOutputStream.\n\nThe problem appears to be due to the BlockedOutputStream.write(byte[], int, int) implementation that recursively writes out blocks/buffers out. When the stream is passed a large mutation (128MB was sufficient to trigger the error for me), this will cause a StackOverflowError. \n\nThis is appears to be specifically with encryption at rest turned on.\n\nA simple fix would be to unroll the recursion.'}"
accumulo,bugs-dot-jar_ACCUMULO-2713_6138a80f,"{'BugID': 'ACCUMULO-2713', 'Summary': 'Instance secret written out with other configuration items to RFiles and WALogs when encryption is turned on', 'Description': 'The encryption at rest feature records configuration information in order to encrypted RFiles and WALogs so that if the configuration changes, the files can be read back.  The code that does this recording hovers up all the ""instance.*"" entries, and does not pick out the instance.secret as a special one not to write.  Thus the instance secret goes into each file in the clear, which is non-ideal to say the least.\n\nPatch forthcoming.'}"
accumulo,bugs-dot-jar_ACCUMULO-2742_1f7dd2d5,"{'BugID': 'ACCUMULO-2742', 'Summary': 'History command incorrectly numbers commands', 'Description': 'When you use the history command, it will provide you with a list of previous commands that have been executed, each with a command number. However, if you try to use history expansion by number to invoke one of those commands, you will be off by one.\n\nI think this is because the history command in added to the list after it shows you the list, and pushes everything else up by one. Uncertain if this is something we do wrong, or if this is an upstream JLine bug.'}"
accumulo,bugs-dot-jar_ACCUMULO-2748_ff8c2383,"{'BugID': 'ACCUMULO-2748', 'Summary': 'MockTableOperations.deleteRow does not handle null for start or end keys', 'Description': 'The deleteRow function does not check for null values for start or end keys.\nThese null values are passed down into key constructor which will throw a NullPointerException:\njava.lang.NullPointerException\n\tat org.apache.accumulo.core.data.Key.<init>(Key.java:103)\n\tat org.apache.accumulo.core.client.mock.MockTableOperations.deleteRows(MockTableOperations.java:315)\n\nThe API semantics dictate:\nif (start == null ) then start == Text()\nif (end == null ) then end == maxKey()\n\n\n\n'}"
accumulo,bugs-dot-jar_ACCUMULO-2857_9fcca2ed,"{'BugID': 'ACCUMULO-2857', 'Summary': 'MockTableOperations.tableIdMap always returns tableName as ID', 'Description': 'Noticed and fixed this during ACCUMULO-378.\n\nAn exception was thrown unexpectedly when trying to use tableIdMap with a MockInstance. Lift fix from 93c8bddc71d1ee190649eeab263205185d75421c into main tree.'}"
accumulo,bugs-dot-jar_ACCUMULO-2899_31aea2ad,"{'BugID': 'ACCUMULO-2899', 'Summary': 'WAL handling fails to deal with 1.4 -> 1.5 -> 1.6', 'Description': 'After doing a 1.4 -> 1.5 -> 1.6 upgrade that still has WALs for some tables, the 1.6 instance fails to correctly handle the 1.4 recovered WALs.\n\nThis can happen either through not waiting long enough after the upgrade to 1.5 or because of an offline table brought online on 1.6 (ala ACCUMULO-2816).'}"
accumulo,bugs-dot-jar_ACCUMULO-2928_f99b5654,"{'BugID': 'ACCUMULO-2928', 'Summary': 'Missing toString, hashCode and equals methods on BatchWriterConfig', 'Description': ""Tried to test equality of two BatchWriterConfig objects, found they're missing all of the methods from Object that they should be implementing.""}"
accumulo,bugs-dot-jar_ACCUMULO-2952_11d11e0d,"{'BugID': 'ACCUMULO-2952', 'Summary': 'DefaultLoadBalancer takes a long time when tablets are highly unbalanced', 'Description': 'After creating a thousand splits on a large cluster, I noticed the master was only moving tablets to one server at a time.'}"
accumulo,bugs-dot-jar_ACCUMULO-2962_023be574,"{'BugID': 'ACCUMULO-2962', 'Summary': ""RangeInputSplit Writable methods don't serialize IteratorSettings"", 'Description': 'Was trying to figure out why some information was getting lost on a RangeInputSplit after serialization, and found out it was because the serialization and deserialization of the class didn\'t include the configured IteratorSettings.\n\nThis likely isn\'t a big problem for normal users as, when no IteratorSettings are configured on the RangeInputSplit, it falls back to pulling from the Configuration, but it\'s possible, with ""non-standard"" uses of mapreduce, that information could be missing in the Configuration that the mappers receive, and would subsequently error.'}"
accumulo,bugs-dot-jar_ACCUMULO-2962_2fd7633f,"{'BugID': 'ACCUMULO-2962', 'Summary': ""RangeInputSplit Writable methods don't serialize IteratorSettings"", 'Description': 'Was trying to figure out why some information was getting lost on a RangeInputSplit after serialization, and found out it was because the serialization and deserialization of the class didn\'t include the configured IteratorSettings.\n\nThis likely isn\'t a big problem for normal users as, when no IteratorSettings are configured on the RangeInputSplit, it falls back to pulling from the Configuration, but it\'s possible, with ""non-standard"" uses of mapreduce, that information could be missing in the Configuration that the mappers receive, and would subsequently error.'}"
accumulo,bugs-dot-jar_ACCUMULO-2974_5eceb10e,"{'BugID': 'ACCUMULO-2974', 'Summary': 'Unable to assign single tablet table migrated to 1.6.0', 'Description': ""Sorry for the screen caps, no copy/paste from machines.\n\nBackground- several tables migrated from 1.5.1 to 1.6.0. Only one of which was a single tablet. Upon starting, we noticed that that single table was not loading and the master was reporting an unassigned tablet. Had a stack trace in the monitor (attached).\n\nAlso attached is a a metadata scan of the table in question (ID: 12). I was able to get a functional copy of the table by offlining 12 and cloning it. It functioned without issues. Attached is a copy of it's metadata scan as well (ID: 9o)\n\nThe stack trace leads me to it being a specific issue with the contents of srv:dir, and the only difference is the relative vs. absolute file names. This cluster was not changed to multiple namenodes and ../tables/default_tablet does not exist. There are other tables which still use the relative naming scheme, and the system does not seem to be having issues with them.""}"
accumulo,bugs-dot-jar_ACCUMULO-3006_d6472040,"{'BugID': 'ACCUMULO-3006', 'Summary': ""Don't allow viewfs in instance.volumes"", 'Description': 'I think one of our folks put viewfs into instance.volumes on accident. File references in accumulo.root and accumulo.metadata were then written with viewfs in the path. The garbage collector then throws errors as compactions occur and it tries delete and move the files to the hdfs users trash directory.\n\nviewfs should never be allowed in instance.volumes property. It should fail.'}"
accumulo,bugs-dot-jar_ACCUMULO-3015_f848178e,"{'BugID': 'ACCUMULO-3015', 'Summary': ""RangeInputSplit doesn't serialize table name"", 'Description': 'Found another missed member in the serialization of RangeInputSplit: the table name.\n\nNot a huge deal because the table information should still be in the Configuration for most users, but this does break in ""advanced"" uses of mapreduce. Work around is to re-set the table in the RangeInputSplit in your overridden InputFormat.getRecordReader or make sure the Configuration is consistent from getRecordReader and getSplits.'}"
accumulo,bugs-dot-jar_ACCUMULO-3055_94c2a31f,"{'BugID': 'ACCUMULO-3055', 'Summary': 'calling MiniAccumuloCluster.stop multiple times fails with NPE', 'Description': 'On the mailing list [~ctubbsii] mentioned seeing some NPEs in the stderr for {{mvn verify}}.\n\nI see one here when running mvn verify with either hadoop profile:\n\n{quote}\nException in thread ""Thread-0"" java.lang.NullPointerException\n\tat org.apache.accumulo.minicluster.MiniAccumuloCluster.stopProcessWithTimeout(MiniAccumuloCluster.java:449)\n\tat org.apache.accumulo.minicluster.MiniAccumuloCluster.stop(MiniAccumuloCluster.java:376)\n\tat org.apache.accumulo.minicluster.MiniAccumuloCluster$1.run(MiniAccumuloCluster.java:318)\n{quote}\n\nThe relevant piece of code (in 1.5.2-SNAP) is the {{executor.execute}} below\n\n{code}\n  private int stopProcessWithTimeout(final Process proc, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {\n    FutureTask<Integer> future = new FutureTask<Integer>(new Callable<Integer>() {\n        @Override\n        public Integer call() throws InterruptedException {\n          proc.destroy();\n          return proc.waitFor();\n        }\n    });\n\n    executor.execute(future);\n\n    return future.get(timeout, unit);\n  }\n{code}\n\nReading through the code for stop, it nulls out executor when it\'s done. So the easy way to get an NPE is calling stop() multiple times on a MAC instance. Since we have a shutdown hook that calls stop, that means that a single user invocation of stop should result in a NPE later.\n\nSince start() doesn\'t allow multiple starts, we probably shouldn\'t allow multiple stops. That would mean adding logic to the shutdown hook to check if we\'re already stopped or making a private unguarded version of stop that allows multiple calls and using that from the hook.\n\ncriteria for closing this issue:\n\n* MAC should document wether calling stop() multiple times is allowed\n* fix MAC.stop to either guard against multiple calls or handle them gracefully\n* find out why this only gets an NPE in one place. Do we rely on the shutdown hook everywhere?'}"
accumulo,bugs-dot-jar_ACCUMULO-3077_17654199,"{'BugID': 'ACCUMULO-3077', 'Summary': 'File never picked up for replication', 'Description': 'I was running some tests and noticed that a single file was getting ignored. The logs were warning that the Status message that was written to {{accumulo.metadata}} didn\'t have a createdTime on the Status record.\n\nThe odd part is that all other Status messages had a createdTime and were successfully replicated. Looking at the writes from the TabletServer logs, the expected record *was* written by the TabletServer, and writing a test with the full series of Status records written does net the correct Status (which was different than what was observed in the actual table).\n\nLooking into it, the log which was subject to this error was the first WAL that was used when the instance was started. Because the table configurations are lazily configured when they are actually used, I believe that the StatusCombiner that is set on {{accumulo.metadata}} was not seen by the TabletServer, and the VersioningIterator ""ate"" the first record.\n\nI need to come up with a way that I can be sure that all tservers will have seen the Combiner set on accumulo.metadata before any data is written to it to avoid losing a record like this.'}"
accumulo,bugs-dot-jar_ACCUMULO-3143_ddd2c3bc,"{'BugID': 'ACCUMULO-3143', 'Summary': 'InputTableConfig missing isOfflineScan field in Serializer', 'Description': 'InputTableConfig write(DataOutput dataOutput) forgets to write out the isOfflineScan field, which makes it always false when it gets unserialized.'}"
accumulo,bugs-dot-jar_ACCUMULO-3150_72fd6bec,"{'BugID': 'ACCUMULO-3150', 'Summary': ""MiniAccumuloConfig doesn't set 0 for monitor log4j port"", 'Description': ""MonitorLoggingIT will fail on a host if the monitor is already running because MAC doesn't configure itself to use an ephemeral port. We haven't really noticed this because MAC doesn't start a monitor by default.""}"
accumulo,bugs-dot-jar_ACCUMULO-3218_1b35d263,"{'BugID': 'ACCUMULO-3218', 'Summary': 'ZooKeeperInstance only uses first ZooKeeper in list of quorum', 'Description': 'Had tests running which had a quorum of 3 ZooKeeper servers. One appears to have died and the test was then unable to connect to the Accumulo shell, hanging on trying to connect to ZooKeeper.\n\nThere was no client.conf file present, so a ClientConfiguration was constructed from accumulo-site.xml.\n\n{code}\nthis.zooKeepers = clientConf.get(ClientProperty.INSTANCE_ZK_HOST);\n{code}\n\nWhen the commons configuration AbstractConfiguration class is used with the get() method, only the first element in the value is returned, as the implementation treats the other items as a list because of the default separator of a comma.\n\nIt\'s easily reproduced with the following:\n\n{code}\n    ZooKeeperInstance inst = new ZooKeeperInstance(""accumulo"", ""localhost,127.0.0.1"");\n    System.out.println(inst.getZooKeepers());\n{code}\n\nThe above will print\n\n{noformat}\nlocalhost\n{noformat}\n\ninstead of the expected\n\n{noformat}\nlocalhost,127.0.0.1\n{noformat}'}"
accumulo,bugs-dot-jar_ACCUMULO-3229_891584fb,"{'BugID': 'ACCUMULO-3229', 'Summary': 'Shell displays authTimeout poorly', 'Description': 'The authTimeout in the shell is displayed badly when executing {{about -v}}.\nEven though it is configured in integer minutes, it is converted to seconds for display as a floating point number with 2 decimals. This makes no sense, since the decimals will always be {{.00}}.\n\nWe can keep the units in seconds, I guess, but this needs to be displayed with {{%ds}} not {{%.2fs}}. This was broken in ACCUMULO-3224 by using TimeUnit to convert the number, instead of dividing by 1000.0 as we were doing manually before.'}"
accumulo,bugs-dot-jar_ACCUMULO-3242_15e83709,"{'BugID': 'ACCUMULO-3242', 'Summary': 'Consolidate ZK code WRT retries', 'Description': 'A couple of general ZK things that should be fixed up:\n\n# Multiple means of automatic retrying of recoverable ZooKeeper errors through use of an InvocationHandler and a Proxy around IZooReader(Writer)\n# Encapsulate retry logic\n# Switch over callers to use the retrying instance instead of the non-retrying instance'}"
accumulo,bugs-dot-jar_ACCUMULO-334_9d8cc45d,"{'BugID': 'ACCUMULO-334', 'Summary': 'Bulk random walk test failed', 'Description': 'The bulk random walk test failed while running on a 10 node cluster w/ the following error message.\n\n{noformat}\n18 23:36:05,167 [bulk.Setup] INFO : Starting bulk test on 459a04a0\n\n\n19 00:24:33,950 [randomwalk.Framework] ERROR: Error during random walk\njava.lang.Exception: Error running node Bulk.xml\n        at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)\n        at org.apache.accumulo.server.test.randomwalk.Framework.run(Framework.java:61)\n        at org.apache.accumulo.server.test.randomwalk.Framework.main(Framework.java:114)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.accumulo.start.Main$1.run(Main.java:89)\n        at java.lang.Thread.run(Thread.java:662)\nCaused by: java.lang.Exception: Error running node bulk.Verify\n        at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:253)\n        at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)\n        ... 8 more\nCaused by: java.lang.Exception: Bad key at r0d646 cf:000 [] 1326932285943 false -1\n        at org.apache.accumulo.server.test.randomwalk.bulk.Verify.visit(Verify.java:51)\n        at org.apache.accumulo.server.test.randomwalk.Module.visit(Module.java:249)\n        ... 9 more\n{noformat}\n\nLooking at the table the rows [r0d646, r0edd9] and [r0f056, r10467] all had -1 values.  There was a tablet that overlapped the first range of -1 rows exactly 268;r0edd9;r0d645.  This tablet had only the following activity on a tablet server and was then merged out of existence.  The merge operation was 268;r10eff;r093b1.\n\n{noformat}\n19 00:05:10,966 [tabletserver.Tablet] DEBUG: Files for low split 268;r0edd9;r0d645  [/b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf]\n19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9\n19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 opened \n19 00:05:15,029 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azi/I0001azm.rf 17138 0\n19 00:05:15,103 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf_tmp\n19 00:05:15,339 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 import /b-0001azx/I0001azy.rf 16620 0\n19 00:05:15,651 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 181,080 read | 60,360 written | 553,761 entries/sec |  0.327 secs\n19 00:05:15,661 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azi/I0001azm.rf, /b-0001azp/I0001azt.rf, /b-0001azp/I0001azu.rf, /t-0001ale/A0001an3.rf] --> /t-0001apj/A0001bri.rf\n19 00:05:30,672 [tabletserver.Tablet] DEBUG: Starting MajC 268;r0edd9;r0d645 [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf_tmp\n19 00:05:30,810 [tabletserver.Compactor] DEBUG: Compaction 268;r0edd9;r0d645 60,360 read | 60,360 written | 534,159 entries/sec |  0.113 secs\n19 00:05:30,824 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 MajC [/b-0001azx/I0001azy.rf] --> /t-0001apj/C0001brn.rf\n19 00:05:30,943 [tabletserver.Tablet] DEBUG: initiateClose(saveState=true queueMinC=false disableWrites=false) 268;r0edd9;r0d645\n19 00:05:30,943 [tabletserver.Tablet] DEBUG: completeClose(saveState=true completeClose=true) 268;r0edd9;r0d645\n19 00:05:30,947 [tabletserver.Tablet] TABLET_HIST: 268;r0edd9;r0d645 closed\n19 00:05:30,947 [tabletserver.TabletServer] DEBUG: Unassigning 268;r0edd9;r0d645@(null,xxx.xxx.xxx.xxx:9997[134d7425fc59413],null)\n19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645\n19 00:05:30,949 [tabletserver.TabletServer] INFO : unloaded 268;r0edd9;r0d645\n\n{noformat}\n\n\nFor the second range of -1 values [r0f056, r10467], r0f056 corresponds to the split point r0f055.  Howerver, there is no split point corresponding to r10467. All of the tablets w/ a split of r0f055 lived on one tablet server.  \n\n{noformat}\n19 00:02:21,262 [tabletserver.Tablet] TABLET_HIST: 268<;r0d645 split 268;r0f055;r0d645 268<;r0f055\n19 00:02:21,263 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 opened \n19 00:02:21,264 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 opened \n19 00:02:44,504 [tabletserver.Tablet] TABLET_HIST: 268<;r0f055 split 268;r11da6;r0f055 268<;r11da6\n19 00:02:44,505 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 opened \n19 00:05:10,974 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0d645 split 268;r0edd9;r0d645 268;r0f055;r0edd9\n19 00:05:10,975 [tabletserver.Tablet] TABLET_HIST: 268;r0f055;r0edd9 opened \n19 00:05:15,023 [tabletserver.Tablet] TABLET_HIST: 268;r11da6;r0f055 split 268;r0f622;r0f055 268;r11da6;r0f622\n19 00:05:15,024 [tabletserver.Tablet] TABLET_HIST: 268;r0f622;r0f055 opened \n{noformat}\n\nAll of the tablets mentioned so far were all merged away in the same merge operation, making this operation a possible place were data loss occurred.  However, I can not pinpoint the issue at this point in time.  Below is a little info about the merge from the master logs showing which tablets were involved in the merge.\n\n{noformat}\n19 00:05:30,616 [master.EventCoordinator] INFO : Merge state of 268;r10eff;r093b1 set to WAITING_FOR_CHOPPED\n19 00:05:30,677 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r09927;r0903a\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940c] to chop 268;r0ca9e;r09927\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc5940a] to chop 268;r0d2b5;r0ca9e\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59412] to chop 268;r0d645;r0d2b5\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0edd9;r0d645\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f055;r0edd9\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f622;r0f055\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r0f68b;r0f622\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r10c14;r0f68b\n19 00:05:30,678 [master.Master] INFO : Asking xxx.xxx.xxx.xxx:9997[134d7425fc59413] to chop 268;r110f7;r10c14\n{noformat}\n\nWhen this test verifies its data and detects data loss, there is no easy way to determine at what time the data loss occurred.  It might be useful to modify the data in the bulk test such that it is easier to determine the time when data was lost.  For example the continuous ingest test creates linked list and it is possible to determine tight time bounds when a node was ingested.  However that may change the nature of this test and the bugs that it might find.'}"
accumulo,bugs-dot-jar_ACCUMULO-3383_97f16db4,"{'BugID': 'ACCUMULO-3383', 'Summary': 'AccumuloVFSClassloader creates conflicting local cache directory names when vfs.cache.dir property is set.', 'Description': ""When the vfs.cache.dir property is not set, the AccumuloVFSClassloader will use java.io.tmpdir as a base directory for the local cache of jars and then generate a unique directory name using a combination of the processid, hostname and userid executing the JVM.\n\nWhen the vfs.cache.dir property is set, that value is used as the base directory and  an attempt to generate a unique directory is made using an AtomicInteger. This isn't suitable because for non-long lived processes, this will always be 1 - and there's a good chance that directory already exists and is owned by another user, and not writable to by the user in question. \n\nThis leads to a failure of the invoked accumulo component to start.\n\nModify the behavior of the unique directory creation when vfs.cache.dir is set so that it employs the same mechanism for unique directory naming that is used when it is not set.\n""}"
accumulo,bugs-dot-jar_ACCUMULO-3385_a3267d3e,"{'BugID': 'ACCUMULO-3385', 'Summary': 'DateLexicoder fails to correctly order dates prior to 1970', 'Description': 'DateLexicoder incorrectly orders dates before 1970 at the end of all other dates.\n\nTherefore, the order was correct for all dates if the user only wrote dates before 1970, or only dates after 1970, but not if they did both.\n\nThe DateLexicoder should be fixed to store using a signed LongLexicoder internally, instead of the ULongLexicoder that it used before.'}"
accumulo,bugs-dot-jar_ACCUMULO-3408_81d25bc2,"{'BugID': 'ACCUMULO-3408', 'Summary': 'display the exact number of tablet servers', 'Description': 'This is a regression of ACCUMULO-1140\n\n'}"
accumulo,bugs-dot-jar_ACCUMULO-3424_27d4ee21,"{'BugID': 'ACCUMULO-3424', 'Summary': 'Token class option always requires token property', 'Description': 'In testing out ACCUMULO-2815, I attempted to manually provide a KerberosToken to authenticate myself and then launch the shell, but ran into an issue. The KerberosToken (in its current state) needs no options: it\'s wholly functional on its own.\n\n{{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken}}  gives an error\n\n{noformat}\n2014-12-16 11:41:09,712 [shell.Shell] ERROR: com.beust.jcommander.ParameterException: Must supply either both or neither of \'--tokenClass\' and \'--tokenProperty\'\n{noformat}\n\nAnd providing an empty option just prints the help message {{accumulo shell -tc org.apache.accumulo.core.client.security.tokens.KerberosToken -l """"}}\n\nI\'m guessing the latter is just how the JCommander DynamicParameter is implemented, but I don\'t see a reason why every authentication *must* have some properties provided to it.'}"
accumulo,bugs-dot-jar_ACCUMULO-3474_cfb832a1,"{'BugID': 'ACCUMULO-3474', 'Summary': 'ProxyServer ignores value of isDeleted on ColumnUpdate', 'Description': 'The ProxyServer ignores the actual boolean value of the isDeleted flag on a ColumnUpdate.  If the isDeleted value is set, regardless of the actual boolean value, the ProxyServer marks the update as a delete.\n\nThe ProxyServer should be updated to check the value of the flag.'}"
accumulo,bugs-dot-jar_ACCUMULO-3475_7651b777,"{'BugID': 'ACCUMULO-3475', 'Summary': ""Shell.config()'s return value is ignored."", 'Description': ""{{Shell.config()}} returns a boolean which is true if there was an error configuring the shell, but the value is never observed. This can result in other unintended errors (like trying to use the ConsoleReader member when it's not initialized).""}"
accumulo,bugs-dot-jar_ACCUMULO-3634_9339ecf8,"{'BugID': 'ACCUMULO-3634', 'Summary': 'AuthenticationTokenSecretManager might delete key while ZooAuthenticationKeyWatcher enumerates existing keys', 'Description': ""Noticed the following race condition.\n\nThe secret manager (in the master) on startup will enumerate the old keys used for creating delegation tokens and delete the keys that are expired.\n\nAt the same time, the watcher (in each tserver) might see some updates to these keys and update the secret manager. There's a race condition there that the watcher might try to read a key that the secret manager just deleted.\n\nNeed to catch the NoNodeException in the watcher and just accept that it's ok if one of these children are deleted to avoid a scary error in the monitor.""}"
accumulo,bugs-dot-jar_ACCUMULO-366_db4a291f,"{'BugID': 'ACCUMULO-366', 'Summary': 'master killed a tablet server', 'Description': 'Master killed a tablet server for having long hold times.\n\nThe tablet server had this error during minor compaction:\n\n{noformat}\n01 23:57:20,073 [security.ZKAuthenticator] ERROR: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004\norg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /accumulo/88cd0f63-a36a-4218-86b1-9ba1d2cccf08/users/user004\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1243)\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1271)\n        at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:103)\n        at org.apache.accumulo.core.zookeeper.ZooUtil.recursiveDelete(ZooUtil.java:117)\n        at org.apache.accumulo.server.zookeeper.ZooReaderWriter.recursiveDelete(ZooReaderWriter.java:67)\n        at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.accumulo.server.zookeeper.ZooReaderWriter$1.invoke(ZooReaderWriter.java:169)\n        at $Proxy4.recursiveDelete(Unknown Source)\n        at org.apache.accumulo.server.security.ZKAuthenticator.dropUser(ZKAuthenticator.java:252)\n        at org.apache.accumulo.server.security.Auditor.dropUser(Auditor.java:104)\n        at org.apache.accumulo.server.client.ClientServiceHandler.dropUser(ClientServiceHandler.java:136)\n        at sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at cloudtrace.instrument.thrift.TraceWrap$1.invoke(TraceWrap.java:58)\n        at $Proxy2.dropUser(Unknown Source)\n        at org.apache.accumulo.core.client.impl.thrift.ClientService$Processor$dropUser.process(ClientService.java:2257)\n        at org.apache.accumulo.core.tabletserver.thrift.TabletClientService$Processor.process(TabletClientService.java:2037)\n        at org.apache.accumulo.server.util.TServerUtils$TimedProcessor.process(TServerUtils.java:151)\n        at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:631)\n        at org.apache.accumulo.server.util.TServerUtils$THsHaServer$Invocation.run(TServerUtils.java:199)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at org.apache.accumulo.core.util.LoggingRunnable.run(LoggingRunnable.java:34)\n        at java.lang.Thread.run(Thread.java:662)\n\n{noformat}\n\nThis tablet was the result of a split that occurred during a delete.  The master missed this tablet when taking tablets offline.\n\nWe need to do a consistency check on the offline tablets before deleting the table information in zookeeper.\n\n'}"
accumulo,bugs-dot-jar_ACCUMULO-3718_73ce9cfb,"{'BugID': 'ACCUMULO-3718', 'Summary': 'not possible to create a Mutation object from scala w/o some extra helper code', 'Description': ""issue: \n\nit's not possible to create a Mutation object from scala without employing a standalone java jar wrapper. the preferred method for creating the object has you do it in two stages: create with table row, then employ Mutation.put() to populate the object with the actual mutation data. when you do this in scala, you get a\n\njava.lang.IllegalStateException: Can not add to mutation after serializing it at org.apache.accumulo.core.data.Mutation.put(Mutation.java:168) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:163) at org.apache.accumulo.core.data.Mutation.put(Mutation.java:211)\n\nerror. I *think* this has something to do with the byte array going out of scope in Scala but somehow not in Java. If you concat the operations (constuctor().put(data, data, ...) you don't run into the error, but scala sees a Unit return type, so you can't actually add the mutation to a BatchWriter. The only way I was able to get around this was to create a stand-alone jar with a method that created then returned a populated mutation object. \n\nI wasn't sure whether or not to call this a bug or an enhancement. given that you probably want Accumulo to play nice with Scala I decided to call it a bug. \n\nbelow is a link to the stack overflow thread I created whilst figuring all this out: \n\nhttp://stackoverflow.com/questions/29497547/odd-error-when-populating-accumulo-1-6-mutation-object-via-spark-notebook/29527189#29527189\n\n\n""}"
accumulo,bugs-dot-jar_ACCUMULO-3746_47c64d9a,"{'BugID': 'ACCUMULO-3746', 'Summary': ""ClientConfiguration.getAllPropertiesWithPrefix doesn't work"", 'Description': ""I think I introduced this method for trace.span.receiver.*, and didn't write a test for it.  My mistake.""}"
accumulo,bugs-dot-jar_ACCUMULO-3897_699b8bf0,"{'BugID': 'ACCUMULO-3897', 'Summary': 'ShutdownTServer never sets requestedShutdown', 'Description': 'ACCUMULO-1259 made ShutdownTServer a bit more sane WRT to what it was doing and the FATE repo interface.\n\nOne attempt it makes it to not repeatedly invoke shutdownTServer on the master..\n\nExcept {{requestedShutdown}} is never set to {{true}}.'}"
accumulo,bugs-dot-jar_ACCUMULO-3945_36225565,"{'BugID': 'ACCUMULO-3945', 'Summary': ""In Accumulo 1.7.0, connecting to a minicluster started via bin/accumulo minicluster doesn't work"", 'Description': 'In Accumulo 1.7.0, connecting to a minicluster started via ""bin/accumulo minicluster"" doesn\'t work.  When connecting, it appears to ignore the ZK port supplied in the command and is attempting to listen to ZK on 2181.\n\nFor example:\naccumulo-1.7.0 > bin/accumulo minicluster\n…\nMini Accumulo Cluster\n\n  Directory:            /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0\n  Logs:                 /var/folders/rv/44k88tps4ql0dc1f68ck4d2w0000gn/T/1437925819514-0/logs\n  Instance Name:        miniInstance\n  Root Password:        secret\n  ZooKeeper:            localhost:56783\n  Shutdown Port:        4445\n\n  To connect with shell, use the following command :\n    accumulo shell -zh localhost:56783 -zi miniInstance -u root\n\nSuccessfully started on Sun Jul 26 11:50:28 EDT 2015\n===================\n\nFrom a new terminal:\n\naccumulo-1.7.0 > accumulo shell -zh localhost:56783 -zi miniInstance -u root\nPassword: *******\n…. 60 seconds later ….\n2015-07-26 11:52:44,436 [tracer.ZooTraceClient] ERROR: Unabled to get destination tracer hosts\nin ZooKeeper, will retry in 5000 milliseconds\njava.lang.RuntimeException: Failed to connect to zookeeper (localhost:2181) within 2x zookeeper\ntimeout period 30000\n\tat org.apache.accumulo.fate.zookeeper.ZooSession.connect(ZooSession.java:124)\n\nShell - Apache Accumulo Interactive Shell\n-\n- version: 1.7.0\n- instance name: miniInstance\n- instance id: a371d4ac-8bc7-4a6a-865f-5f3c8e27fbe1\n-\n- type \'help\' for a list of available commands\n-\nroot@miniInstance>\n\n'}"
accumulo,bugs-dot-jar_ACCUMULO-4029_5ca779a0,"{'BugID': 'ACCUMULO-4029', 'Summary': 'hashCode for Mutation has an unfortunate implementation', 'Description': ""While looking at how a tablet server processes constraint violations, I happened to look into Mutation's hashCode implementation:\n\n{code}\n  @Override\n  public int hashCode() {\n    return toThrift(false).hashCode();\n  }\n{code}\n\nClicking through to TMutation hashCode finds this gem:\n\n{code}\n  @Override\n  public int hashCode() {\n    return 0;\n  }\n{code}\n""}"
accumulo,bugs-dot-jar_ACCUMULO-4098_a2c2d38a,"{'BugID': 'ACCUMULO-4098', 'Summary': 'ConditionalWriterIT is failing', 'Description': 'I noticed that the ConditionalWriterIT was failing in master.   Using the following command with {{git bisect}} I tracked it down to commit {{3af75fc}} for ACCUMULO-4077 as the change which broke the IT.  Have not looked into why its failing yet.\n\n{noformat}\nmvn clean verify -Dit.test=ConditionalWriterIT -Dfindbugs.skip -Dcheckstyle.skip -Dtest=foo -DfailIfNoTests=false\n{noformat}'}"
accumulo,bugs-dot-jar_ACCUMULO-4113_27300d81,"{'BugID': 'ACCUMULO-4113', 'Summary': 'Fix incorrect usage of ByteBuffer', 'Description': 'While working on ACCUMULO-4098 I found one place where ByteBuffer was being used incorrectly.   Looking around the code, I have found other places that are using ByteBuffer incorrectly.  Some of the problems I found are as follows :\n\n * Calling {{ByteBuffer.array()}} without calling {{ByteBuffer.hasArray()}}.\n * Using {{ByteBuffer.position()}} or {{ByteBuffer.limit()}} without adding {{ByteBuffer.arrayOffset()}} when dealing with an array returned by {{ByteBuffer.array()}}.\n * Using {{ByteBuffer.arrayOffset()}} without adding {{ByteBuffer.position()}} when dealing with an array returned by {{ByteBuffer.array()}}.\n\n'}"
accumulo,bugs-dot-jar_ACCUMULO-412_5594b2e0,"{'BugID': 'ACCUMULO-412', 'Summary': 'importdirectory failing on split table', 'Description': ""bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.""}"
accumulo,bugs-dot-jar_ACCUMULO-412_be2fdba7,"{'BugID': 'ACCUMULO-412', 'Summary': 'importdirectory failing on split table', 'Description': ""bulk import for the wikisearch example isn't working properly: files are not being assigned to partitions if there are splits.""}"
accumulo,bugs-dot-jar_ACCUMULO-4138_4d23d784,"{'BugID': 'ACCUMULO-4138', 'Summary': 'CompactCommand description is incorrect', 'Description': 'The compact command has the following description \n{code}\nroot@accumulo> compact -?\nusage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w]\ndescription: sets all tablets for a table to major compact as soon as possible (based on current time)\n  -?,--help                       display this help\n  -b,--begin-row <begin-row>      begin row (inclusive)\n     --cancel                     cancel user initiated compactions\n  -e,--end-row <end-row>          end row (inclusive)\n  -nf,--noFlush                   do not flush table data in memory before compacting.\n  -ns,--namespace <namespace>     name of a namespace to operate on\n  -p,--pattern <pattern>          regex pattern of table names to operate on\n  -pn,--profile <profile>         iterator profile name\n  -t,--table <tableName>          name of a table to operate on\n  -w,--wait                       wait for compact to finish\n{code}\n\nHowever, the --begin-row is not inclusive.  Here is a simple demonstration.\n{code}\ncreatetable compacttest\naddsplits a b c\ninsert ""a"" ""1"" """" """"\ninsert ""a"" ""2"" """" """"\ninsert ""b"" ""3"" """" """"\ninsert ""b"" ""4"" """" """"\ninsert ""c"" ""5"" """" """"\ninsert ""c"" ""6"" """" """"\nflush -w\nscan -t accumulo.metadata -np\ncompact -b a -e c -t compacttest -w\nscan -t accumulo.metadata -np\ndeletetable compacttest -f\n{code}\n\nYou will see that file associated with the \'a\' split is still a F flush file, which the files in the \'b\' and \'c\' split are A files.\n\nNot sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.'}"
accumulo,bugs-dot-jar_ACCUMULO-4138_50db442b,"{'BugID': 'ACCUMULO-4138', 'Summary': 'CompactCommand description is incorrect', 'Description': 'The compact command has the following description \n{code}\nroot@accumulo> compact -?\nusage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w]\ndescription: sets all tablets for a table to major compact as soon as possible (based on current time)\n  -?,--help                       display this help\n  -b,--begin-row <begin-row>      begin row (inclusive)\n     --cancel                     cancel user initiated compactions\n  -e,--end-row <end-row>          end row (inclusive)\n  -nf,--noFlush                   do not flush table data in memory before compacting.\n  -ns,--namespace <namespace>     name of a namespace to operate on\n  -p,--pattern <pattern>          regex pattern of table names to operate on\n  -pn,--profile <profile>         iterator profile name\n  -t,--table <tableName>          name of a table to operate on\n  -w,--wait                       wait for compact to finish\n{code}\n\nHowever, the --begin-row is not inclusive.  Here is a simple demonstration.\n{code}\ncreatetable compacttest\naddsplits a b c\ninsert ""a"" ""1"" """" """"\ninsert ""a"" ""2"" """" """"\ninsert ""b"" ""3"" """" """"\ninsert ""b"" ""4"" """" """"\ninsert ""c"" ""5"" """" """"\ninsert ""c"" ""6"" """" """"\nflush -w\nscan -t accumulo.metadata -np\ncompact -b a -e c -t compacttest -w\nscan -t accumulo.metadata -np\ndeletetable compacttest -f\n{code}\n\nYou will see that file associated with the \'a\' split is still a F flush file, which the files in the \'b\' and \'c\' split are A files.\n\nNot sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.'}"
accumulo,bugs-dot-jar_ACCUMULO-4138_eb0f9b41,"{'BugID': 'ACCUMULO-4138', 'Summary': 'CompactCommand description is incorrect', 'Description': 'The compact command has the following description \n{code}\nroot@accumulo> compact -?\nusage: compact [<table>{ <table>}] [-?] [-b <begin-row>] [--cancel] [-e <end-row>] [-nf] [-ns <namespace> | -p <pattern> | -t <tableName>]  [-pn <profile>]  [-w]\ndescription: sets all tablets for a table to major compact as soon as possible (based on current time)\n  -?,--help                       display this help\n  -b,--begin-row <begin-row>      begin row (inclusive)\n     --cancel                     cancel user initiated compactions\n  -e,--end-row <end-row>          end row (inclusive)\n  -nf,--noFlush                   do not flush table data in memory before compacting.\n  -ns,--namespace <namespace>     name of a namespace to operate on\n  -p,--pattern <pattern>          regex pattern of table names to operate on\n  -pn,--profile <profile>         iterator profile name\n  -t,--table <tableName>          name of a table to operate on\n  -w,--wait                       wait for compact to finish\n{code}\n\nHowever, the --begin-row is not inclusive.  Here is a simple demonstration.\n{code}\ncreatetable compacttest\naddsplits a b c\ninsert ""a"" ""1"" """" """"\ninsert ""a"" ""2"" """" """"\ninsert ""b"" ""3"" """" """"\ninsert ""b"" ""4"" """" """"\ninsert ""c"" ""5"" """" """"\ninsert ""c"" ""6"" """" """"\nflush -w\nscan -t accumulo.metadata -np\ncompact -b a -e c -t compacttest -w\nscan -t accumulo.metadata -np\ndeletetable compacttest -f\n{code}\n\nYou will see that file associated with the \'a\' split is still a F flush file, which the files in the \'b\' and \'c\' split are A files.\n\nNot sure if the fix is to update the commands description, which would be easy, or to make the begin row actually inclusive.'}"
accumulo,bugs-dot-jar_ACCUMULO-414_116d5928,"{'BugID': 'ACCUMULO-414', 'Summary': 'Make sure iterators handle deletion entries properly', 'Description': 'In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.'}"
accumulo,bugs-dot-jar_ACCUMULO-414_ebf22df0,"{'BugID': 'ACCUMULO-414', 'Summary': 'Make sure iterators handle deletion entries properly', 'Description': 'In minor compaction scope and in non-full major compaction scopes the iterator may see deletion entries. These entries should be preserved by all iterators except ones that are strictly scan-time iterators that will never be configured for the minc or majc scopes. Deletion entries are only removed during full major compactions.'}"
accumulo,bugs-dot-jar_ACCUMULO-633_8dad5e0f,"{'BugID': 'ACCUMULO-633', 'Summary': 'FirstEntryInRowIterator is broken and has no test', 'Description': 'In 1.4 and trunk, the iterator throws a NullPointerException when seeked.\n\nIn 1.3 the iterator runs, but there is a question as to what it should do when it is seeked to the middle of a row.  Currently, it returns the first key found within the range.  I believe this should be changed to ignore the remaining portion of that row and return the first key of the next row.  Should this change be made in 1.3, or should I leave it as is and just change it in 1.4 and greater?'}"
accumulo,bugs-dot-jar_ACCUMULO-776_dc9f23d9,"{'BugID': 'ACCUMULO-776', 'Summary': 'TimestampFilter should serialize start and end as longs in the IteratorSetting', 'Description': ""Although the TimestampFilter supports using longs to set the start or end timestamp, it formats them as strings using SimpleDateFormat when storing or retrieving them in the IteratorSetting.\n\nThis results in exceptions when the timestamps being used aren't able to be formatted as _yyyyMMddHHmmssz_. For example, try {{setEnd(253402300800001,true)}}\n\nInstead, {{setStart()}} and {{setEnd()}} could just as easily use {{String.valueOf(long i)}} to store the values, and {{init()}} could retrieve them using {{Long.valueOf(String s)}}.  ""}"
accumulo,bugs-dot-jar_ACCUMULO-795_9453bcfa,"{'BugID': 'ACCUMULO-795', 'Summary': ""MockTable doesn't obey useVersions parameter"", 'Description': ""The constructor for {{MockTable}} will call {{IteratorUtil.generateInitialTableProperties()}}, and thus set a versioning iterator on itself regardless of whether the useVersion parameter is set to true or false. \n\nI believe {{MockTable}}'s constructor should call IteratorUtil.generateInitialTableProperties() only if useVersions is true, otherwise, it should populate {{settings}} with a new {{TreeMap}}""}"
accumulo,bugs-dot-jar_ACCUMULO-821_a450ac2f,"{'BugID': 'ACCUMULO-821', 'Summary': 'MockBatchScanner inappropriately filters on ranges', 'Description': 'I believe I have a legitimate case where an iterator will return something outside of the seeked-to range.  This appears to work in a live system, but fails to work in test cases using the MockBatchScanner.  I believe this is because the MockBatchScanner filters on the supplied ranges in addition to seeking the iterators to each range.  Either we need to remove this range filter, or fix the real system to do the same thing.  I prefer the former of course.'}"
accumulo,bugs-dot-jar_ACCUMULO-843_65390f8c,"{'BugID': 'ACCUMULO-843', 'Summary': 'Mock does not implement locality groups or merging', 'Description': 'The Mock Instance does not implement locality groups and throws an exception if one attempts to set them. It would be useful for the unit tests that I am writing for the Accumulo proxy to have at least minimal locality group functionality in the Mock instance, for example simply storing the groups and returning the stored groups when asked for.\n\n*Edit: Tablet merging would be useful as well.'}"
accumulo,bugs-dot-jar_ACCUMULO-844_692efde2,"{'BugID': 'ACCUMULO-844', 'Summary': 'VisibilityFilter does not catch BadArgumentException', 'Description': 'If an invalid column visibility makes it into the system, then the VisibilityFilter may not handle it properly.   The accept method handles VisibilityParseException, but some of the parse code throws a BadArgumentException which is not handled.'}"
accumulo,bugs-dot-jar_ACCUMULO-907_4aeaeb2a,"{'BugID': 'ACCUMULO-907', 'Summary': 'stacking combiners produces a strange result', 'Description': 'Paste the following into your shell:\n\n{noformat}\ndeletetable test\ncreatetable test\nsetiter -t test -p 16 -scan -n test_1 -class org.apache.accumulo.core.iterators.user.SummingCombiner\n\ncount:a\n\nSTRING\nsetiter -t test -p 17 -scan -n test_2 -class org.apache.accumulo.core.iterators.user.SummingCombiner\n\ncount:a\n\nSTRING\nsetiter -t test -p 18 -scan -n test_3 -class org.apache.accumulo.core.iterators.user.SummingCombiner\n\ncount:a\n\nSTRING\nsetiter -t test -p 10 -scan -n test_4 -class org.apache.accumulo.core.iterators.user.SummingCombiner\n\ncount\n\nSTRING\ninsert row count a 1\ninsert row count a 1\ninsert row count b 1\ninsert row count b 1\ninsert row count b 1\ninsert row count c 1\nscan\n{noformat}\n\nI expect:\n\n{noformat}\nrow count:a []    2\nrow count:b []    3\nrow count:c []    1\n{noformat}\n\nBut instead, I get this:\n{noformat}\nrow count:a []    12\n{noformat}\n\n'}"
camel,bugs-dot-jar_CAMEL-3276_205420e2,"{'BugID': 'CAMEL-3276', 'Summary': 'Multicast with pipeline may cause wrong aggregated exchange', 'Description': 'This is a problem when using 2 set of nested pipeline and doing a transform as the first processor in that pipeline\n{code}\n                from(""direct:start"").multicast(new SumAggregateBean())\n                    .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to(""log:foo"").end()\n                    .pipeline().transform(bean(IncreaseOne.class)).bean(new IncreaseTwo()).to(""log:bar"").end()\n                .end()\n                .to(""mock:result"");\n{code}\n\n'}"
camel,bugs-dot-jar_CAMEL-3281_f7dd2fff,"{'BugID': 'CAMEL-3281', 'Summary': 'RouteBuilder - Let if fail if end user is configuring onException etc after routes', 'Description': 'All such cross cutting concerns must be defined before routes.\n\nWe should throw an exception if end user has configured them after routes, which is currently not supported in the DSL.'}"
camel,bugs-dot-jar_CAMEL-3314_4badd9c5,"{'BugID': 'CAMEL-3314', 'Summary': 'Property resolve in EIP does not work when in a sub route.', 'Description': 'The 2.5 feature: ""The EIP now supports property placeholders in the String based options (a few spots in Java DSL where its not possible). For example: \n<convertBodyTo type=""String"" charset=""{{foo.myCharset}}""/>"" does not work correctly when ie nested in a <choice> tag.\n\nSee discussion: http://camel.465427.n5.nabble.com/Camel-2-5-Propertyplaceholders-and-Spring-DSL-still-not-working-td3251608.html#a3251608\n\nExample route:\n\nThis works: \n<route> \n        <from uri=""direct:in"" /> \n        <convertBodyTo type=""String"" charset=""{{charset.external}}"" />\t\n        <log message=""Charset: {{charset.external}}"" /> \n        <to uri=""mock:out"" /> \n</route> \n\nThis fails: \n<route> \n        <from uri=""direct:in"" /> \n        <choice> \n                <when> \n                        <constant>true</constant> \n                        <convertBodyTo type=""String"" charset=""{{charset.external}}"" />\t\n                </when> \n        </choice> \n        <to uri=""mock:out"" /> \n</route> '}"
camel,bugs-dot-jar_CAMEL-3388_0919a0f6,"{'BugID': 'CAMEL-3388', 'Summary': '@OutHeaders in bean binding issue with InOnly MEP', 'Description': 'When you invoke a bean with a method signature like this in Camel 2.5.0/HEAD, the in and out message both are null (the ""Hello!"" value just disappears):\n\n{code:java}\n    public String doTest(@Body Object body, @Headers Map headers, @OutHeaders Map outHeaders) {\n        return ""Hello!"";\n    }\n{code}\n\nThe same thing without the headers works OK:\n\n{code:java}\n    public String doTest(@Body Object body) {\n        return ""Hello!"";\n    }\n{code}\nSee camel-core/src/test/java/org/apache/camel/component/bean/BeanWithHeadersAndBodyInject3Test.java'}"
camel,bugs-dot-jar_CAMEL-3394_18e1a142,"{'BugID': 'CAMEL-3394', 'Summary': 'Splitter and Multicast EIP marks exchange as exhausted to early if exception was thrown from an evaluation', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Cannot-handle-Exception-thrown-from-Splitter-Expression-tp3286043p3286043.html'}"
camel,bugs-dot-jar_CAMEL-3395_8433e6db,"{'BugID': 'CAMEL-3395', 'Summary': 'Splitter - Exchange.CORRELATION_ID should point back to parent Exchange id', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Splitted-exchange-has-incorrect-correlation-ID-tp3289354p3289354.html'}"
camel,bugs-dot-jar_CAMEL-3428_320545cd,"{'BugID': 'CAMEL-3428', 'Summary': 'DefaultCamelContext.getEndpoint(String name, Class<T> endpointType) throws Nullpointer for unknown endpoint', 'Description': ""The method getEndpoint throws an NullPointerException when it's called with an unknown endpoint name:\n\njava.lang.NullPointerException\n\tat org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:480)\n\tat org.apache.camel.impl.DefaultCamelContextTest.testGetEndPointByTypeUnknown(DefaultCamelContextTest.java:95)\n\nThe patch is attached.\n""}"
camel,bugs-dot-jar_CAMEL-3433_e76d23b0,"{'BugID': 'CAMEL-3433', 'Summary': 'Undefined header results in Nullpointer when expression is evaluated', 'Description': 'If you define a filter for a header that is not defined like\n\nfrom(""p:a"").filter(header(""header"").in(""value"")).to(""p:b"");\n\nit results in a NullPointerException:\n\n{code}\n2010-12-15 10:07:45,920 [main] ERROR DefaultErrorHandler            - \nFailed delivery for exchangeId: 0215-1237-1292404064936-0-2. \nExhausted after delivery attempt: 1 caught: java.lang.NullPointerException\n\tat org.apache.camel.builder.ExpressionBuilder$40.evaluate(ExpressionBuilder.java:955)\n\tat org.apache.camel.impl.ExpressionAdapter.evaluate(ExpressionAdapter.java:36)\n\tat org.apache.camel.builder.BinaryPredicateSupport.matches(BinaryPredicateSupport.java:54)\n\tat org.apache.camel.builder.PredicateBuilder$5.matches(PredicateBuilder.java:127)\n\tat org.apache.camel.processor.FilterProcessor.process(FilterProcessor.java:46)\n\tat org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)\n\tat org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)\n\tat org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)\n{code}\n\nThis test reproduces the problem:\n{code}\npublic void testExpressionForUndefinedHeader(){\n    Expression type = ExpressionBuilder.headerExpression(""header"");\n    Expression expression = ExpressionBuilder.constantExpression(""value"");\n    Expression convertToExpression = ExpressionBuilder.convertToExpression(expression, type);\n    convertToExpression.evaluate(exchange, Object.class);\n}\n{code}'}"
camel,bugs-dot-jar_CAMEL-3448_b345dd82,"{'BugID': 'CAMEL-3448', 'Summary': 'Route scoped onException may pick onException from another route if they are the same class type', 'Description': ""If you have a clash with route scoped onException and have the exact same class, then the key in the map isn't catering for this. And thus a 2nd route could override the 1st route onException definition.\n\nFor example:\n\nfrom X route A\n  onException IOException\n\nfrom Y route B\n  onException IOException\n\nThe map should contain 2 entries, but unfortunately it only contain 1. This only happens when its an exact type match.\n""}"
camel,bugs-dot-jar_CAMEL-3498_b4606700,"{'BugID': 'CAMEL-3498', 'Summary': 'Splitter Component: Setting \'streaming = ""true""\' breaks error handling', 'Description': 'Setting \'streaming = ""true""\' breaks error handling:\nIf an exception is thrown in a processor, the exception in the subExchange is copied to the original exchange in MulticastProcessor line 554. In Splitter line 140 the original exchange is copied, including the exception that was thrown while processing the previous exchange. This prevents all subsequent exchanges from being processed successfully.\n'}"
camel,bugs-dot-jar_CAMEL-3531_41e4b5b9,"{'BugID': 'CAMEL-3531', 'Summary': 'scala - xpath not working together with choice/when', 'Description': 'When using the Scala DSL, xpath expressions inside when() do not work as expected. As an example:\n{code:none}\n     ""direct:a"" ==> {\n     choice {\n        when (xpath(""//hello"")) to (""mock:english"")\n        when (xpath(""//hallo"")) {\n          to (""mock:dutch"")\n          to (""mock:german"")\n        } \n        otherwise to (""mock:french"")\n      }\n    }\n\n// Send messages\n""direct:a"" ! (""<hello/>"", ""<hallo/>"", ""<hellos/>"")\n{code}\n\nHere we should receive 1 message in each of the mocks. For whatever reason, all 3 messages go to mock:english. Similar routes work as expected with the Java DSL. '}"
camel,bugs-dot-jar_CAMEL-3535_b56d2962,"{'BugID': 'CAMEL-3535', 'Summary': 'Aggregation fails to call onComplete for exchanges if the aggregation is after a bean or process.', 'Description': ""When creating a route that contains an aggregation, if that aggregation is preceded by a bean or process, it will fail to call AggregateOnCompletion.onComplete(). I've attached a unit test that can show you the behavior. Trace level loggging will need to be enabled to see the difference. With the call to the bean, it won't show the following log entry:\n{noformat}TRACE org.apache.camel.processor.aggregate.AggregateProcessor - Aggregated exchange onComplete: Exchange[Message: ab]{noformat}\nIf you remove the bean call, it'll start calling onComplete() again.\n\nWhat I've noticed is that if this call is not made, it ends up in a memory leak since the inProgressCompleteExchanges HashSet in AggregateProcessor never has any exchange ID's removed.""}"
camel,bugs-dot-jar_CAMEL-3545_050c542e,"{'BugID': 'CAMEL-3545', 'Summary': ""MethodCallExpression doesn't validate whether the method exists for all cases"", 'Description': ""I tried to refactor\n\n{code:title=org.apache.camel.model.language.MethodCallExpression.java}\n    public Expression createExpression(CamelContext camelContext) {\n        Expression answer;\n\n        if (beanType != null) {            \n            instance = ObjectHelper.newInstance(beanType);\n            return new BeanExpression(instance, getMethod(), parameterType); // <--\n        } else if (instance != null) {\n            return new BeanExpression(instance, getMethod(), parameterType); // <--\n        } else {\n            String ref = beanName();\n            // if its a ref then check that the ref exists\n            BeanHolder holder = new RegistryBean(camelContext, ref);\n            // get the bean which will check that it exists\n            instance = holder.getBean();\n            answer = new BeanExpression(ref, getMethod(), parameterType);\n        }\n\n        // validate method\n        validateHasMethod(camelContext, instance, getMethod(), parameterType);\n\n        return answer;\n    }\n{code}\n\nto\n\n{code:title=org.apache.camel.model.language.MethodCallExpression.java}\n    public Expression createExpression(CamelContext camelContext) {\n        Expression answer;\n\n        if (beanType != null) {            \n            instance = ObjectHelper.newInstance(beanType);\n            answer = new BeanExpression(instance, getMethod(), parameterType); // <--\n        } else if (instance != null) {\n            answer = new BeanExpression(instance, getMethod(), parameterType); // <--\n        } else {\n            String ref = beanName();\n            // if its a ref then check that the ref exists\n            BeanHolder holder = new RegistryBean(camelContext, ref);\n            // get the bean which will check that it exists\n            instance = holder.getBean();\n            answer = new BeanExpression(ref, getMethod(), parameterType);\n        }\n\n        // validate method\n        validateHasMethod(camelContext, instance, getMethod(), parameterType);\n\n        return answer;\n    }\n{code}\n\nso that the created BeanExpression is also validate if you provide the bean type or an instance. With this change, some tests in org.apache.camel.language.SimpleTest fails.\nI'm not sure whether the tests are faulty or if it's a bug.\nAlso not sure whether this should fixed in 2.6. ""}"
camel,bugs-dot-jar_CAMEL-3617_02626724,"{'BugID': 'CAMEL-3617', 'Summary': 'Inconsistent filename value when move attribute is used with File component', 'Description': 'Unless I miss a point, when I use the following endpoint, the file:name value is incorrect and is equal to file:absolute.path\n\n<endpoint id=""fileEndpoint"" uri=""file:${queue.input.folder}?recursive=true&amp;include=.*\\.dat&amp;move=${queue.done.folder}/$simple{file:name}&amp;moveFailed=${queue.failed.folder}/$simple{file:name}"" />\n\n${queue.input.folder}, ${queue.done.folder} and ${queue.failed.folder} are absolute paths resolved by Spring.\n\nIn fact, Camel tries to move the file to ${queue.done.folder}/${queue.input.folder}/$simple{file:name}\nI\'ve also tried using $simple{header.CamelFileName} instead of $simple{file:name} and it gives the same result.\n\nFor now, I\'ve found a workaround using a processor which put the CamelFileName header value into a ""destFile"" property \n<endpoint id=""fileEndpoint"" uri=""file:${queue.input.folder}?recursive=true&amp;include=.*\\.dat&amp;move=${queue.done.folder}/$simple{property.destFile}&amp;moveFailed=${queue.failed.folder}/$simple{property.destFile}"" />\n'}"
camel,bugs-dot-jar_CAMEL-3690_2a3f3392,"{'BugID': 'CAMEL-3690', 'Summary': 'Endpoints may be shutdown twice as they are tracked in two lists in CamelContext', 'Description': 'Endpoint is a Service which means they are listed in both a endpoint and service list. They should only be listed in the endpoint list.\n\nThis avoids issues with endpoints may be shutdown twice when Camel shutdown.\n\nSee nabble\nhttp://camel.465427.n5.nabble.com/QuartzComponent-do-not-delete-quartz-worker-threads-when-shutdown-Camel-tp3393728p3393728.html'}"
camel,bugs-dot-jar_CAMEL-3709_4c37e773,"{'BugID': 'CAMEL-3709', 'Summary': ""interceptFrom and from(Endpoint) don't work together"", 'Description': 'When using interceptFrom(String) together with from(Endpoint), the below Exception occurs during the routes building process. Looking at RoutesDefinition.java:217 reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms\' URIs to fail. As far as I can tell, the way to fix this would be to add {{setUri(myEndpoint.getEndpointUri())}} in the constructor {{FromDefinition(Endpoint endpoint)}}.\n\nBelow the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to {{from(""myEndpoint"")}}.\n{code}\norg.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: null due to: null\n\tat org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:109)\n\tat org.apache.camel.model.RoutesDefinition.route(RoutesDefinition.java:217)\n\tat org.apache.camel.model.RoutesDefinition.from(RoutesDefinition.java:167)\n\tat org.apache.camel.builder.RouteBuilder.from(RouteBuilder.java:101)\n\tat dk.mobilethink.adc2.endpoint.UnsetUriTest$1.configure(UnsetUriTest.java:18)\n\tat org.apache.camel.builder.RouteBuilder.checkInitialized(RouteBuilder.java:318)\n\tat org.apache.camel.builder.RouteBuilder.configureRoutes(RouteBuilder.java:273)\n\tat org.apache.camel.builder.RouteBuilder.addRoutesToCamelContext(RouteBuilder.java:259)\n\tat org.apache.camel.impl.DefaultCamelContext.addRoutes(DefaultCamelContext.java:612)\n\tat org.apache.camel.test.CamelTestSupport.setUp(CamelTestSupport.java:111)\n\tat junit.framework.TestCase.runBare(TestCase.java:132)\n\tat org.apache.camel.test.TestSupport.runBare(TestSupport.java:65)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:124)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:232)\n\tat junit.framework.TestSuite.run(TestSuite.java:227)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nCaused by: java.lang.NullPointerException\n\tat org.apache.camel.util.UnsafeUriCharactersEncoder.encode(UnsafeUriCharactersEncoder.java:56)\n\tat org.apache.camel.util.URISupport.normalizeUri(URISupport.java:162)\n\tat org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:107)\n\t... 24 more\n{code}\n\n{code}\npackage dk.mobilethink.adc2.endpoint;\n\nimport org.apache.camel.Endpoint;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.test.CamelTestSupport;\n\npublic class UnsetUriTest extends CamelTestSupport {\n\t@Override\n\tprotected RouteBuilder createRouteBuilder() throws Exception {\n\n\t\treturn new RouteBuilder() {\n\t\t\tpublic void configure() throws Exception {\n\t\t\t\tinterceptFrom(""URI1"").to(""irrelevantURI"");\n\n\t\t\t\tEndpoint myEndpoint = getContext().getComponent(""direct"").createEndpoint(""ignoredURI"");\n\t\t\t\t\n//\t\t\t\tgetContext().addEndpoint(""myEndpoint"", myEndpoint);\n\t\t\t\tfrom(myEndpoint)\n\t\t\t\t\t.inOnly(""log:foo"");\n\t\t\t}\n\t\t};\n\t}\n\n\tpublic void testNothing() { }\n}\n{code}\n'}"
camel,bugs-dot-jar_CAMEL-3727_ff2713d1,"{'BugID': 'CAMEL-3727', 'Summary': ""Recipient list with parallel processing doesn't reuse aggregation threads"", 'Description': 'When I\'m using recipient list in parallel mode {{aggregateExecutorService}} in {{MulticastProcessor}} doesn\'t reuse threads and is creating one new thread per each request.\n\nTo reproduce this bug simply add a loop to {{RecipientListParallelTest.testRecipientListParallel()}} test:\n{code:title=RecipientListParallelTest.java|borderStyle=solid}\n    public void testRecipientListParallel() throws Exception {\n        for (int i = 0; i < 10000; i++) {\n            MockEndpoint mock = getMockEndpoint(""mock:result"");\n            mock.reset();\n            mock.expectedBodiesReceivedInAnyOrder(""c"", ""b"", ""a"");\n            template.sendBodyAndHeader(""direct:start"", ""Hello World"", ""foo"", ""direct:a,direct:b,direct:c"");\n            assertMockEndpointsSatisfied();\n        }\n    }\n{code}\n\nIn the logs you can find:\n{code}\n2011-02-28 13:22:30,984 [) thread #0 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:31,984 [) thread #4 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:32,984 [) thread #8 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:34,000 [ thread #12 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:35,000 [ thread #14 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:36,000 [ thread #15 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:37,015 [ thread #16 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n2011-02-28 13:22:38,015 [ thread #17 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.\n{code}\n'}"
camel,bugs-dot-jar_CAMEL-3757_c1b2f2f8,"{'BugID': 'CAMEL-3757', 'Summary': 'Auto mock endpoints should strip parameters to avoid confusing when accessing the mocked endpoint', 'Description': 'If you use mocking existing endpoints, which is detailed here\nhttp://camel.apache.org/mock.html\n\nWe should stip parameters of the mocked endpoint, eg {{file:xxxx?noop=true}}. eg so the mocked endpoint would be {{mock:file:xxxx}} without any of the parameters.\n\nOtherwise the mock endpoint expects those parameters is part of the mock endpoint and will fail creating the mock endpoint.'}"
camel,bugs-dot-jar_CAMEL-3760_5225e6e3,"{'BugID': 'CAMEL-3760', 'Summary': 'ManagementNamingStrategy - Should normalize ObjectName to avoid using illegal characters', 'Description': ""For example when using JMS in the loanbroaker example. There us a colon in the JMS queue name which is invalid char in JMX.\n\n2011-03-06 08:26:55,859 [main           ] WARN  ManagedManagementStrategy      - Cannot check whether the managed object is registered. This exception will be ignored.\njavax.management.MalformedObjectNameException: Could not create ObjectName from: org.apache.camel:context=vesta.apache.org/camel-1,type=threadpools,name=JmsReplyManagerTimeoutChecker[queue2:parallelLoanRequestQueue]. Reason: javax.management.MalformedObjectNameException: Invalid character ':' in value part of property\n\tat org.apache.camel.management.DefaultManagementNamingStrategy.createObjectName(DefaultManagementNamingStrategy.java:315)[camel-core-2.7-SNAPSHOT.jar:2.7-SNAPSHOT]\n\n\n""}"
camel,bugs-dot-jar_CAMEL-3789_9319e139,"{'BugID': 'CAMEL-3789', 'Summary': 'org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy is not thread-safe', 'Description': 'MarkerFileExclusiveReadLockStrategy is not thread-safe. When I run  a File endpoint with more than one thread the MarkerFileExclusiveReadLockStrategy only deletes the last file to start being processed. \n\nThe MarkerFileExclusiveReadLockStrategy uses global variables: \nprivate File lock; \nprivate String lockFileName; \nand gives them values on the acquireExclusiveReadLock method. When another thread calls the releaseExclusiveReadLock method it uses the global variables to delete the locked file. That means that if another thread came and called the acquireExclusiveReadLock it would have changed the values on the global variables. \n\nIf lock and lockFileName are not global variables the problem seems to disappear and I can a multithreaded File endpoint and not locked file is left undeleted. \n'}"
camel,bugs-dot-jar_CAMEL-3791_52106681,"{'BugID': 'CAMEL-3791', 'Summary': 'Camel should reset the stream cache if the useOriginalInMessage option is true', 'Description': '{code}\n--- src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java\t(revision 1083672)\n+++ src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java\t(working copy)\n@@ -591,18 +591,23 @@\n         // is the a failure processor to process the Exchange\n         if (processor != null) {\n \n-            // reset cached streams so they can be read again\n-            MessageHelper.resetStreamCache(exchange.getIn());\n-\n             // prepare original IN body if it should be moved instead of current body\n             if (data.useOriginalInMessage) {\n                 if (log.isTraceEnabled()) {\n                     log.trace(""Using the original IN message instead of current"");\n                 }\n                 Message original = exchange.getUnitOfWork().getOriginalInMessage();\n                 exchange.setIn(original);\n             }\n\n+            // reset cached streams so they can be read again\n+            MessageHelper.resetStreamCache(exchange.getIn());\n{code}'}"
camel,bugs-dot-jar_CAMEL-3847_de9399f3,"{'BugID': 'CAMEL-3847', 'Summary': 'Adding type converter should clear misses map for the given type', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/addTypeConverter-does-not-clear-misses-in-BaseTypeConverterRegistry-tp4288871p4288871.html'}"
camel,bugs-dot-jar_CAMEL-3878_b9094cb5,"{'BugID': 'CAMEL-3878', 'Summary': 'Stopping a route should not stop context scoped error handler', 'Description': 'When stopping a route using .stopRoute from CamelContext or JMX etc. then the error handler should not be stopped if its a context scoped error handler, as it would be re-used.\n\nWe should defer stopping those resources till Camel is shutting down.'}"
camel,bugs-dot-jar_CAMEL-4011_cbffff59,"{'BugID': 'CAMEL-4011', 'Summary': 'type converters should return NULL for Double.NaN values instead of 0', 'Description': 'see this discussion...http://camel.465427.n5.nabble.com/XPath-for-an-Integer-td4422095.html\n\nUpdate the ObjectConverter.toXXX() methods to check for Double.NaN and return NULL instead of relying on Number.intValue()\n'}"
camel,bugs-dot-jar_CAMEL-4211_4efddb3f,"{'BugID': 'CAMEL-4211', 'Summary': 'URISupport - Normalize URI should support parameters with same key', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Problems-with-jetty-component-and-posts-with-more-then-one-value-for-a-field-tp4576908p4576908.html\n\nThe end user is using jetty producer component to send a HTTP POST/GET to some external client. In the endpoint uri he have the parameters, and there are 2 times {{to}} as parameter key. Currently Camel loses the 2nd {{to}} parameter. '}"
camel,bugs-dot-jar_CAMEL-4354_96e40c3c,"{'BugID': 'CAMEL-4354', 'Summary': 'header added using an EventNotifier is not present at AggregationStrategy for http endpoints', 'Description': 'A new header added using an EventNotifier is not present when the exchange is aggregated with an AggregationStrategy.\nThis is happening only if the enpoint type is http, ftp doesn\'t have this issue.\n\nThis was working with an early version of 2.8.0-SNAPSHOT\n\nFollowing the EventNotifier code used.\n\n{code:title=ExchangeSentEventNotifier.java|borderStyle=solid}\npublic class ExchangeSentEventNotifier extends EventNotifierSupport {\n\n\t@Override\n\tprotected void doStart() throws Exception {\n        /*\n         *  filter out unwanted events\n         *  we are interested only in ExchangeSentEvent\n         */\n        setIgnoreCamelContextEvents(true);\n        setIgnoreServiceEvents(true);\n        setIgnoreRouteEvents(true);\n        setIgnoreExchangeCreatedEvent(true);\n        setIgnoreExchangeCompletedEvent(true);\n        setIgnoreExchangeFailedEvents(true);\n        setIgnoreExchangeSentEvents(false);\t\t\n\t}\n\n\t@Override\n\tprotected void doStop() throws Exception {\n\n\t}\n\n\t@Override\n\tpublic boolean isEnabled(EventObject event) {\n\t\treturn event instanceof ExchangeSentEvent;\n\t}\n\n\t@Override\n\tpublic void notify(EventObject event) throws Exception {\n    \tif(event.getClass() == ExchangeSentEvent.class){\n            ExchangeSentEvent eventSent = (ExchangeSentEvent)event;\n            \n            log.debug(""Took "" + eventSent.getTimeTaken() + "" millis to send to: "" + eventSent.getEndpoint());\n\n            //storing time taken to the custom header            \n            eventSent.getExchange().getIn().setHeader(""x-time-taken"", eventSent.getTimeTaken());\n            \n    \t}\n\t\t\n\t}\n\n}\n{code} '}"
camel,bugs-dot-jar_CAMEL-4370_7345fefc,"{'BugID': 'CAMEL-4370', 'Summary': ""It's hardly possible to use all expression of the Simple language to create file names in the file component"", 'Description': 'Sometimes it can be necessary to use custom headers to create a file name.\n\nFor example, I declare my file endpoint in the following manner:\n\n{code}\n<route id=""fileReader"">\n    <from uri=""file://rootFolder?move=.backup&amp;moveFailed=.error/${header.CustomHeader}"" />\n    <to uri=""file://out""/>\n</route>\n{code}\n\nThe header ""CustomHeader"" cannot be read because of the following snippets of code in the org.apache.camel.component.file.GenericFile\n\n{code}\n/**\n * Bind this GenericFile to an Exchange\n */\npublic void bindToExchange(Exchange exchange) {\n    exchange.setProperty(FileComponent.FILE_EXCHANGE_FILE, this);\n    GenericFileMessage<T> in = new GenericFileMessage<T>(this);\n    exchange.setIn(in);\n    populateHeaders(in);\n}\n\n/**\n * Populates the {@link GenericFileMessage} relevant headers\n *\n * @param message the message to populate with headers\n */\npublic void populateHeaders(GenericFileMessage<T> message) {\n    if (message != null) {\n        message.setHeader(Exchange.FILE_NAME_ONLY, getFileNameOnly());\n        message.setHeader(Exchange.FILE_NAME, getFileName());\n        message.setHeader(""CamelFileAbsolute"", isAbsolute());\n        message.setHeader(""CamelFileAbsolutePath"", getAbsoluteFilePath());\n\n        if (isAbsolute()) {\n            message.setHeader(Exchange.FILE_PATH, getAbsoluteFilePath());\n        } else {\n            // we must normalize path according to protocol if we build our own paths\n            String path = normalizePathToProtocol(getEndpointPath() + File.separator + getRelativeFilePath());\n            message.setHeader(Exchange.FILE_PATH, path);\n        }\n\n        message.setHeader(""CamelFileRelativePath"", getRelativeFilePath());\n        message.setHeader(Exchange.FILE_PARENT, getParent());\n\n        if (getFileLength() >= 0) {\n            message.setHeader(""CamelFileLength"", getFileLength());\n        }\n        if (getLastModified() > 0) {\n            message.setHeader(Exchange.FILE_LAST_MODIFIED, new Date(getLastModified()));\n        }\n    }\n}\n{code}\n\nAs you can see a new ""in"" message is created and not all the headers from the original message are copied to it.'}"
camel,bugs-dot-jar_CAMEL-4388_f39bc60d,"{'BugID': 'CAMEL-4388', 'Summary': 'Exeptions cannot be propagated to the parent route when using LogEIP', 'Description': 'Here is unit test that demonstrates the problem.\nFor the unit test pass successfully it\'s necessary to delete LogEIP from the route.\n\n{code}\npackage org.apache.camel.impl;\n\nimport org.apache.camel.Exchange;\nimport org.apache.camel.Processor;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.test.junit4.CamelTestSupport;\nimport org.junit.Test;\n\npublic class PropagateExceptionTest extends CamelTestSupport {\n\n    @Test\n    public void failure() throws Exception {\n        getMockEndpoint(""mock:handleFailure"").whenAnyExchangeReceived(new Processor() {\n            @Override\n            public void process(Exchange exchange) throws Exception {\n                throw new RuntimeException(""TEST EXCEPTION"");\n            }\n        });\n\n        getMockEndpoint(""mock:exceptionFailure"").expectedMessageCount(1);\n        sendBody(""direct:startFailure"", ""Hello World"");\n        assertMockEndpointsSatisfied();\n    }\n\n    @Test\n    public void success() throws Exception {\n        getMockEndpoint(""mock:handleSuccess"").whenAnyExchangeReceived(new Processor() {\n            @Override\n            public void process(Exchange exchange) throws Exception {\n                throw new RuntimeException(""TEST EXCEPTION"");\n            }\n        });\n\n        getMockEndpoint(""mock:exceptionSuccess"").expectedMessageCount(1);\n        sendBody(""direct:startSuccess"", ""Hello World"");\n        assertMockEndpointsSatisfied();\n    }\n\n    @Override\n    protected RouteBuilder[] createRouteBuilders() throws Exception {\n        return new RouteBuilder[] {\n                new RouteBuilder() {\n                    public void configure() throws Exception {\n                        from(""direct:startFailure"")\n                            .onException(Throwable.class)\n                                .to(""mock:exceptionFailure"")\n                                .end()\n                            .to(""direct:handleFailure"")\n                            .to(""mock:resultFailure"");\n\n                        from(""direct:handleFailure"")\n                            .errorHandler(noErrorHandler())\n                            .log(""FAULTY LOG"")\n                            .to(""mock:handleFailure"");\n                    }\n                },\n\n                new RouteBuilder() {\n                    public void configure() throws Exception {\n                        from(""direct:startSuccess"")\n                            .onException(Throwable.class)\n                                .to(""mock:exceptionSuccess"")\n                                .end()\n                            .to(""direct:handleSuccess"")\n                            .to(""mock:resultSuccess"");\n\n                        from(""direct:handleSuccess"")\n                            .errorHandler(noErrorHandler())\n                            .to(""mock:handleSuccess"");\n                    }\n                }\n        };\n    }\n}\n{code}'}"
camel,bugs-dot-jar_CAMEL-4467_79168a23,"{'BugID': 'CAMEL-4467', 'Summary': 'LifecycleStrategy should be started/stopped when CamelContext is starting/stopping', 'Description': 'The LifecycleStrategy strategies is not start/stopped if they are a Service, such as the DefaultManagementLifecycleStrategy'}"
camel,bugs-dot-jar_CAMEL-4474_06a8489a,"{'BugID': 'CAMEL-4474', 'Summary': 'file: consumer does not create directory', 'Description': ""According to http://camel.apache.org/file2.html autoCreate is true by default and should for a consumer create the directory.\n{noformat}\nautoCreate \ttrue \tAutomatically create missing directories in the file's pathname. For the file consumer, that means creating the starting directory. For the file producer, it means the directory the files should be written to. \n{noformat}\nThis does not happen and thus a route startup would fail.""}"
camel,bugs-dot-jar_CAMEL-4482_e38494f1,"{'BugID': 'CAMEL-4482', 'Summary': 'Using custom expression in Splitter EIP which throws exception, is not triggering onException', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Global-exception-not-invoked-in-case-of-Exception-fired-while-iterating-through-File-Splitter-td4826097.html\n\nWe should detect exceptions occurred during evaluation of the expression, and then cause the splitter EIP to fail asap.'}"
camel,bugs-dot-jar_CAMEL-4486_f98ac676,"{'BugID': 'CAMEL-4486', 'Summary': 'Exceptions are not propagated to the parent route when endpoint cannot be resolved in the RoutingSlip EIP', 'Description': 'Here is the unit test to reproduce the issue\n\n{code}\npackage org.test;\n\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.component.mock.MockEndpoint;\nimport org.apache.camel.test.junit4.CamelTestSupport;\nimport org.junit.Test;\n\npublic class RecipientListTest extends CamelTestSupport {\n\n    public static class Router {\n        public String findEndpoint() {\n            return ""unresolved://endpoint"";\n        }\n    }\n\n    @Test\n    public void recipientList() throws Exception {\n        MockEndpoint endpoint = getMockEndpoint(""mock://error"");\n        endpoint.expectedMessageCount(1);\n\n        sendBody(""direct://parent"", ""Hello World!"");\n\n        assertMockEndpointsSatisfied();\n    }\n\n    @Override\n    protected RouteBuilder createRouteBuilder() throws Exception {\n        return new RouteBuilder() {\n            @Override\n            public void configure() throws Exception {\n                from(""direct://parent"")\n                    .onException(Throwable.class)\n                        .to(""mock://error"")\n                    .end()\n                    .to(""direct://child"");\n\n                from(""direct://child"")\n                    .errorHandler(noErrorHandler())\n                    .routingSlip(bean(Router.class));\n            }\n        };\n    }\n\n}\n{code}'}"
camel,bugs-dot-jar_CAMEL-4509_8e3450f4,"{'BugID': 'CAMEL-4509', 'Summary': 'Header not set after dead letter queue handles unmarshal error', 'Description': 'We have a route which unmarshals a soap msg into an object.  On that route is a dead letter queue error handler.  That DLQ sets headers on the message used later for error reporting.\n\nIf the error is thrown by the marshaller, the *first header* that we try to set is wiped out.  The 2nd header is set with no problem.  If an error is thrown by something other than the marshaller, the correct headers are set.\n\nSee attached project with failed test case (canSetHeadersOnBadXmlDeadLetter)'}"
camel,bugs-dot-jar_CAMEL-4513_9e05f77f,"{'BugID': 'CAMEL-4513', 'Summary': 'simple predicate fails to introspect the exception in an onException clause using onWhen', 'Description': 'The bug occured in the 2.6.0 version of Camel I\'m using. I haven\'t test it against the latest version but I\'ve checked the sources and it doesn\'t seem to have change since.\n\nGiven a camel route, with a onException clause like this :\n\n{code}\nthis.onException(MyException.class)\n    .onWhen(simple(""${exception.myExceptionInfo.aValue} == true""))\n    ...\n{code}\n\nMyException is a customed exception like this :\n\n{code:title=MyException.java}\npublic class MyException extends Exception {\n   ....\n   public MyExceptionInfo getMyExceptionInfo() {\n     ...\n   }\n}\n{code}\n\nWhat I\'ve observed is that when BeanExpression.OgnlInvokeProcessor.process iterate through the methods to calls, it does :\n{code}\n                // only invoke if we have a method name to use to invoke\n                if (methodName != null) {\n                    InvokeProcessor invoke = new InvokeProcessor(holder, methodName);\n                    invoke.process(resultExchange);\n\n                    // check for exception and rethrow if we failed\n                    if (resultExchange.getException() != null) {\n                        throw new RuntimeBeanExpressionException(exchange, beanName, methodName, resultExchange.getException());\n                    }\n\n                    result = invoke.getResult();\n                }\n{code}\n\nIt successfully invoke the method : invoke.process(resultExchange);\nBut it checks for exception in the exchange. Since we are in an exception clause, there is an actual exception (thrown by the application, but unrelated with the expression language search) and it fails\n\nThere is a simple workaround for that : writing his own predicate class to test wanted conditions'}"
camel,bugs-dot-jar_CAMEL-4536_df9f4a6a,"{'BugID': 'CAMEL-4536', 'Summary': 'Using AuthorizationPolicy on a Route prevents Processors from being exposed via JMX', 'Description': 'Using AuthorizationPolicy on a route (e.g., using .policy(myAuthPolicy) in a Java DSL) prevents that processors on this route are exposed via JMX. \n\nSteps to reproduce:\n\n-) Start the Camel app in the attached test case (MyRouteBuilder)\n-) Open JConsole\n-) Connect to the corresponding local process\n-) Under ""processors"" only the processors from the route without the policy are shown, but not the ones from the route where a policy is used'}"
camel,bugs-dot-jar_CAMEL-4542_c408c3ed,"{'BugID': 'CAMEL-4542', 'Summary': 'Can\'t find splitter bean in registry using multiple camel contexts with ""vm"" endpoint', 'Description': 'The splitter component can use a bean with a ""split method"". It seems that this ""split bean"" is handled as expression and resolved lately using Camel Context from current exchange.\n\nIf I send an exchange using a separate CamelContext (""client"")\n\n<camelContext id=""client"" xmlns=""http://camel.apache.org/schema/spring"">\n</camelContext>\n\nto a route defined in another CamelContext (""server"") using in-memory transport like ""direct"" or ""vm""\n\n<camelContext id=""server"" xmlns=""http://camel.apache.org/schema/spring"">\n\n   <route id=""route02"" trace=""false"" streamCache=""false"">\n     <from uri=""vm:route02""/>\n     <split>\n       <method bean   =""stringLineSplitter"" method=""split""/>\n       <log    message=""before sending: ${body}""/>\n       <inOut  uri    =""vm:route04""/>\n       <log    message=""after sending""/>\n     </split>\n     <to uri=""mock:route02""/>\n   </route>\n\n</camelContext>\n\nthe test fails with \n\n""Cannot find class: stringLineSplitter"" (Camel 2.8.0). \n""org.apache.camel.NoSuchBeanException - No bean could be found in the registry for: stringLineSplitter"" (Camel 2.9-SNAPSHOT)\n\nIf I understood Camel right it fails\nbecause it tries to resolve this bean based on client Camel Context\nwhich is still set at the current exchange send from ""client"" to ""server"" but it\ndoesn\'t contain the bean.\n\nIf I send an exchange using same ""client"" CamelContext to another route in\n""server"" CamelContext involving ""external"" components like ""jms"" (ActiveMQ)\n\n<camelContext id=""server"" xmlns=""http://camel.apache.org/schema/spring"">\n\n   <route id=""route03"" trace=""false"" streamCache=""false"">\n     <from uri=""jms:queue:route03""/>\n     <split>\n       <method bean   =""stringLineSplitter"" method=""split""/>\n       <log    message=""before sending: ${body}""/>\n       <inOut  uri    =""vm:route04""/>\n       <log    message=""after sending""/>\n     </split>\n     <to uri=""mock:route03""/>\n   </route>\n\n</camelContext>\n\nthe test passed successfully. It seems that ""jms"" component creates a\nnew exchange using ""server"" CamelContext.\n'}"
camel,bugs-dot-jar_CAMEL-4682_1e54865c,"{'BugID': 'CAMEL-4682', 'Summary': 'When stopping CamelContext should not clear lifecycleStrategies, to make restart safely possible', 'Description': 'We should not clear the lifecycleStrategies on CamelContext when stop() is invoked, as if we restart by invoking start(), the lifecycle strategies should be in use again.'}"
camel,bugs-dot-jar_CAMEL-5137_afa1d132,"{'BugID': 'CAMEL-5137', 'Summary': 'Timer component does not suspend', 'Description': 'A route which begins with a Timer consumer does not suspend the consumer when the route is suspended.'}"
camel,bugs-dot-jar_CAMEL-5140_8898d491,"{'BugID': 'CAMEL-5140', 'Summary': 'bean component - @Handler should take precedence in a bean that implements Predicate', 'Description': 'If you use a bean in a Camel route, and have not specified the method name to invoke. Then Camel has to scan for suitable methods to use. And for that we have the @Handler annotation which should take precedence in this process. However if the bean implements Predicate, or Processor, then Camel will use that. However the @Handler should be used instead, as this is what the end-user expects. And also what we tell in the docs.'}"
camel,bugs-dot-jar_CAMEL-5154_a8586a69,"{'BugID': 'CAMEL-5154', 'Summary': 'Simple language - OGNL - Invoking explicit method with no parameters should not cause ambiguous exception for overloaded methods', 'Description': 'If you want to invoke a method on a bean which is overloaded, such as a String with toUpperCase having\n- toUpperCase()\n- toUpperCase(Locale)\n\nThen if you specify this in a simple ognl expression as follows\n{code}\n${body.toUpperCase()}\n{code}\n\nThen Camel bean component should pick the no-parameter method as specified.\n'}"
camel,bugs-dot-jar_CAMEL-5187_8cadc344,"{'BugID': 'CAMEL-5187', 'Summary': 'JMX issues on WebSphere', 'Description': 'While setting up a Camel web application for WebSphere (7) I encountered two issues\n\n1. Documentation: the Camel JMX docs proposes the following settings for WebSphere:\n{code}\n<camel:jmxAgent id=""agent"" createConnector=""true"" mbeanObjectDomainName=""org.yourname"" mbeanServerDefaultDomain=""WebSphere""/>\n{code}\n\nThis registers the beans with the PlatformMbeanServer instead of the WebSphere MBean server. The following setup works better:\n{code}\n<camel:jmxAgent id=""agent"" createConnector=""false"" mbeanObjectDomainName=""org.yourname"" usePlatformMBeanServer=""false"" mbeanServerDefaultDomain=""WebSphere""/>\n{code}\n\n2. For each Camel route, the same Tracer and DefaultErrorHandler MBeans are tried to be registered over and over again. Because WebSphere changes the ObjectNames on registration, \n\n{{server.isRegistered(name);}} in {{DefaultManagementAgent#registerMBeanWithServer}} always returns false, which causes the MBean to be re-registered, which again cause Exceptions, e.g.\n\n{code}\n14:35:48,198 [WebContainer : 4] [] WARN  - DefaultManagementLifecycleStrategy.onErrorHandlerAdd(485) | Could not register error handler builder: ErrorHandlerBuilderRef[CamelDefaultErrorHandlerBuilder] as ErrorHandler MBean.\njavax.management.InstanceAlreadyExistsException: org.apache.camel:cell=wdf-lap-0319Node01Cell,name=""DefaultErrorHandlerBuilder(ref:CamelDefaultErrorHandlerBuilder)"",context=wdf-lap-0319/camelContext,type=errorhandlers,node=wdf-lap-0319Node01,process=server1\n\tat com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:465)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.internal_addObject(DefaultMBeanServerInterceptor.java:1496)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:975)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:929)\n\tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)\n\tat com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:494)\n\tat com.ibm.ws.management.PlatformMBeanServer.registerMBean(PlatformMBeanServer.java:484)\n\tat org.apache.camel.management.DefaultManagementAgent.registerMBeanWithServer(DefaultManagementAgent.java:320)\n\tat org.apache.camel.management.DefaultManagementAgent.register(DefaultManagementAgent.java:236)\n...\n{code}\n\nThe web application starts up, but with a lot of exceptions in the log.\n\nProposal:\nInstead of using a Set<ObjectName> for mbeansRegistered, use a Map<ObjectName, ObjectName> where the key is the ""Camel"" ObjectName and the value is the actually deployed ObjectName.\n\nI will provide a patch that illustrates the idea.\n'}"
camel,bugs-dot-jar_CAMEL-5215_033eb6fe,"{'BugID': 'CAMEL-5215', 'Summary': 'The file producer should use the charset encoding when writing the file if configured', 'Description': 'When writing to a file, we offer the charset option on the endpoint, as well the charset property set on the exchange.\nHowever in a route  that is optimized as\n{code}\nfrom file\n to file\n{code}\n\nThen we optimize to do a file move operation instead. We should detect the charset configured and then we would need to stream and write using the configured charset.'}"
camel,bugs-dot-jar_CAMEL-5224_2db5570f,"{'BugID': 'CAMEL-5224', 'Summary': 'The done file got deleted, when using the file component even if noop property set to true', 'Description': ""We are consuming a feed from a mounted windows network drive, where we have rw access.\nDuring the download we shouldn't touch anything so other users see the directory intact.\n\nHowever even if we turn noop=true the done file got deleted after successfull conumptions\n""}"
camel,bugs-dot-jar_CAMEL-5261_55c2e2d8,"{'BugID': 'CAMEL-5261', 'Summary': 'SEDA/VM requires completely same URI on producer and consumer side when consumer route is adviced', 'Description': 'The producer side and consumer side of the SEDA (and VM) component seems to require the completely same URI to be able to communicate. Completely same meaning that all URI options must be the same on both sides. The strange thing is that this only is required when I have adviced the consumer route. 2.9.0 does not have this problem.\n\nAttached a unit test - the producerWithDifferentUri will fail on 2.9.1 and 2.9.2. If the advice is removed it will not.'}"
camel,bugs-dot-jar_CAMEL-5357_4cf7e80e,"{'BugID': 'CAMEL-5357', 'Summary': 'URI normalization - Should detect already percent encoded values', 'Description': 'If an uri has a percent encoded value, eg using %20, %25 etc, then the normalization logic in Camel should detect this and keep the value as is.\n\nCurrently it would end up double encoding %25, that becomes %2525, and so forth.\n\nIts the code in UnsafeUriCharactersEncoder that has the bug'}"
camel,bugs-dot-jar_CAMEL-5432_93935780,"{'BugID': 'CAMEL-5432', 'Summary': 'Dynamically added SEDA-route is not working', 'Description': 'Dynamically removing and adding a SEDA-route creates a not working route in Camel 2.10.0.\nIt is working in 2.9.2.\n\nTest-Code:\n{code}\npublic class DynamicRouteTest extends CamelTestSupport {\n\n    @Override\n    protected RouteBuilder createRouteBuilder() throws Exception {\n        return new RouteBuilder() {\n\n            @Override\n            public void configure() throws Exception {\n                from(""seda:in"").id(""sedaToMock"").to(""mock:out"");\n            }\n        };\n    }\n    \n    @Test\n    public void testDynamicRoute() throws Exception {\n        MockEndpoint out = getMockEndpoint(""mock:out"");\n        out.expectedMessageCount(1);\n        \n        template.sendBody(""seda:in"", ""Test Message"");\n        \n        out.assertIsSatisfied();\n        \n        CamelContext camelContext = out.getCamelContext();\n        camelContext.stopRoute(""sedaToMock"");\n        camelContext.removeRoute(""sedaToMock"");\n        \n        camelContext.addRoutes(createRouteBuilder());\n        out.reset();\n        out.expectedMessageCount(1);\n        \n        template.sendBody(""seda:in"", ""Test Message"");\n        \n        out.assertIsSatisfied();\n        \n    }\n} \n\n{code}'}"
camel,bugs-dot-jar_CAMEL-5437_da05f5aa,"{'BugID': 'CAMEL-5437', 'Summary': ""Add support for batch consumer's empty messages to aggregator"", 'Description': 'Aggregator supports completion based on the batch consumer data (option completionFromBatchConsumer)\n\nSome batch consumers (eg. File) can send an empty message if there is no input (option sendEmptyMessageWhenIdle for File consumer).\n\nAggregator is unable to handle such messages properly - the messages are aggregated, but Aggregator never completes.\n\n\nHere is the relevant fragment from AggregateProcessor.isCompleted(String,\nExchange)\n\nint size = exchange.getProperty(Exchange.BATCH_SIZE, 0, Integer.class);\nif (size > 0 && batchConsumerCounter.intValue() >= size) {\n    ....\n}\n\n\nPlease add support for this combination of options.'}"
camel,bugs-dot-jar_CAMEL-5515_b3bb8670,"{'BugID': 'CAMEL-5515', 'Summary': ""thread java DSL doesn't provide full function out of box"", 'Description': ""\nthread() doesn't has extra parameter for setting the thread name, and the other thread() method doesn't add the output process rightly.""}"
camel,bugs-dot-jar_CAMEL-5570_a57830ed,"{'BugID': 'CAMEL-5570', 'Summary': 'maximumRedeliveries is inherited for other exceptions thrown while redelivering with maximumRedeliveries(-1)', 'Description': 'Given a route:\n\n{code}\nfrom(""direct:source"")\n   .onException(FirstException.class)\n         .handled(true)\n         .maximumRedeliveries(-1)\n    .end()\n    .onException(SecondException.class)\n        .handled(true)\n        .to(""direct:error"")\n    .end()\n    .to(""direct:destination"");\n{code}\n\nIf the consumer of direct:destination throws a FirstException, the message will be redelivered. Now if a SecondException is thrown while redelivering the message to direct:destination, it does NOT go to direct:error, as you would expect, but is redelivered again; using the same RedeliveryPolicy as for FirstException.\n\nI have attached a test that illustrates this.\n\nIn OnExceptionDefinition.createRedeliveryPolicy, maximumRedeliveries is set to 0 if the OnExceptionDefinition has outputs and the parent RedeliveryPolicy has explicitly set maximumRedeliveries > 0. The latter check fails when maximumRedeliveries is -1 (infinite retries), and the parent RedeliveryPolicy is returned.\n\nI have attached a patch that ensures that we don\'t inherit the parent maximumRedeliveries even if it is set to -1.'}"
camel,bugs-dot-jar_CAMEL-5571_0e87b84f,"{'BugID': 'CAMEL-5571', 'Summary': 'Camel proxies should not forward hashCode() method invocations', 'Description': 'Given a Camel proxy for an @InOnly service interface, and a route from the proxy to a JMS endpoint, calling hashCode() on the proxy throws an exception, either immediately or after a number of retries, depending on the route configuration.\n\nSee the attached test case for different scenarios.\n\nThe reason is that hashCode() is forwarded by the CamelInvocationHandler to the remote endpoint, which does not make sense in this case.\n'}"
camel,bugs-dot-jar_CAMEL-5644_15d0fd9b,"{'BugID': 'CAMEL-5644', 'Summary': 'Bean component - Should use try conversion when choosing method based on parameter type matching', 'Description': 'When the bean component has to pick among overloaded methods, then it does best matching on parameter types etc.\n\nWe should relax the type conversion to try attempt.\n'}"
camel,bugs-dot-jar_CAMEL-5681_78c73502,"{'BugID': 'CAMEL-5681', 'Summary': 'Using recipient list in a doTry ... doCatch situation dont work properly', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Issue-with-doTry-doCatch-not-routing-correctly-tp5720325.html\n\nThe end user would expect that doTry .. doCatch will overrule. However it gets a bit further more complicated if the try block routes to other routes and using EIPs such as recipient list.\n'}"
camel,bugs-dot-jar_CAMEL-5683_0c3c7d1b,"{'BugID': 'CAMEL-5683', 'Summary': 'JMS connection leak with request/reply producer on temporary queues', 'Description': ""Over time I see the number of temporary queues in ActiveMQ slowly climb. Using JMX information and memory dumps in MAT, I believe the cause is a connection leak in Apache Camel.\n\nMy environment contains 2 ActiveMQ brokers in a network of brokers configuration. There are about 15 separate applications which use Apache Camel to connect to the broker using the ActiveMQ/JMS component. The various applications have different load profiles and route configurations.\n\nIn the more active client applications, I found that ActiveMQ was listing 300+ consumers when, based on my configuration, I would expect no more than 75. The vast majority of the consumers are sitting on a temporary queue. Over time, the 300 number increments by one or two over about a 4 hour period.\n\nI did a memory dump on one of the more active client applications and found about 275 DefaultMessageListenerContainers. Using MAT, I can see that some of the containers are referenced by JmsProducers in the ProducerCache; however I can also see a large number of listener containers that are no longer being referenced at all. I was also able to match up a soft-references producer/listener endpoint with an unreferenced listener which means a second producer was created at some point.\n\nLooking through the ProducerCache code, it looks like the LRU cache uses soft-references to producers, in my case a JmsProducer. This seems problematic for two reasons:\n- If memory gets constrained and the GC cleans up a producer, it is never properly stopped.\n- If the cache gets full and the map removes the LRU producer, it is never properly stopped.\n\nWhat I believe is happening, is that my application is sending a few request/reply messages to a JmsProducer. The producer creates a TemporaryReplyManager which creates a DefaultMessageListenerContainer. At some point, the JmsProducer is claimed by the GC (either via the soft-reference or because the cache is full) and the reply manager is never stopped. This causes the listener container to continue to listen on the temporary queue, consuming local resources and more importantly, consuming resources on the JMS broker.\n\nI haven't had a chance to write an application to reproduce this behavior, but I will attach one of my route configurations and a screenshot of the MAT analysis looking at DefaultMessageListenerContainers. If needed, I could provide the entire memory dump for analysis (although I rather not post it publicly). The leak depends on memory usage or producer count in the client application because the ProducerCache must have some churn. Like I said, in our production system we see about 12 temporary queues abandoned per client per day.\n\nUnless I'm missing something, it looks like the producer cache would need to be much smarter to support stopping a producer when the soft-reference is reclaimed or a member of the cache is ejected from the LRU list.\n\n""}"
camel,bugs-dot-jar_CAMEL-5699_6d63a502,"{'BugID': 'CAMEL-5699', 'Summary': 'LogFormatter throws a NPE when all elements are disabled', 'Description': 'There are perfectly valid cases where you may want to output a log message with no elements displayed, i.e. with showExchangeId=false, showBody=false, etc.\n\nFor example, when you want to print a ""signal"" log line for a particular transaction and you\'re already using MDC logging with breadcrumbs enabled. You may already have all the info you need: logging category, severity, breadcrumbId. You are not interested in anything else.\n\nCurrently, disabling all elements leads to a NPE.'}"
camel,bugs-dot-jar_CAMEL-5704_708e756d,"{'BugID': 'CAMEL-5704', 'Summary': 'Split inside Split - Parallel processing issue - Thread is getting wrong Exchange when leaving inner split ', 'Description': 'A small JUnit recreation case is attached.\nWhen using embedded split inside a split with parallel processing, threads are getting a wrong exchange (or wrong exchange copy) just after leaving the inner split and returning to the parent split.\n\nIn the test case, we split a file by comma in a parent split (Block split), then by line separator in inner split (Line Split). \nWe expect 2 files in output, each of them containing the respective Blocks.\n\nHowever, once inner split is complete, each thread is supposed to add a 11th line in the result(i).txt file saying split(i) is complete.  \nBug is that one of the thread ends up with parent split Exchange (copy?) from the other thread, and appends wrong information into the wrong file.\n\nExpected:\n---------\n(result0.txt)\nBlock1 Line 1:Status=OK\nBlock1 Line 2:Status=OK\nBlock1 Line 0:Status=OK\nBlock1 Line 4:Status=OK\nBlock1 Line 3:Status=OK\nBlock1 Line 8:Status=OK\nBlock1 Line 5:Status=OK\nBlock1 Line 6:Status=OK\nBlock1 Line 7:Status=OK\nBlock1 Line 9:Status=OK\n0 complete\n\n(result1.txt)\nBlock2 Line 0:Status=OK\nBlock2 Line 3:Status=OK\nBlock2 Line 1:Status=OK\nBlock2 Line 2:Status=OK\nBlock2 Line 6:Status=OK\nBlock2 Line 4:Status=OK\nBlock2 Line 7:Status=OK\nBlock2 Line 9:Status=OK\nBlock2 Line 5:Status=OK\nBlock2 Line 8:Status=OK\n1 complete\n\nActual:\n-------\n(result0.txt)\nBlock1 Line 1:Status=OK\nBlock1 Line 2:Status=OK\nBlock1 Line 0:Status=OK\nBlock1 Line 4:Status=OK\nBlock1 Line 3:Status=OK\nBlock1 Line 8:Status=OK\nBlock1 Line 5:Status=OK\nBlock1 Line 6:Status=OK\nBlock1 Line 7:Status=OK\nBlock1 Line 9:Status=OK\n0 complete0 complete\n\n(result1.txt)\nBlock2 Line 0:Status=OK\nBlock2 Line 3:Status=OK\nBlock2 Line 1:Status=OK\nBlock2 Line 2:Status=OK\nBlock2 Line 6:Status=OK\nBlock2 Line 4:Status=OK\nBlock2 Line 7:Status=OK\nBlock2 Line 9:Status=OK\nBlock2 Line 5:Status=OK\nBlock2 Line 8:Status=OK\n\n\nThis issue exist in 2.8.x, and probably in 2.10.x as well.\nThis is a Splitter/MulticastProcessor or Pipeline issue but not quite familiar with the code, I am having hard time tracking it. '}"
camel,bugs-dot-jar_CAMEL-5707_3f70d612,"{'BugID': 'CAMEL-5707', 'Summary': 'NotifyBuilder should be thread safe', 'Description': 'In high concurrent tests the NotifyBuilder may miss a counter.'}"
camel,bugs-dot-jar_CAMEL-5720_4a05eccf,"{'BugID': 'CAMEL-5720', 'Summary': 'Aggregate EIP - Dynamic completion size should override fixed values if in exchange', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Bug-with-completionSize-on-AggregatorProcessor-tp5721307.html'}"
camel,bugs-dot-jar_CAMEL-5796_de6dd425,"{'BugID': 'CAMEL-5796', 'Summary': 'The combination of the transacted DSL together with the <setHeader> or <setBody> prohibits to resolve the properties properly.', 'Description': 'Given the property {{myKey}} defined as:\n{code}\nmyKey=myValue\n{code}\n\nThen consider the following trivial route:\n{code:xml}\n<route>\n  <from uri=""activemq:queue:okay"" />\n    <transacted />\n    <setHeader headerName=""myHeader"">\n      <constant>{{myKey}}</constant>\n    </setHeader>\n  <to uri=""mock:test"" />\n</route>\n{code}\n\nBecause of the usage of the {{transacted}} DSL the property placeholder {{{{myKey}}}} will not be resolved to {{myValue}} properly. This behaviour would disappear if you would remove the {{transacted}} DSL. And I\'m observing the same behaviour using the {{setBody}} DSL as well.\n'}"
camel,bugs-dot-jar_CAMEL-5826_a04674f2,"{'BugID': 'CAMEL-5826', 'Summary': 'Apache Camel 2.9 Splitter with tokenize dont work with namespaces', 'Description': 'when trying to tokenize a stream having namespaces, no tokens are produced with inheritNamespaceTagName property.\n\n-------------------------------------------------------------------\n\n<route id=""hrp.connectorsCtxt.sddRcvFile2"">\n<from\n                           uri=""file:C:\\Temp\\esb\\sdd\\in?recursive=true&amp;preMove=.processing&amp;move=../.processed"" />\n                    <camel:split streaming=""true"">\n                           <tokenize token=""suiviDemande"" inheritNamespaceTagName=""suivisDemandes"" xml=""true""/>\n                           <log message=""${header.CamelSplitIndex} : ${in.body}"" />\n                    </camel:split>\n             </route>\n\n-------------------------------------------------------------------\n'}"
camel,bugs-dot-jar_CAMEL-5844_e775071b,"{'BugID': 'CAMEL-5844', 'Summary': 'Camel Tracer not showing some EIP names', 'Description': 'In order to debug Camel routes, I have enabled the Tracer as follows:         getContext().setTracing(true);\n\nHowever, I have observed that some EIP names and routes are not being printed on console, making it a bit confusing to follow. As far as I know, this happens with:\n* process(): the processor is not printed in the tracer; it\'s just empty (see below)\n* marshall(): the marshaller name is not printed in the tracer; it\'s just empty (see below)\n* setBody(): this step is also printed empty\n* from(""activiti:...""): this route step is not printed altogether\n\nFor simplicity, I only provide the examples for process() and marshall(), bit I can provide more information if needed.\n\n{panel:title=Route2 Config}\nfrom(""vm:processIncomingOrders"")\n  .process(new IncomingOrdersProcessor())\n  .split(body())\t// iterate list of Orders\n  .to(""log:incomingOrder1?showExchangeId=true"")\n  .process(new ActivitiStarterProcessor())\n  .to(""log:incomingOrder2?showExchangeId=true"")\t\t\t\n  .to(""activiti:activiti-camel-example"");\n{panel}\n\n{panel:title=Route2 Tracer}\nINFO  03-12 12:09:31,899 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2) from(vm://processIncomingOrders) -->  <<< Pattern:InOnly, [...]\nINFO  03-12 12:09:34,899 (IncomingOrdersProcessor.java:process:39)  -Processing incoming orders (from Web Services)\n[ORDER id:120 partName: wheel amount: 2 customerName: Honda Mechanics]\n[ORDER id:121 partName: engine amount: 4 customerName: Volvo]\n[ORDER id:122 partName: steering wheel amount: 3 customerName: Renault]\nINFO  03-12 12:09:34,900 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-3 >>> (route2)  --> split[body] <<< Pattern:InOnly, [...]\n{panel}\n\n\n\n{panel:title=Route6 config}\nfrom(""direct:ordercsv"")\n  .marshal().bindy(BindyType.Csv, ""net.atos.camel.entities"")\n  .to(""file:d://cameldata/orders?fileName=orders-$\\{date:now:yyyyMMdd-hhmmss}.csv"");\n{panel}\n\n{panel:title=Route6 Tracer}\nINFO  03-12 12:09:37,313 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6) direct://ordercsv -->  <<< Pattern:InOnly, [...]\nINFO  03-12 12:09:37,320 (MarkerIgnoringBase.java:info:96)  -ID-ES-CNU2113RXH-51211-1354532898719-0-8 >>> (route6)  --> file://d://cameldata/orders?fileName=orders-%24%7Bdate%3Anow%3AyyyyMMdd-hhmmss%7D.csv <<< Pattern:InOnly,  [...]\n{panel}\n\n'}"
camel,bugs-dot-jar_CAMEL-6447_020c451a,"{'BugID': 'CAMEL-6447', 'Summary': 'endChoice() has no effect in nested choice definition', 'Description': 'I just upgraded from 2.10.4 to 2.11.0 and noticed that nested choice definitions started acting strangely. For example:\n\n{code:java}\n            .choice()\n                .when(header(Exchange.EXCEPTION_CAUGHT).isNotNull())\n                    // 1\n                    .setBody(exceptionMessage().append(SystemUtils.LINE_SEPARATOR).append(exceptionStackTrace()))\n                    .choice()\n                        .when(header(HEADER_CONTROLLER_ID).isNotNull())\n                            // 1a\n                            .setHeader(Exchange.FILE_NAME, simple(AUDIT_CONTROLLER_FAILED_FILENAME + "".error.log""))\n                            .to(ENDPOINT_AUDIT_DIR)\n                        .otherwise()\n                            // 1b\n                            .setHeader(Exchange.FILE_NAME, simple(AUDIT_FAILED_FILENAME + "".error.log""))\n                            .to(ENDPOINT_AUDIT_DIR)\n                            // INSERTING .end() here solves the issue\n                        .endChoice()\n                    .log(LoggingLevel.WARN, ""DLQ written: ${in.header.CamelExceptionCaught}""\n                .otherwise()\n                    // 2\n                    .log(LoggingLevel.WARN, ""DLQ written"" + MESSAGE_LOG_FORMAT)\n                .end()\n{code}\n\nI have a test that is supposed to go through 1 and 1a. However it now passes through 1 and 2!\nIt looks like the endChoice() in 1b has no effect and the otherwise() in 2 is executed instead of 1b. Inserting and end() statement as shown seems to solve the issue, but it looks suspicious.\n\nIt\'s probably a regression introduced by the fix for CAMEL-5953, but I\'m not 100% sure. '}"
camel,bugs-dot-jar_CAMEL-6557_2c5a42db,"{'BugID': 'CAMEL-6557', 'Summary': 'AbstractListAggregationStrategy does not work with batch completion strategy', 'Description': ""When my aggregator extends AbstractListAggregationStrategy, I never get aggregator completions from the batch consumer.\n\nIf I change my aggregator to be something like:\n\n{code}\n    Foo foo = newExchange.getIn().getBody(Foo.class);\n    List<Foo> list = null;\n    Exchange outExchange;\n    if (oldExchange == null) {\n      list = new LinkedList<Foo>();\n      list.add(foo);\n      newExchange.getIn().setBody(list);\n      outExchange = newExchange;\n    } else {\n      list = oldExchange.getIn().getBody(List.class);\n      list.add(foo);\n      outExchange = oldExchange;\n    }\n    return outExchange;\n{code}\n\nthen it works fine.\n\nI'm guessing this is has something to do with AbstractListAggregationStrategy messing with properties or wrapping the actual exchanges (since the batch completion is triggered based on Exchange.BATCH_SIZE property)""}"
camel,bugs-dot-jar_CAMEL-6593_7f8a295a,"{'BugID': 'CAMEL-6593', 'Summary': 'Predicates from java dsl are not dumped to xml correctly', 'Description': 'Predicates defined in the java dsl are not dumped to xml when using jmx.\n\nEg, this java dsl route:\n{code}\nfrom(""seda:a"").choice().when(header(""test"").isNotNull()).log(""not null"").end().to(""mock:a"");\n{code}\n\nWill be dumped as this:\n{code}\n<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?>\n<route group=""com.example.TestRoute"" id=""route1"" xmlns=""http://camel.apache.org/schema/spring"">\n    <from uri=""seda:a""/>\n    <choice id=""choice23"">\n        <when id=""when24"">\n            <expressionDefinition/>\n            <log message=""not null"" id=""log20""/>\n        </when>\n    </choice>\n    <to uri=""mock:a"" id=""to17""/>\n</route>\n{code}\n\nThe <expressionDefinition> element should contain the expression.\n\nThis seems similar to CAMEL-4733.'}"
camel,bugs-dot-jar_CAMEL-6604_4209fabb,"{'BugID': 'CAMEL-6604', 'Summary': 'Routing slip and dynamic router EIP - Stream caching not working', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/stream-caching-to-HTTP-end-point-tp5736608.html'}"
camel,bugs-dot-jar_CAMEL-6604_55751402,"{'BugID': 'CAMEL-6604', 'Summary': 'Routing slip and dynamic router EIP - Stream caching not working', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/stream-caching-to-HTTP-end-point-tp5736608.html'}"
camel,bugs-dot-jar_CAMEL-6607_2d7051ed,"{'BugID': 'CAMEL-6607', 'Summary': 'Tokenize XML does not support child elements with names similar to their parent', 'Description': 'This XML will not split on Trip, as Trip has a child which starts with Trip\n<Trip>\n<Triptype>\n</Triptype>\n</Trip>\n\nThe bug was introduced in https://issues.apache.org/jira/browse/CAMEL-6004\nI believe the regex in TokenXMLExpressionIterator needs to be fixed\n\nsee enclosed test'}"
camel,bugs-dot-jar_CAMEL-6610_ed7e7c9f,"{'BugID': 'CAMEL-6610', 'Summary': 'Always got IndexOutOfBoundsException when customized id of wireTap component', 'Description': 'when I\'m tring to execute below route:\n{code}\nfrom(""timer:foo"").wireTap(""direct:a"").id(""wiretap_1"").to(""log:a"");\nfrom(""direct:a"").to(""log:b"");\n{code}\nI always got IndexOutOfBoundsException:\n{color:red}\nException in thread ""main"" java.lang.IndexOutOfBoundsException: Index: -1\n\tat java.util.Collections$EmptyList.get(Collections.java:3212)\n\tat org.apache.camel.model.ProcessorDefinition.id(ProcessorDefinition.java:1025)\n\tat org.talend.esb.liugang.camel.wiretap.TestWiretap$1.configure(TestWiretap.java:14)\n\tat org.apache.camel.builder.RouteBuilder.checkInitialized(RouteBuilder.java:322)\n\tat org.apache.camel.builder.RouteBuilder.configureRoutes(RouteBuilder.java:276)\n\tat org.apache.camel.builder.RouteBuilder.addRoutesToCamelContext(RouteBuilder.java:262)\n\tat org.apache.camel.impl.DefaultCamelContext.addRoutes(DefaultCamelContext.java:650)\n\tat org.talend.esb.liugang.camel.wiretap.TestWiretap.main(TestWiretap.java:10)\n{color}\nI tried on 2.11.1, 2.11.2-SNAPSHOT, both of them have the same problem (not sure 2.12-SNAPSHOT).'}"
camel,bugs-dot-jar_CAMEL-6667_1fc7bd7a,"{'BugID': 'CAMEL-6667', 'Summary': ""Loop EIP doesn't honour copy option in some circumstances"", 'Description': 'Happens when the Async Routing Engine variant of the Loop logic kicks in, and there are more than two processors in the loop body, e.g. \n\\\\\n\\\\\n{code:java}\n.loop(3)\n  .to(""activemq:queue:abc?exchangePattern=InOut"")\n  .to(""activemq:queue:def?exchangePattern=InOut"")\n.end()\n{code}\n\nThe wrong inflight Exchange is copied (instead of the original one), and since the implicit Pipeline has copied the OUT message from the 1st endpoint to the IN message, the original IN message is lost fully.'}"
camel,bugs-dot-jar_CAMEL-6687_617eab1c,"{'BugID': 'CAMEL-6687', 'Summary': ""Using simple language OGNL expressions doesn't work for Bean Binding when a field is null"", 'Description': 'The following functionality doesn\'t work, when one of the fields is null: \n\nhttp://camel.apache.org/bean-binding.html\n{quote}\nYou can also use the OGNL support of the Simple expression language. Now suppose the message body is an object which has a method named asXml. To invoke the asXml method we can do as follows:\n{code}.bean(OrderService.class, ""doSomething(${body.asXml}, ${header.high})""){code}\n\nInstead of using .bean as shown in the examples above, you may want to use .to instead as shown:\n{code}.to(""bean:orderService?method=doSomething(${body.asXml}, ${header.high})""){code}\n{quote}\n\nA test case is provided. Instead of getting values of fields ""foo"" and ""bar"" respectively, the first parameter (which should be null) receives value of pojo.toString(), while the second parameter receives the correct value.'}"
camel,bugs-dot-jar_CAMEL-6723_b92d6237,"{'BugID': 'CAMEL-6723', 'Summary': 'Message history - Possible ArrayIndexOutOfBoundsException', 'Description': None}"
camel,bugs-dot-jar_CAMEL-6743_745a85ab,"{'BugID': 'CAMEL-6743', 'Summary': 'Using @Simple (or others) bean parameter binding for boolean type should eval as predicate', 'Description': 'For example\n{code}\n        public void read(String body, @Simple(""${header.foo} != null"") boolean foo) {\n{code}\n\nThe foo parameter is a boolean and thus the @Simple expression should be evaluated as a predicate and not as an Expression which happens today.'}"
camel,bugs-dot-jar_CAMEL-6779_f412d744,"{'BugID': 'CAMEL-6779', 'Summary': 'StaxConverter: encoding problems for XMLEventReader and XMLStreamReader', 'Description': 'StaxConverter creates XMLEventReader and XMLStreamReader always with a specified encoding. However, the encoding of the data the readers should read is not always known. Therefore exceptions occur.\n\nThe solution is easy: The encoding should not be set so that the readers can determine the encoding.'}"
camel,bugs-dot-jar_CAMEL-6810_6b210169,"{'BugID': 'CAMEL-6810', 'Summary': 'Bean Component/BeanBinding: Body as InputStream parametr (specified as ${body} in route)', 'Description': 'I discovered following problem (which was already shortly discussed in [Camel user forum|http://camel.465427.n5.nabble.com/Bean-component-Bean-Binding-Body-as-InputStream-parametr-specified-as-body-in-route-td5740656.ht]).\n\nI have a ""streamBodyBindingBean"" bean with this method:\n{code}\npublic void bodyBinding(InputStream in) throws IOException {\n  int byteCount = 0;\n  int c;\n  while((c = in.read()) != -1)\n    byteCount++;\n  System.out.println(""ByteCount: "" + byteCount);\n}\n{code}\n\nAnd this route:\n{code}\n<route id="""" trace=""true"">\n  <from uri=""direct://body-input-stream-binding-in""/>\n  <to uri=""bean://streamBodyBindingBean?method=bodyBinding(${body})""/>\n  <!-- to uri=""bean://isBodyBindingBean""/--> \n  <to uri=""mock://body-input-stream-binding-out""/>\n</route>\n{code}\n\nAnd here is a way how I send exchange from test stuff:\n{code}\nByteArrayInputStream in = new ByteArrayInputStream(\n  ""Small body, which I want to bind as InputStream"".getBytes(""UTF-8"")\n);\nExchange exchange = createExchangeWithBody(in);\nexchange.setPattern(ExchangePattern.InOnly);\ntemplate.send(""direct://body-input-stream-binding-in"", exchange); \n{code}\n\nIn this case I got a sysout message: {{ByteCount: 0}}, but when I used the commented variant in the route, I got expected result: {{ByteCount: 47""}}.\n\nWhen I change the route and use bean component 2 times (both variant of bean method invocation), then I got:\n\n{noformat}\n2013-10-01 12:26:37.259 DEBUG {main} [SendProcessor] >>>> Endpoint[bean://isBodyBindingBean?method=bodyBinding%28%24%7Bbody%7D%29] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]]\nByteCount: 0\n2013-10-01 12:26:37.289 DEBUG {main} [SendProcessor] >>>> Endpoint[bean://isBodyBindingBean] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]]\nByteCount: 47\n2013-10-01 12:26:37.307 DEBUG {main} [SendProcessor] >>>> Endpoint[mock://body-input-stream-binding-out] Exchange[Message: [Body is instance of org.apache.camel.StreamCache]] \n{noformat}\n\nThe strange for me is {{MethodInfo}} class, line 526:\n{code}\n// the parameter value was not already valid, but since the simple language have evaluated the expression\n// which may change the parameterValue, so we have to check it again to see if its now valid\nexp = exchange.getContext().getTypeConverter().convertTo(String.class, parameterValue);\n// String values from the simple language is always valid\nif (!valid) {\n  ...\n}\n{code}\n\nThe line after comment caused that my ""InputStream"" is transformed into String, what can be a problem in case of ""big"" InputStream.\n\nI know that I can use only second variant of ""bean method invocation"", which is enough for my need, but I only want to point out to this situation.'}"
camel,bugs-dot-jar_CAMEL-6889_cd40b712,"{'BugID': 'CAMEL-6889', 'Summary': 'CBR - Should break out if exception was thrown when evaluating predicate', 'Description': 'If having a CBR and the predicate throws an exception, then the next predicate is called before error handler triggers.\n\nWe should break out when exception is detected like pipeline/multicast can do.\n\n'}"
camel,bugs-dot-jar_CAMEL-6918_5761250c,"{'BugID': 'CAMEL-6918', 'Summary': ""Error handler for SEDA producer doesn't work"", 'Description': 'Exceptions thrown by seda producer bypass exception handling and bubble up to original caller. \n\n'}"
camel,bugs-dot-jar_CAMEL-6936_4954d573,"{'BugID': 'CAMEL-6936', 'Summary': 'FTP route with idempotent repo does not detect modified files', 'Description': 'Per my forum post:\nhttp://camel.465427.n5.nabble.com/inProgressRepository-Not-clearing-for-items-in-idempotentRepository-td5742613.html\n\nI\'m attempting to consume messages from an FTP server using an idempotent repository to ensure that I do not re-download a file unless it has been modified. \n\nHere is my (quite simple) camel configuration: \n{code}\n        <beans:bean id=""downloadRepo"" class=""org.apache.camel.processor.idempotent.FileIdempotentRepository"" >\n                <beans:property name=""fileStore"" value=""/tmp/.repo.txt""/>\n                <beans:property name=""cacheSize"" value=""25000""/>\n                <beans:property name=""maxFileStoreSize"" value=""1000000""/>\n        </beans:bean>\n\n        <camelContext trace=""true"" xmlns=""http://camel.apache.org/schema/spring"">\n                <endpoint id=""myFtpEndpoint"" uri=""ftp://me@localhost?password=****&binary=true&recursive=true&consumer.delay=15000&readLock=changed&passiveMode=true&noop=true&idempotentRepository=#downloadRepo&idempotentKey=$simple{file:name}-$simple{file:modified}"" />\n                <endpoint id=""myFileEndpoint"" uri=""file:///tmp/files""/>\n\n        <route>\n            <from uri=""ref:myFtpEndpoint"" />\n            <to uri=""ref:myFileEndpoint"" />\n        </route>\n{code}\n\nWhen I start my application for the first time, all files are correctly downloaded from the FTP server and stored in the target directory, as well as recorded in the idempotent repo. \n\nWhen I restart my application, all files are correctly detected as being in the idempotent repo already on the first poll of the FTP server, and are not re-downloaded: \n\n13-11-04 16:52:10,811 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: FtpFile[name=test1.txt, dir=false, file=true] \n2013-11-04 16:52:10,811 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: This consumer is idempotent and the file has been consumed before. Will skip this file: RemoteFile[test1.txt] \n\nHowever, on all subsequent polls to the FTP server the idempotent check is short-circuited because the file is ""in progress"": \n\n2013-11-04 16:53:10,886 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: FtpFile[name=test1.txt, dir=false, file=true]\n2013-11-04 16:53:10,886 TRACE [Camel (camel-1) thread #0 - ftp://me@localhost] org.apache.camel.component.file.remote.FtpConsumer: Skipping as file is already in progress: test1.txt \n\nI am using camel-ftp:2.11.1 (also observing same behavior with 2.12.1)  When I inspect the source code I notice two interesting things. \nFirst, the GenericFileConsumer check that determines whether a file is already inProgress which is called from isValidFile() always adds the file to the inProgressRepository: \n{code}\n    protected boolean isInProgress(GenericFile<T> file) { \n        String key = file.getAbsoluteFilePath(); \n        return !endpoint.getInProgressRepository().add(key); \n    } \n{code}\n\nSecond, if a file is determined to match an entry already present in the idempotent repository it is discarded (GenericFileConsumer.isValidFile() returns false).  This means it is never published to an exchange, and thus never reaches the code which would remove it from the inProgressRepository. \n\nSince the inProgress check happens before the Idempotent Check, we will always short circuit after we get into the inprogress state, and the file will never actually be checked again. '}"
camel,bugs-dot-jar_CAMEL-6948_f744afd9,"{'BugID': 'CAMEL-6948', 'Summary': 'ProducerCache should not only stop non-singelton Producers but also shutdown them afterwards as well if the given Producer is a ShutdownableService', 'Description': ""Currently because of this bug the {{doShutdown}} hook of the following non-singleton Producers doesn't kick in at all:\n\n- {{JpaProducer}}\n- {{Mina2Producer}}\n\nWhich could cause resources leaking.""}"
camel,bugs-dot-jar_CAMEL-6964_6b2ffb30,"{'BugID': 'CAMEL-6964', 'Summary': 'Camel FileComponent: Done file will not be removed if moveFailed option is configured and an error occurs', 'Description': 'Only the ""real"" file is moved to the directory specified with the moveFailed-option. The done file still exists in the source folder and will not be deleted.'}"
camel,bugs-dot-jar_CAMEL-6987_37e0e6bb,"{'BugID': 'CAMEL-6987', 'Summary': 'JMX - browseMessageAsXml for files does not work if includeBody is enabled', 'Description': 'If you use the JXM API to browse file endpoints and want to load the file content with includeBody = true, then the file is not loaded.\n\nThere is a little bug in MessgeHelper'}"
camel,bugs-dot-jar_CAMEL-7016_4ed448c7,"{'BugID': 'CAMEL-7016', 'Summary': 'JMX - Update route from xml on route mbean should update current route only', 'Description': 'If you do not have id of the route in the XML then Camel thinks its a new route to be added. We should ensure we handle that, and only update current route as that is the intend of this operation.\n\nIf you want to add new routes use mbean operation on camelcontext instead.'}"
camel,bugs-dot-jar_CAMEL-7018_3244c1e5,"{'BugID': 'CAMEL-7018', 'Summary': 'Using custom beans with @ManagedResource shows unavailable standard attributes', 'Description': 'If you have a custom bean with @ManagedResource and your own attr/ops then Camel adds its default attrs/ops which it should not as they are not available.\n\nSee screenshot'}"
camel,bugs-dot-jar_CAMEL-7055_15e1077d,"{'BugID': 'CAMEL-7055', 'Summary': 'NullPointerException at FileInputStreamCache.<init>(FileInputStreamCache.java:52) in connection with DataFormat.marshal', 'Description': 'Stack Trace:\n{code}\nCaused by: java.lang.NullPointerException\n\tat org.apache.camel.converter.stream.FileInputStreamCache.<init>(FileInputStreamCache.java:52)\n\tat org.apache.camel.converter.stream.CachedOutputStream.newStreamCache(CachedOutputStream.java:199)\n\tat org.apache.camel.processor.MarshalProcessor.process(MarshalProcessor.java:79)\n{code}\n\nError occurs, if streamCache is true and the stream is put into the file system because the spool threashold is reached. \n\nThe following is happening:\nThe Marshall Processor handels over to the DataFromat.marshal method a CachedOutputStream instance. In the marschal method data are written into the output stream, when the spool threshold is reached the data are streamed into the file system. Finally the output stream is closed and the CachedOutputStream instance deletes the cached file during closing. The next processor tries to read the FileInputStreamCache and gets the NullPointerException.\n\nCurrently this problem can occur in the following DataFormat classes (because they close the stream, which is actually correct):\n\nGzipDataFormat\nCryptoDataFormat\nPGPDataFormat\nSerializationDataFormat\nXMLSecurityDataFormat\nZipDataFormat\n\nMy proposal is not to delete the cached file during closing the output stream. The cached file shall only be closed on the onCompletion event of the route. See attached patch.\n\n\n'}"
camel,bugs-dot-jar_CAMEL-7100_00a9b02b,"{'BugID': 'CAMEL-7100', 'Summary': 'CLONE - Camel Splitter eat up exceptions recorded by the underlying Scanner', 'Description': 'See http://camel.465427.n5.nabble.com/Trouble-with-split-tokenize-on-linux-td5721677.html for details'}"
camel,bugs-dot-jar_CAMEL-7125_6641f182,"{'BugID': 'CAMEL-7125', 'Summary': 'tokenizeXml fails when attributes have a / in them', 'Description': '{{tokenizeXml}} does not work or produce value xml output when attributes contain a {{/}}.\n\nThe test below will fail under 2.12.2\n\n{code:java}\nimport org.apache.camel.EndpointInject;\nimport org.apache.camel.Produce;\nimport org.apache.camel.ProducerTemplate;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.component.mock.MockEndpoint;\nimport org.apache.camel.test.junit4.CamelTestSupport;\nimport org.junit.Test;\n\npublic class CamelTokenizeXmlTest extends CamelTestSupport {\n\n  @EndpointInject(uri = ""mock:result"")\n  protected MockEndpoint resultEndpoint;\n  @Produce(uri = ""direct:start"")\n  protected ProducerTemplate template;\n\n  @Test\n  public void testXmlWithSlash() throws Exception {\n    String message = ""<parent><child attr=\'/\' /></parent>"";\n    resultEndpoint.expectedBodiesReceived(""<child attr=\'/\' />"");\n    template.sendBody(message);\n    resultEndpoint.assertIsSatisfied();\n  }\n\n  @Override\n  protected RouteBuilder createRouteBuilder() {\n    return new RouteBuilder() {\n      @Override\n      public void configure() {\n        from(""direct:start"").split().tokenizeXML(""child"").to(""mock:result"");\n      }\n    };\n  }\n}\n{code}'}"
camel,bugs-dot-jar_CAMEL-7130_7c9326f4,"{'BugID': 'CAMEL-7130', 'Summary': 'Set XsltBuilder allowStax attribute to be true by default', 'Description': 'It could be more effective and safe to use the stax API by default.'}"
camel,bugs-dot-jar_CAMEL-7130_cc192f87,"{'BugID': 'CAMEL-7130', 'Summary': 'Set XsltBuilder allowStax attribute to be true by default', 'Description': 'It could be more effective and safe to use the stax API by default.'}"
camel,bugs-dot-jar_CAMEL-7146_b6981cfd,"{'BugID': 'CAMEL-7146', 'Summary': 'NPE in Aggregator when completionSize = 1', 'Description': 'A Camel aggregator with persistence repository cannot have a completionSize of 1. If this is configured, every message produces a NPE with the attached stacktrace. \n\nI have also attached a small example project that shows the Exception. As soon as the completionSize is > 1, it runs fine.\n\nThis is just a minor flaw, since I cannot think about a really useful case with completionSize 1, but it worked with earlier versions of Camel. \n\nAs an alternative (if completionSize 1 should not be used), Camel could throw an error during Context startup when completionSize < 2.'}"
camel,bugs-dot-jar_CAMEL-7160_095fa2b4,"{'BugID': 'CAMEL-7160', 'Summary': 'Throttling has problems with rate changes', 'Description': 'When using the throttler with the header expression for controlling the rate, changing the rate does not work reliably. \n\nSome more information can be found in the following mail thread:\n\nhttp://camel.465427.n5.nabble.com/Problems-with-dynamic-throttling-td5746613.html'}"
camel,bugs-dot-jar_CAMEL-7163_5f726d0b,"{'BugID': 'CAMEL-7163', 'Summary': 'BacklogDebugger - Should not change body/header type to string', 'Description': 'When using the backlog debugger then updating the body/headers would currently force those to become string type.\n\nWe should preserve existing type, and allow end users to specify a new type. And also make it possible to remove body/headers as well.'}"
camel,bugs-dot-jar_CAMEL-7167_1e33fcbc,"{'BugID': 'CAMEL-7167', 'Summary': 'AbstractListAggregationStrategy : at the end of the split, the body is not replaced by the agregated list', 'Description': 'Using a class that extends AbstractListAggregationStrategy to rebuild a List after the completion of the split cause the body not to be replaced by the agregated list at the end of the split.\n\nIn other words (AbstractListAggregationStrategy.onCompletion(Exchange exchange) is never called.\n\n\nHere is what I do :\n\nfrom(HANDLE_A_LIST)//\n            .split(body(), new ListAggregationStrategy())// body is an arrayList of String\n            .to(""log:foo"")//\n            .end()// end split\n            // the body is a string instead of a List\n            .end()// end route\n\n   \nclass ListAggregationStrategy extends AbstractListAggregationStrategy<String>\n    {\n\n        @Override\n        public String getValue(Exchange exchange)\n        {\n            return exchange.getIn().getBody();\n        }\n    }\n\nAs workaround, I use .setBody(property(Exchange.GROUPED_EXCHANGE)) after the end of the split.'}"
camel,bugs-dot-jar_CAMEL-7209_5f78c646,"{'BugID': 'CAMEL-7209', 'Summary': 'NIOConverter.toByteArray return bad data.', 'Description': 'Current implmentation of NIOConverter.toByteArray return the byte array\nthat back the buffer. Array can be bigger that relevant data in ByteBuffer.'}"
camel,bugs-dot-jar_CAMEL-7213_336663c9,"{'BugID': 'CAMEL-7213', 'Summary': 'NIOConverter need to call flip() when we put something into the buffer', 'Description': 'When we create a ByteBuffer, we need to make sure it is ready to be read.'}"
camel,bugs-dot-jar_CAMEL-7239_ae419224,"{'BugID': 'CAMEL-7239', 'Summary': 'Address the SchemaFactory thread safe issue.', 'Description': 'SchemaFactory is not thread safe, we need to do addition work in ValidatorProcessor to avoid the threads issue.'}"
camel,bugs-dot-jar_CAMEL-7241_18c23fa8,"{'BugID': 'CAMEL-7241', 'Summary': 'ByteBuffer to String conversion uses buffer capacity not limit', 'Description': 'Camel\'s conversion logic for ByteBuffer\'s to String\'s has a bug where camel uses a ByteBuffers capacity() instead of it\'s limit().\n\nIf you allocate a large byte buffer and only partially fill it with data, and use camel to convert this into a string, camel tries to convert all the bytes, even the non-used ones.\n\nThis unit test reproduces this bug.\n\n{code}\n    @Test\n    public void testByteBufferToStringConversion()\n    {\n        String str = ""123456789"";\n        ByteBuffer buffer = ByteBuffer.allocate( 16 );\n        buffer.put( str.getBytes() );\n\n        Exchange exchange = new DefaultExchange( context() );\n        exchange.getIn().setBody( buffer );\n        assertEquals( str, exchange.getIn().getBody( String.class ) );\n    }\n{code}'}"
camel,bugs-dot-jar_CAMEL-7271_a5a2f750,"{'BugID': 'CAMEL-7271', 'Summary': 'AbstractListGroupedExchangeAggregationStrategy produces failed exchange if first received exchange fails', 'Description': 'If the first exchange received by a (concrete implementation of) AggregationStrategy  contains an exception, then the result of the aggregation will also contain that exception, and so will not continue routing without error. This makes the first received exchange have an effect that subsequent exchanges do not have.\n\nThe specific use case multicasts to GroupedExchangeAggregationStrategy. The MulticastProcessor.doDone function uses ExchangeHelper.copyResults to copy the aggregated result to the original exchange. The copyResults method copies the exception as well, thereby propagating the error.\n\n The attached unit test has 3 tests, testAFail, testBFail, and testAllGood. All three of these should pass, but testAFail does not.\n\nWhat is happening is that AbstractListAggregationStrategy is directly storing its values on and returning the first exchange:\n    public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {\n        List<V> list;\n\n        if (oldExchange == null) {\n            list = getList(newExchange);\n        } else {\n            list = getList(oldExchange);\n        }\n\n        if (newExchange != null) {\n            V value = getValue(newExchange);\n            if (value != null) {\n                list.add(value);\n            }\n        }\n\n        return oldExchange != null ? oldExchange : newExchange;\n    }\n\nThe pre-CAMEL-5579 version of GroupedExchangeAggregationStrategy created a fresh exchange to store and return the aggregated exchanges:\n    public Exchange aggregate(Exchange oldExchange, Exchange newExchange) {\n        List<Exchange> list;\n        Exchange answer = oldExchange;\n\n        if (oldExchange == null) {\n            answer = new DefaultExchange(newExchange);\n            list = new ArrayList<Exchange>();\n            answer.setProperty(Exchange.GROUPED_EXCHANGE, list);\n        } else {\n            list = oldExchange.getProperty(Exchange.GROUPED_EXCHANGE, List.class);\n        }\n\n        if (newExchange != null) {\n            list.add(newExchange);\n        }\n        return answer;\n    }\n'}"
camel,bugs-dot-jar_CAMEL-7275_44cad623,"{'BugID': 'CAMEL-7275', 'Summary': 'Using doTry .. doCatch with recipient list should not trigger error handler during recipient list work', 'Description': 'When you have a route like this\n\n{code}\n                from(""direct:start"")\n                    .doTry()\n                        .recipientList(constant(""direct:foo"")).end()\n                    .doCatch(Exception.class)\n                        .to(""mock:catch"")\n                        .transform().constant(""doCatch"")\n                    .end()\n                    .to(""mock:result"");\n{code}\n\nThen if an exception was thrown it should be catch by doCatch\n\nA similar route with to instead works as expected.'}"
camel,bugs-dot-jar_CAMEL-7304_fa165d6b,"{'BugID': 'CAMEL-7304', 'Summary': 'InterceptSendToEndpoint does not work where uri needs to be normalized', 'Description': 'interceptSendToEndpoint(""sftp://hostname:22/testDirectory?privateKeyFile=/user/.ssh.id_rsa"") is not intercepted because uri passed to InterceptSendToEndpointDefinition is not normalized.\n\nAs a result InterceptSendToEndpointDefinition createProcessor() method fails to match EndpointHelper.matchEndpoint(routeContext.getCamelContext(), uri, getUri()) and InterceptSendToEndpoint is not created.'}"
camel,bugs-dot-jar_CAMEL-7344_91228815,"{'BugID': 'CAMEL-7344', 'Summary': 'Some endpoints configured using beans may result in NPE under DEBUG mode', 'Description': ""CAMEL-6130 seems to have introduced this issue or more precisely speaking, it has made this issue visible.\n\nDefaultEndpoint's toString() method seems to require its endpoint string value to be set. If it's not set, the toString method throws an exception. A fully built endpoint always has its endpoint string value set, thus there is no issue. However, an endpoint being manually set up may not have its endpoint string value set from the beginning (e.g., when its super class uses the DefaultEndpoint's default constructor to instantiate using a bean based instantiation).\n\nThe debug log statement introduced in CAMEL-6130 invokes this toString method during the endpoint setup.\n\nThat means, a spring based CXF endpoint may result in the following exception under the debug mode.\n\nSLF4J: Failed toString() invocation on an object of type [org.apache.camel.component.cxf.CxfSpringEndpoint]\njava.lang.IllegalArgumentException: endpointUri is not specified and org.apache.camel.component.cxf.CxfSpringEndpoint does not implement createEndpointUri() to create a default value\nat org.apache.camel.impl.DefaultEndpoint.getEndpointUri(DefaultEndpoint.java:154)\nat org.apache.camel.impl.DefaultEndpoint.toString(DefaultEndpoint.java:139)\nat org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:304)\nat org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)\nat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)\nat org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:271)\nat org.apache.camel.util.IntrospectionSupport.setProperty(IntrospectionSupport.java:528)\nat org.apache.camel.util.IntrospectionSupport.setProperty(IntrospectionSupport.java:570)\nat org.apache.camel.util.IntrospectionSupport.setProperties(IntrospectionSupport.java:454)\nat org.apache.camel.util.EndpointHelper.setProperties(EndpointHelper.java:249)\nat org.apache.camel.component.cxf.CxfEndpoint.setCamelContext(CxfEndpoint.java:840)\n\nI  wonder whether we really need DefaultEndpoint's getEndpointUri() to throw an exception when it's endpoint string value is not set. But if we keep this rule, we must catch the exception in its toString() method so that we won't throw the above exception when the toString() method is called during the endpoint setup\n\nI would propose to add the exception catching in the toString method. If we decide to change the getEndpointUri() method to not throw the exception (that change will likely require the NPE check at the users of this method), we can make that change and remove the exception catch from the toString method\n\nThis issue affects camel 2.11.0 and later versions.""}"
camel,bugs-dot-jar_CAMEL-7359_9cb09d14,"{'BugID': 'CAMEL-7359', 'Summary': 'Simple Language - Additional after text after inbuilt function call is ignored', 'Description': ""The following Simple expression is valid and runs OK - however it may have been appropriate to report an error to the developer.\n\n{code:xml}\n            <setBody>\n                <simple>${bodyAs(java.lang.String) Additional text ignored...}</simple>\n            </setBody>\n{code}\n\nThe above seems a somewhat contrived example; However this is a more 'realistic' scenario in which the behaviour is not unexpected -\n\n{code:xml}\n            <setBody>\n                <simple>${bodyAs(java.lang.String).toUpperCase()}</simple>\n            </setBody>\n{code}\n\nThe above simple expression will simply set the body to be of type java.lang.String, however will not invoke the subsequent toUpperCase() call - likewise no error is reported to the developer.\n\nCamel has the same issue when using the function of headerAs and mandatoryBodyAs.""}"
camel,bugs-dot-jar_CAMEL-7359_e6fbbf04,"{'BugID': 'CAMEL-7359', 'Summary': 'Simple Language - Additional after text after inbuilt function call is ignored', 'Description': ""The following Simple expression is valid and runs OK - however it may have been appropriate to report an error to the developer.\n\n{code:xml}\n            <setBody>\n                <simple>${bodyAs(java.lang.String) Additional text ignored...}</simple>\n            </setBody>\n{code}\n\nThe above seems a somewhat contrived example; However this is a more 'realistic' scenario in which the behaviour is not unexpected -\n\n{code:xml}\n            <setBody>\n                <simple>${bodyAs(java.lang.String).toUpperCase()}</simple>\n            </setBody>\n{code}\n\nThe above simple expression will simply set the body to be of type java.lang.String, however will not invoke the subsequent toUpperCase() call - likewise no error is reported to the developer.\n\nCamel has the same issue when using the function of headerAs and mandatoryBodyAs.""}"
camel,bugs-dot-jar_CAMEL-7364_7bbb88ba,"{'BugID': 'CAMEL-7364', 'Summary': 'JpaMessageIdRepository uses EntityManager non thread-safe', 'Description': 'In our product we have found strange behavior of JpaMessageIdRepository when change version 2.9.2 to 2.12.3.\nThe reason for this was that EntityManager assigned in the constructor org.apache.camel.processor.idempotent.jpa.JpaMessageIdRepository, but\nEntityManager not required to be thread safe.\nhttp://download.oracle.com/otn-pub/jcp/persistence-2.0-fr-oth-JSpec/persistence-2_0-final-spec.pdf page 286.\nI think need assign the EntityManager in each method separately.'}"
camel,bugs-dot-jar_CAMEL-7418_cabee0e9,"{'BugID': 'CAMEL-7418', 'Summary': 'org.apache.camel.impl.JndiRegistry.findByTypeWithName', 'Description': 'I guess this line isn\'t correct:\nif (type.isInstance(pair.getClass()) || type.getName().equals(pair.getClassName()))\n\nThe variable ""pair.getClass()"" always returns ""javax.naming.NameClassPair"" or its subclasses and the method ""isInstance"" works only with Instances, but doesnt Classes.\n\n\n I think the correct code should be:\nif (type.isAssignableFrom(Class.forName(pair.getClassName())))\n\n\nI\'ve tried to test a transacted route, but i couldnt because the error: \nFailed to create route route1 at: >>> Transacted[] <<< in route: Route(route1)[[From[direct:start]] -> [Transacted[]]] because of No bean could be found in the registry of type: PlatformTransactionManager\n'}"
camel,bugs-dot-jar_CAMEL-7429_43956f93,"{'BugID': 'CAMEL-7429', 'Summary': 'Camel Properties Component concatenation issue ', 'Description': 'Hi,\n\nSuppose you have a properties file of this type\n\n{code}\n#PROPERTIES CONCATENATION\nprop1=file:\nprop2=dirname\nconcat.property={{prop1}}{{prop2}}\n\n#PROPERTIES WITHOUT CONCATENATION\nproperty.complete=file:dirname\n{code}\n\nand you want to use the property concat.property. Using Camel 2.10.3 loading this property doesn\'t create any kind of problem. When I upgrade to Camel 2.12.3 I get an exception, that you can reproduce with the following informations.\n\nIn *DefaultPropertiesParser* class of org.apache.camel.component.properties package, I found a strange behaviour relative to that specific kind of property.  When I execute a test like the following, (the first try to use concatenated property and the second try to use property without concatenation):\n\n{code:title=PropertiesComponentConcatenatePropertiesTest.java}\nimport org.apache.camel.CamelContext;\nimport org.apache.camel.ContextTestSupport;\nimport org.apache.camel.builder.RouteBuilder;\n\npublic class PropertiesComponentConcatenatePropertiesTest extends ContextTestSupport {\n    \n    @Override\n    protected CamelContext createCamelContext() throws Exception {\n        CamelContext context = super.createCamelContext();\n        context.addComponent(""properties"", new PropertiesComponent(""classpath:org/apache/camel/component/properties/concatenation.properties""));\n        return context;\n    }\n    \n    @Override\n    protected void setUp() throws Exception {\n        System.setProperty(""environment"", ""junit"");\n        super.setUp();\n    }\n    \n    @Override\n    protected void tearDown() throws Exception {\n        System.clearProperty(""environment"");\n        super.tearDown();\n    }\n    \n    public void testConcatPropertiesComponentDefault() throws Exception {\n        context.addRoutes(new RouteBuilder() {\n            @Override\n            public void configure() throws Exception {\n                from(""direct:start"").setBody(simple(""${properties:concat.property}""))\n                .to(""mock:result"");\n            }\n        });\n        context.start();\n\n        getMockEndpoint(""mock:result"").expectedBodiesReceived(""file:dirname"");\n\n        template.sendBody(""direct:start"", ""Test"");\n\n        assertMockEndpointsSatisfied();\n    }\n    \n    public void testWithoutConcatPropertiesComponentDefault() throws Exception {\n        context.addRoutes(new RouteBuilder() {\n            @Override\n            public void configure() throws Exception {\n                from(""direct:start"").setBody(simple(""${properties:property.complete}""))\n                .to(""mock:result"");\n            }\n        });\n        context.start();\n\n        getMockEndpoint(""mock:result"").expectedBodiesReceived(""file:dirname"");\n\n        template.sendBody(""direct:start"", ""Test"");\n\n        assertMockEndpointsSatisfied();\n    }\n}\n{code}\n\nThe first test return the following exception:\n{code}\norg.apache.camel.CamelExecutionException: Exception occurred during execution on the exchange: Exchange[Message: Test]\n\tat org.apache.camel.util.ObjectHelper.wrapCamelExecutionException(ObjectHelper.java:1379)\n\tat org.apache.camel.util.ExchangeHelper.extractResultBody(ExchangeHelper.java:622)\n\tat org.apache.camel.impl.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:467)\n\tat org.apache.camel.impl.DefaultProducerTemplate.extractResultBody(DefaultProducerTemplate.java:463)\n\tat org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:139)\n\tat org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:144)\n\tat org.apache.camel.component.properties.PropertiesComponentConcatenatePropertiesTest.testConcatPropertiesComponentDefault(PropertiesComponentConcatenatePropertiesTest.java:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat junit.framework.TestCase.runTest(TestCase.java:176)\n\tat junit.framework.TestCase.runBare(TestCase.java:141)\n\tat org.apache.camel.TestSupport.runBare(TestSupport.java:58)\n\tat junit.framework.TestResult$1.protect(TestResult.java:122)\n\tat junit.framework.TestResult.runProtected(TestResult.java:142)\n\tat junit.framework.TestResult.run(TestResult.java:125)\n\tat junit.framework.TestCase.run(TestCase.java:129)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:255)\n\tat junit.framework.TestSuite.run(TestSuite.java:250)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nCaused by: org.apache.camel.RuntimeCamelException: java.lang.IllegalArgumentException: Expecting }} but found end of string from text: prop1}}{{prop2\n\tat org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1363)\n\tat org.apache.camel.builder.ExpressionBuilder$78.evaluate(ExpressionBuilder.java:1784)\n\tat org.apache.camel.support.ExpressionAdapter.evaluate(ExpressionAdapter.java:36)\n\tat org.apache.camel.builder.SimpleBuilder.evaluate(SimpleBuilder.java:83)\n\tat org.apache.camel.processor.SetBodyProcessor.process(SetBodyProcessor.java:46)\n\tat org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:398)\n\tat org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191)\n\tat org.apache.camel.processor.Pipeline.process(Pipeline.java:118)\n\tat org.apache.camel.processor.Pipeline.process(Pipeline.java:80)\n\tat org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191)\n\tat org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:51)\n\tat org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:191)\n\tat org.apache.camel.processor.UnitOfWorkProducer.process(UnitOfWorkProducer.java:73)\n\tat org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:378)\n\tat org.apache.camel.impl.ProducerCache$2.doInProducer(ProducerCache.java:1)\n\tat org.apache.camel.impl.ProducerCache.doInProducer(ProducerCache.java:242)\n\tat org.apache.camel.impl.ProducerCache.sendExchange(ProducerCache.java:346)\n\tat org.apache.camel.impl.ProducerCache.send(ProducerCache.java:184)\n\tat org.apache.camel.impl.DefaultProducerTemplate.send(DefaultProducerTemplate.java:124)\n\tat org.apache.camel.impl.DefaultProducerTemplate.sendBody(DefaultProducerTemplate.java:137)\n\t... 22 more\nCaused by: java.lang.IllegalArgumentException: Expecting }} but found end of string from text: prop1}}{{prop2\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.doParseUri(DefaultPropertiesParser.java:90)\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:51)\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:38)\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.createPlaceholderPart(DefaultPropertiesParser.java:189)\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.doParseUri(DefaultPropertiesParser.java:105)\n\tat org.apache.camel.component.properties.DefaultPropertiesParser.parseUri(DefaultPropertiesParser.java:51)\n\tat org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:158)\n\tat org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:117)\n\tat org.apache.camel.builder.ExpressionBuilder$78.evaluate(ExpressionBuilder.java:1781)\n\t... 40 more\n{code}\n\nIt seems that *DefaultPropertiesParser* doesn\'t like concatenation of properties. I\'ve forked Camel project on GitHub and I\'ve added the unit test posted above. Here is the link: https://github.com/ancosen/camel\n\nInvestigating the history of the particular class I found that the problem should arise from:\n *CAMEL-5328 supports resolution of nested properties in PropertiesComponent*\n\nHere is the link of the commit:\nhttps://github.com/apache/camel/commit/83f4b0f485521967d05de4e65025c4558a75ff3c\n\nThanks.\nBye'}"
camel,bugs-dot-jar_CAMEL-7448_35bde2b2,"{'BugID': 'CAMEL-7448', 'Summary': 'throttle EIP - unchanged value', 'Description': 'Throttler Documentation [1] states ""If the header is absent, then the Throttler uses the old value. So that allows you to only provide a header if the  value is to be changed"".\n\nhowever if the expression evaluates to null (header missing from message) the Throttler throws an exception (Throttler.java:108).\n\nThe workaround is to ensure that all messages carry the value (if the value is the same no changes will take affect). Adding an option to turn this on and off (e.g. allowNullException) would make it much easier to use (as per camel-users thread [2]).\n\n[1] http://camel.apache.org/throttler.html\n[2] http://camel.465427.n5.nabble.com/throttle-EIP-unchanged-value-td5751300.html\n'}"
camel,bugs-dot-jar_CAMEL-7456_02da984a,"{'BugID': 'CAMEL-7456', 'Summary': 'Camel PropertiesComponent ignores custom parser in Blueprint', 'Description': 'I have implemented a custom PropertiesParser which allows me to use system property placeholders in propertyPrefix and propertySuffix.\n\nIn my use case the propertyPrefix is defined as ""$\\{container.stage}."", where container.stage is a jvm option defined at container creation. The value is one of dev, test and prod.\n\nThis works fine in Java DSL world (SCR bundle), but custom parser is ignored in Blueprint. Here is sample of my blueprint xml:\n{code}\n <cm:property-placeholder id=""integration"" persistent-id=""org.apache.camel.sample.temp"" placeholder-prefix=""[["" placeholder-suffix=""]]"">\n    <cm:default-properties>\n        <cm:property name=""example"" value=""this value is the default""/>\n        <cm:property name=""dev.example"" value=""this value is used in development environment""/>\n        <cm:property name=""test.example"" value=""this value is used in test environment""/>\n        <cm:property name=""prod.example"" value=""this value is used in production environment""/>\n    </cm:default-properties>\n</cm:property-placeholder>\n\n<bean id=""parser"" class=""org.apache.camel.sample.MyCustomPropertiesParser""/>\n\n<!-- Load properties for current container stage -->\n<bean id=""properties"" class=""org.apache.camel.component.properties.PropertiesComponent"">\n    <property name=""propertiesParser"" ref=""parser""/>\n    <property name=""propertyPrefix"" value=""${container.stage}.""/>\n    <property name=""fallbackToUnaugmentedProperty"" value=""true""/>\n    <property name=""location"" value=""blueprint:integration,classpath:properties/temp.properties""/></bean>\n\n<camelContext id=""temp"" xmlns=""http://camel.apache.org/schema/blueprint"">\n    <route id=""exampleRoute"">\n        <from uri=""timer:foo?period=5000""/>\n        <transform>\n            <simple>{{example}}</simple>\n        </transform>\n        <to uri=""log:something""/>\n    </route>\n</camelContext>\n{code}\n\nThe reason it did not work was because by default, it uses blueprint property resolver (useBlueprintPropertyResolver=""true"") to bridge PropertiesComponent to blueprint in order to support looking up property placeholders from the Blueprint Property Placeholder Service. Then it always creates a BlueprintPropertiesParser object and set it to PropertiesComponent. \n\nThe customer Property Parser I created was only set into the BlueprintPropertiesParser object as a delegate Property Parser. Therefore, it was always the method parseUri() from the BlueprintPropertiesParser object got invoked. The same method from your custom parser was ignored. \n\nFor more detail, please take a look at org.apache.camel.blueprint.CamelContextFactoryBean.initPropertyPlaceholder() function.\n\nThe only workaround is to add the attribute useBlueprintPropertyResolver=""false"" to <camelContext> element to disable default blueprint property resolver. However, I will have to change PropertiesComponent\'s ""location"" property to remove blueprint ""blueprint:integration"" from the comma separated value list:\n{code}\n <property name=""location"" value=""classpath:properties/temp.properties""/> \n{code}\nBecause once I set it to false, I will no longer be able to lookup from blueprint property service.'}"
camel,bugs-dot-jar_CAMEL-7459_57ba1bde,"{'BugID': 'CAMEL-7459', 'Summary': ""parseQuery Drops Char When Last Parameter is RAW with value ending in ')'"", 'Description': 'org.apache.camel.util.URISupport\n\nWhen processing RAW parameters as part of parseQuery a look ahead to the next char is needed in order to determine the end of the RAW value.  The logic to prevent a _StringIndexOutOfBoundsException_ drops the last char when evaluating for _next_ char when the current char (_i_) is the second to last char of the string.\n\nThis becomes an issue when the RAW value ends in \')\' \n\nConsider:\nuri = ""foo=RAW(ba(r))""\nuri.length() = 14\ni = 12\nuri.charAt(12) = \')\'\nuri.charAt(13) = \')\'\n\n(i < uri.legnth() - 2) = 12 < (14 - 2) = 12 < 12 = false\nthus next = ""\\u0000""\n\nThe RAW value now ends satisfying the requirements and the char at index 13 is never read.  The resulting parameter is ""foo=RAW(ba(r)"".\n\nThe logic to prevent the index exception should be ""(i <*=* uri.legnth() -2)"" or ""(i < uri.legnth() - *1*)""'}"
camel,bugs-dot-jar_CAMEL-7478_69b00a31,"{'BugID': 'CAMEL-7478', 'Summary': 'Simple Language - Length of array properties is not correctly evaluated', 'Description': 'If the exchange body is an array, then {{body.length}} returns correctly the length of the array. However, if the array is a property of an object, then not the correct value is returned:\n{code:title=MyClass.java}\npublic class MyClass {\n    public Object[] getMyArray() {\n        return new Object[]{""Hallo"", ""World"", ""!""};\n    }\n}\n{code}\nAccessing the property {{myArray}} with Simple:\n{code}\n<setHeader headerName=""mySimpleHeader"">\n    <simple>body.myArray.length</simple>\n</setHeader>\n<log message=""mySimpleHeader = ${header.mySimpleHeader}"" />\n{code}\nJava:\n{code}\nfinal ProducerTemplate template = main.getCamelTemplate();\ntemplate.sendBody(""direct:start"", new MyClass());\n{code}\nLog:\n{noformat}\n[main] route1 INFO  mySimpleHeader = 1\n{noformat}\nThe return value should be {{3}} instead of {{1}}.'}"
camel,bugs-dot-jar_CAMEL-7513_85ced066,"{'BugID': 'CAMEL-7513', 'Summary': 'Using JPA entities as the argument in Aggregator using POJO', 'Description': 'I have an Aggregator POJO with this method :\n\npublic Map<Hoteles, List<EventoPrecio>> agregaEventoPrecio(Map<Hoteles, List<EventoPrecio>> lista, EventoPrecio evento) \n\nWith this route :\n\nfrom(""timer://tesipro?fixedRate=true&period=60000"").\nbeanRef(""uploadARIService"", ""getEventosPrecio"").\naggregate(constant(true), AggregationStrategies.bean(AgregadorEventos.class, ""agregaEventoPrecio"")).\ncompletionSize(100).\nlog(""Ejecucion de Quartz "");\n\nAnd I get this error :\n\nError occurred during starting Camel: CamelContext(249-camel-9) due Parameter annotations at index 1 is not supported on method: public java.util.HashMap com.tesipro.conectores.interfaces.tesiproconpush.camel.AgregadorEventos.agregaEventoPrecio(java.util.HashMap,com.tesipro.conectores.domain.EventoPrecio)          \n\nIt seems the problem is that annotations are not supported in the aggregator arguments nor in the argument class.\n\nhttps://github.com/apache/camel/blob/3f4f8e9ddcc8de32cca084927a10c5b3bceef7f9/camel-core/src/main/java/org/apache/camel/processor/aggregate/AggregationStrategyBeanInfo.java#L67'}"
camel,bugs-dot-jar_CAMEL-7562_689147e9,"{'BugID': 'CAMEL-7562', 'Summary': 'camel-test - AdviceWith in CBR may add twice', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Camel-AdviceWith-issues-tp5752786.html\n\nWhen using advice-with for a CBR it may add to the when clauses 2 times.'}"
camel,bugs-dot-jar_CAMEL-7568_b3377b16,"{'BugID': 'CAMEL-7568', 'Summary': 'OnComplete does not  work on transactioned route after rollback', 'Description': 'Example:\n{code:title=Route Sample|borderStyle=solid}\nthis.from(""servlet:///test"").routeId(""CamelTestRoute"") \n   .onCompletion() \n      .bean(this.logCompletionRoute) \n   .end() \n   .onException(Exception.class) \n      .log(LoggingLevel.ERROR, this.log, ""Error on processing message. Sending Rollback command!"") \n      .log(LoggingLevel.ERROR, this.log, ""${exception.stacktrace}"") \n      .rollback()\n      .handled(true) \n   .end() \n   .transacted(RouteTransactionConfiguration.PROPAGATION_REQUIRED) \n   .process(new Processor() { \n                @Override \n                public void process(Exchange exchange) throws Exception { \n                    throw new Exception(); \n                }}); \n{code}\n\nIn this sample, the OnCompletion bean never is executed. But, if I remove the ""rollback()"" call, it is executed properly.\n\nthanks,'}"
camel,bugs-dot-jar_CAMEL-7586_1f92fa42,"{'BugID': 'CAMEL-7586', 'Summary': 'NotCompliantMBeanException : Attribute MessageHistory has more than one getter', 'Description': ""Hello, I wasn't able to subscribe on the mailing list, so I'm posting my issue directly here.\n\nIn my project I need to use some _ManagedCamelContextMBean_, which I am trying to access through [JMX.newMBeanProxy|http://docs.oracle.com/javase/8/docs/api/javax/management/JMX.html#newMBeanProxy-javax.management.MBeanServerConnection-javax.management.ObjectName-java.lang.Class-]\n\nHowever, it is not working as I'm getting a *NotCompliantMBeanException* because the attribute _MessageHistory_ is said to have more than one getter.\n\nI checked the source code of newMBeanProxy, then the [JMX 1.4 specification|http://docs.oracle.com/javase/8/docs/technotes/guides/jmx/JMX_1_4_specification.pdf], and then Camel's source code, and it appears that ManagedCamelContextMBean is indeed not respecting the standard MBean.\n\nThe problem is that two methods are defined in _ManagedCamelContextMBean_ : isMessageHistory() and getMessageHistory()\nSince the return type is boolean, isMessageHistory is considered to be a getter, which makes two getter according to the JMX specification and is blocking the newMBeanProxy() method.""}"
camel,bugs-dot-jar_CAMEL-7611_e30f1c53,"{'BugID': 'CAMEL-7611', 'Summary': 'org.apache.camel.util.KeyValueHolder equals bug', 'Description': 'According to java.lang.Object javadoc (http://docs.oracle.com/javase/7/docs/api/java/lang/Object.html), ""equal objects must have equal hash codes"". \n\nCurrent implementation of the ""equals"" and ""hashCode"" method of the org.apache.camel.util.KeyValueHolder does not seem to follow that rule: hashCode is calculated from the key and value attributes while the equals compares only the key attribute. \n\nCould generate unexpected behaviour in certain circumstances.'}"
camel,bugs-dot-jar_CAMEL-7622_faa20255,"{'BugID': 'CAMEL-7622', 'Summary': 'advice-with - No outputs found matching id when upgrading from 2.13 to 2.14', 'Description': 'I have the following route defined with the Java DSL: \n\nfrom(""direct:localMemberLookup"").routeId(""localMemberLookup"") \n        .process(new MemberLookupToSqlParametersProcessor()).id(""sqlParams"") \n        .recipientList(simple(""sql:{{sql.memberLookup}}"")).delimiter(""false"") \n        .to(""log:output"") \n        .process(new MemberLookupProcessor()) \n        // do more processing \n        .to(""log:output""); \n\nI\'m testing it with a test that looks as follows: \n\n@EndpointInject(uri = ""mock:lookupHeaders"") \nMockEndpoint lookupHeaders; \n\n@EndpointInject(uri = ""mock:searchResult"") \nMockEndpoint searchResult; \n\n@EndpointInject(uri = ""mock:lookupResult"") \nMockEndpoint lookupResult; \n\n@Autowired \nCamelContext camelContext; \n\n@Before \npublic void before() throws Exception { \n        ModelCamelContext context = (ModelCamelContext) camelContext; \n        context.setTracing(true); \n        RouteDefinition searchRoute = context.getRouteDefinition(""memberSearchRequest""); \n        searchRoute.to(searchResult); \n\n        RouteDefinition lookupRoute = context.getRouteDefinition(""localMemberLookup""); \n        lookupRoute.adviceWith(context, new AdviceWithRouteBuilder() { \n                @Override \n                public void configure() throws Exception { \n                        weaveById(""sqlParams"").after().to(lookupHeaders); \n                } \n        }); \n        lookupRoute.to(lookupResult); \n        context.start(); \n} \n\nWith Camel 2.13.1, this works fine. However, with 2.14-SNAPSHOT, I get the following error: \n\njava.lang.IllegalArgumentException: There are no outputs which matches: sqlParams in the route \n\nMailing list thread: http://camel.465427.n5.nabble.com/weaveById-works-with-2-13-1-not-with-2-14-SNAPSHOT-td5753809.html'}"
camel,bugs-dot-jar_CAMEL-7736_7ad36e3d,"{'BugID': 'CAMEL-7736', 'Summary': 'Failure to create producer during routing slip or similar eip causes exchange causes error handler not to react properly', 'Description': 'If an endpoint.createProducer throws an exception from a dynamic eip, then the exchange is kept marked as inflight, and the error handler does not react asap and as expected.\n\nThis was working in Camel 2.10.x etc.'}"
camel,bugs-dot-jar_CAMEL-7767_eab06182,"{'BugID': 'CAMEL-7767', 'Summary': 'Mock - Defining assertion on message doest work if using convertTo', 'Description': ""See\nhttp://www.manning-sandbox.com/thread.jspa?threadID=41025&tstart=0\n\n\nThe reason is when you use a method in the fluent builder that returns a ValueBuilder then that didn't detect the predicate.""}"
camel,bugs-dot-jar_CAMEL-7795_19b2aa31,"{'BugID': 'CAMEL-7795', 'Summary': 'Regression: MDC may lose values after when Async Routing Engine is used', 'Description': ""CAMEL-6377 introduced some optimisations in the MDC Logging mechanism which make it lose MDC values when the async routing engine is used.\n\nIf we are using an async component such as CXF, the response or done() callback will be issued from a thread NOT managed by Camel. Therefore, we need the MDCCallback to reset *ALL* MDC values, not just the routeId (as was intended by the commits that caused the regression).\n\nThe situation may be salvaged by the fact that underlying MDC implementations use an InheritableThreadLocal, so the first requests after system initialisation may see correct behaviour, because the MDC values from the requesting thread is propagated to the newly initialised threads in the underlying stack's ThreadPool, as the coreThreads are being initialised within the context of the original threads which act like parent threads.\n\nBut after those first attempts, odd behaviour is seen and all responses from the async endpoint come back without an MDC.""}"
camel,bugs-dot-jar_CAMEL-7883_d57f402b,"{'BugID': 'CAMEL-7883', 'Summary': 'XSD decoding bad guess in Validator', 'Description': 'Validator component does not take imported XSD encoding into account when validating XML. That may lead to validation errors if an imported XSD is ISO-8859-1 encoded and containing non ASCII caracters, even though that XSD declares its encoding correctly in its XML prolog.\n'}"
camel,bugs-dot-jar_CAMEL-7973_799b45df,"{'BugID': 'CAMEL-7973', 'Summary': 'CircuitBreakerLoadBalancer fails on async processors', 'Description': ""The CircuitBreakerLoadBalancer works fine on direct synchronous processor, but it seems to not behave as expected in case of async processor.\n\nTo reproduce the error, it's enough to add a .threads(1) before the mock processor in the CircuitBreakerLoadBalancerTest routeBuilder configuration.\n\nThis misbehaviour seems to be related to the use of the AsyncProcessorConverterHelper to force any processor to behave like asynchronous. \n\nI'm going to propose a patch with the failing test and a proposal of solution.\n\nEDIT:\n\nthe patch contains the fix also to other unexpected behaviour of the CircuitBreaker.\n\nThe second problem addressed is that, after the opening of the circuit, the RejectedExecutionException raised by the circuit breaker is set in the Exchange, but it doesn't return. This cause the processor will receive the Exchange even if the circuit is open. In this case also, if the CircuitBreaker is instructed to react only to specific Exception, it will close the circuit after the following request, because the raised exception would be a RejectedExecutionException instead of the one specified in the configuration.""}"
camel,bugs-dot-jar_CAMEL-7990_d581c4a4,"{'BugID': 'CAMEL-7990', 'Summary': 'IdempotentConsumer - If no messageId should allow Camel error handler to react', 'Description': 'See SO\nhttp://stackoverflow.com/questions/26453348/camel-onexception-doesnt-catch-nomessageidexception-of-idempotentconsumer\n\nThe idempotent consumer should set the exchange on the exchange and invoke the callback, that is an internal routing engine bug in the implementation of that eip.'}"
camel,bugs-dot-jar_CAMEL-8053_cac72b14,"{'BugID': 'CAMEL-8053', 'Summary': 'Memory leak when adding/removing a lot of routes', 'Description': 'Dynamically adding/removing routes to camel causes registrations in org.apache.camel.builder.ErrorHandlerBuilderRef.handlers (Map<RouteContext, ErrorHandlerBuilder>) for RouteContext instances. Those never get removed and can cause leaks if memory consuming objects are attached in the RouteContext for example constant definitions.'}"
camel,bugs-dot-jar_CAMEL-8081_2e985f9b,"{'BugID': 'CAMEL-8081', 'Summary': 'Multicast Aggregator should keep processing other exchange which is not timeout', 'Description': 'It makes sense the multicast aggregator keep processing the exchange even some exchange are timeout. \nHere is [a thread|http://camel.465427.n5.nabble.com/Multicast-with-multiple-timeouts-tp5759576p5759646.html] in the camel user mailing list talks about it.'}"
camel,bugs-dot-jar_CAMEL-8106_39ccf5d6,"{'BugID': 'CAMEL-8106', 'Summary': 'XML parsing error is ignored by xtoknize XML tokenizer', 'Description': 'XML parsing exceptions are ignored by xtokenize XML tokenizer and this is leading to the same token extracted repeated times.\n'}"
camel,bugs-dot-jar_CAMEL-8125_36e7b668,"{'BugID': 'CAMEL-8125', 'Summary': 'PropertyInject gives NullPointerException', 'Description': 'Using the annotation @PropertyInject on a field of the RouteBuilder class gives a NullPointerException\n\npublic class RouteBuilder extends SpringRouteBuilder {\n\t\n\t@PropertyInject(""foo.bar"")\n\tprivate String fooBar;\n        ...\n}\n\nUsing the {{ }} notation in endpoint URIs is working though.\n\n\t\n\n\n'}"
camel,bugs-dot-jar_CAMEL-8137_53b4e90c,"{'BugID': 'CAMEL-8137', 'Summary': 'Simple language does not resolve overloaded method calls', 'Description': ""I am having an issue with the Simple language. I have a property named {{myFile}} with a value of a {{java.nio.file.Path}} object. When I try to use the following expression {noformat} ${property.file.getFileName} {noformat} in order to invoke the getFileName() method I get an exception saying:\n{noformat}\nAmbiguous method invocations possible: [public sun.nio.fs.UnixPath.getFileName(), public abstract java.nio.file.Path java.nio.file.Path.getFileName()]\n{noformat}\n\nI am able to use SpEL if I do\n{noformat}\n#{properties[myFile].getFileName()}\n{noformat}\n\nIt would be nice if Simple supported this as well so I wouldn't have to go through hoops in order to use SpEL since I can't use SpEL to specify parameters in a uri.""}"
camel,bugs-dot-jar_CAMEL-8146_17475d80,"{'BugID': 'CAMEL-8146', 'Summary': 'Starting and stopping routes leak threads', 'Description': 'Seems to be identical consequence as with previous issue CAMEL-5677, but perhaps due to a different cause.\n\nHaving a file or SFTP based route, trying something like:\n{code}\nfor (int i = 0; i < 50; i++) {\n    camelContext.startRoute(routeId);\n    camelContext.stopRoute(routeId);\n}\n{code}\nresults in 50 orphan threads of this type:\n\n{code}\n""Camel (camel) thread #231 - sftp://user@host/path"" #10170 daemon prio=5 os_prio=0 tid=0x00007fa4b46a5800 nid=0x10fc waiting on condition [0x00007fa452934000]\n   java.lang.Thread.State: TIMED_WAITING (parking)\n      at sun.misc.Unsafe.park(Native Method)\n      - parking to wait for  <0x00000000b83dc900> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n      at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n      at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n      at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)\n      at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)\n      at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n      at java.lang.Thread.run(Thread.java:745)\n{code}\n\nSwitching to suspend/resume solves the problem, however I guess the start/stop issue should be addressed.\n'}"
camel,bugs-dot-jar_CAMEL-8227_54d7fc59,"{'BugID': 'CAMEL-8227', 'Summary': 'Using exchangePattern=InOnly in to uris are not used', 'Description': 'Related to CAMEL-5301\n\nWhich was implemented for recipient list. But the same thing should be fixed/implemented for send processor as well.\n\nSee nabble\nhttp://camel.465427.n5.nabble.com/Rest-DSL-org-apache-camel-ExchangeTimedOutException-The-OUT-message-was-not-received-within-20000-mis-tp5761530.html'}"
camel,bugs-dot-jar_CAMEL-8584_dd0f74c0,"{'BugID': 'CAMEL-8584', 'Summary': 'Circuit breaker does not honour halfOpenAfter period', 'Description': ""The CircuitBreakerLoadBalancer will always switch to a half-open state immediately after the first rejected message instead of honouring the halfOpenAfter period.\n\nIt's due to the failed message count getting reset in the rejectExchange method:\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/processor/loadbalancer/CircuitBreakerLoadBalancer.java#L207""}"
camel,bugs-dot-jar_CAMEL-8592_57f72cd9,"{'BugID': 'CAMEL-8592', 'Summary': 'NPE in AbstractListAggregationStrategy if empty list', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/NullPointerException-on-empty-List-in-AbstractListAggregationStrategy-tp5764965.html'}"
camel,bugs-dot-jar_CAMEL-8624_597883fa,"{'BugID': 'CAMEL-8624', 'Summary': 'Bean component - Potential NPE in BeanInfo', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/transformers-not-working-after-update-to-2-15-1-tp5765600.html'}"
camel,bugs-dot-jar_CAMEL-8626_d063f471,"{'BugID': 'CAMEL-8626', 'Summary': 'Leaking exchangesInFlightKeys in ManagedRoute', 'Description': 'Having a camel context with a single route:\n{code}\n        onException(Throwable.class)\n                .handled(true)\n                .process(handleException()); // essentially  doing exchange.setException(someConvertedException);\n\n        from(""direct:generalFlow"")\n                .routingSlip(property(GeneralFlowRoute.class.getName()));\n{code}\n\nstarted from Spring:\n{code}\n    <camelContext id=""flows"" xmlns=""http://camel.apache.org/schema/spring"">\n        <template id=""template"" defaultEndpoint=""direct:generalFlow""/>\n        <routeBuilder ref=""generalFlow""/>\n    </camelContext>\n\n    <bean id=""generalFlow"" class=""com.blabla.GeneralFlowRoute""/>\n{code}\n\nDuring performance test both exchangesInFlightKeys  and exchangesInFlightStartTimestamps are accumulating over time.\n\nBut if the test is run in one thread with debug - nothing is accumulated.\n\nIssue found after migration from 2.14.1 to 2.15.1\n'}"
camel,bugs-dot-jar_CAMEL-8954_7b1253db,"{'BugID': 'CAMEL-8954', 'Summary': 'Lock information is not handovered together with Exchange on-completion synchronizations', 'Description': 'This applies to the file components when using common read-lock strategies:\n\n- *markerFile* - org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy\n- *fileLock* - org.apache.camel.component.file.strategy.FileLockExclusiveReadLockStrategy\n\nThis strategies stores lock information in the Exchange properties:\n\n- *Exchange.FILE_LOCK_FILE_ACQUIRED* == ""CamelFileLockFileAcquired""\n- *Exchange.FILE_LOCK_FILE_NAME* == ""CamelFileLockFileName""\n- *Exchange.FILE_LOCK_EXCLUSIVE_LOCK* == ""CamelFileLockExclusiveLock""\n- *Exchange.FILE_LOCK_RANDOM_ACCESS_FILE* == ""CamelFileLockRandomAccessFile""\n\nLock information is stored as scalar values and can hold information about _only one single lock_.\n\nWhen there are two Exchanges participates in the route, share UoW, and synchronizations are handovered from one Exchange to another, information about both locks can\'t be stored in the Exchange properties and lost. Consequently when on-completion synchronizations are performed, read-lock strategies can\'t access information about all the locks and they are not released.\n\nFor example, after completing this route lock for file1.dat is not released:\n{code:java}\nfrom(""file:data/input-a?fileName=file1.dat&readLock=markerFile"")\n    .pollEnrich(""file:data/input-b?fileName=file2.dat&readLock=markerFile"")\n    .to(""mock:result"");\n{code}\n'}"
camel,bugs-dot-jar_CAMEL-8964_ea8ee025,"{'BugID': 'CAMEL-8964', 'Summary': 'CamelContext - API for control routes may cause Route not to update it state', 'Description': 'See CAMEL-8963\n\nIts the Route instance that do not update it state as well. But the RouteService has the correct state. So one can be Started and the other Suspended.'}"
camel,bugs-dot-jar_CAMEL-9032_108d94f7,"{'BugID': 'CAMEL-9032', 'Summary': 'Bean component - Should filter out abstract methods', 'Description': 'If you call a method on a bean then the introspector should filter out abstract methods if there is class inheritance with abstract defined methods.\n\nSee SO\nhttp://stackoverflow.com/questions/31671894/camel-ambiguousmethodcallexception-abstract-classes'}"
camel,bugs-dot-jar_CAMEL-9124_9da2c05a,"{'BugID': 'CAMEL-9124', 'Summary': 'RedeliveryPattern should support property placeholders', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Can-t-configure-delayPattern-with-property-placeholders-tp5771356.html'}"
camel,bugs-dot-jar_CAMEL-9143_08077733,"{'BugID': 'CAMEL-9143', 'Summary': 'Producers that implement the ServicePoolAware interface cause memory leak due to JMX references', 'Description': 'h4. Description\n\nProducer instances that implement the ServicePoolAware interface will leak memory if their route is stopped, with new producers being leaked every time the route is started/stopped.\n\nKnown implementations that are affected are RemoteFileProducer (ftp, sftp) and Mina2Producer.\n\nThis is due to the behaviour that the SendProcessor which when the route is stopped it shuts down it\'s `producerCache` instance.\n\n{code}\n    protected void doStop() throws Exception {\n        ServiceHelper.stopServices(producerCache, producer);\n    }\n{code}\n\nthis in turn calls `stopAndShutdownService(pool)` which will call stop on the SharedProducerServicePool instance which is a NOOP however it also calls shutdown which effects a stop of the global pool (this stops all the registered services and then clears the pool.\n\n{code}\n    protected void doStop() throws Exception {\n        // when stopping we intend to shutdown\n        ServiceHelper.stopAndShutdownService(pool);\n        try {\n            ServiceHelper.stopAndShutdownServices(producers.values());\n        } finally {\n            // ensure producers are removed, and also from JMX\n            for (Producer producer : producers.values()) {\n                getCamelContext().removeService(producer);\n            }\n        }\n        producers.clear();\n    }\n{code}\n\nHowever no call to `context.removeService(Producer) is called for the entries from the pool only those singleton instances that were in the `producers` map hence the JMX `ManagedProducer` that is created when `doGetProducer` invokes {code}                getCamelContext().addService(answer, false);\n{code} is never removed. \n\nSince the global pool is empty when the next request to get a producer is called a new producer is created, jmx wrapper and all, whilst the old instance remains orphaned retaining any objects that pertain to that instance.\n\nOne workaround is for the producer to call {code}getEndpoint().getCamelContext().removeService(this){code} in it\'s stop method, however this is fairly obscure and it would probably be better to invoke removal of the producer when it is removed from the shared pool.\n\nAnother issue of note is that when a route is shutdown that contains a SendProcessor due to the shutdown invocation on the SharedProcessorServicePool the global pool is cleared of `everything` and remains in `Stopped` state until another route starts it (although it is still accessed and used whilst in the `Stopped` state).\n\nh4. Impact\n\nFor general use where there is no dynamic creation or passivation of routes this issue should be minimal, however in our use case where the routes are not static, there is a certain amount of recreation of routes as customer endpoints change and there is a need to passivate idle routes this causes a considerable memory leak (via SFTP in particular).\n\nh4. Test Case\n{code}\npackage org.apache.camel.component;\n\nimport com.google.common.util.concurrent.AtomicLongMap;\n\nimport org.apache.camel.CamelContext;\nimport org.apache.camel.Consumer;\nimport org.apache.camel.Endpoint;\nimport org.apache.camel.Exchange;\nimport org.apache.camel.Processor;\nimport org.apache.camel.Producer;\nimport org.apache.camel.Route;\nimport org.apache.camel.Service;\nimport org.apache.camel.ServicePoolAware;\nimport org.apache.camel.ServiceStatus;\nimport org.apache.camel.builder.RouteBuilder;\nimport org.apache.camel.impl.DefaultComponent;\nimport org.apache.camel.impl.DefaultEndpoint;\nimport org.apache.camel.impl.DefaultProducer;\nimport org.apache.camel.support.LifecycleStrategySupport;\nimport org.apache.camel.support.ServiceSupport;\nimport org.apache.camel.test.junit4.CamelTestSupport;\nimport org.junit.Test;\n\nimport java.util.Map;\n\nimport static com.google.common.base.Preconditions.checkNotNull;\n\n/**\n * Test memory behaviour of producers using {@link ServicePoolAware} when using JMX.\n */\npublic class ServicePoolAwareLeakyTest extends CamelTestSupport {\n\n  private static final String LEAKY_SIEVE_STABLE = ""leaky://sieve-stable?plugged=true"";\n  private static final String LEAKY_SIEVE_TRANSIENT = ""leaky://sieve-transient?plugged=true"";\n\n\n  private static boolean isPatchApplied() {\n    return Boolean.parseBoolean(System.getProperty(""patchApplied"", ""false""));\n  }\n\n  /**\n   * Component that provides leaks producers.\n   */\n  private static class LeakySieveComponent extends DefaultComponent {\n    @Override\n    protected Endpoint createEndpoint(String uri, String remaining, Map<String, Object> parameters) throws Exception {\n      boolean plugged = ""true"".equalsIgnoreCase((String) parameters.remove(""plugged""));\n      return new LeakySieveEndpoint(uri, isPatchApplied() && plugged);\n    }\n  }\n\n  /**\n   * Endpoint that provides leaky producers.\n   */\n  private static class LeakySieveEndpoint extends DefaultEndpoint {\n\n    private final String uri;\n    private final boolean plugged;\n\n    public LeakySieveEndpoint(String uri, boolean plugged) {\n      this.uri = checkNotNull(uri, ""uri must not be null"");\n      this.plugged = plugged;\n    }\n\n    @Override\n    public Producer createProducer() throws Exception {\n      return new LeakySieveProducer(this, plugged);\n    }\n\n    @Override\n    public Consumer createConsumer(Processor processor) throws Exception {\n      throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public boolean isSingleton() {\n      return true;\n    }\n\n    @Override\n    protected String createEndpointUri() {\n      return uri;\n    }\n  }\n\n  /**\n   * Leaky producer - implements {@link ServicePoolAware}.\n   */\n  private static class LeakySieveProducer extends DefaultProducer implements ServicePoolAware {\n\n    private final boolean plugged;\n\n    public LeakySieveProducer(Endpoint endpoint, boolean plugged) {\n      super(endpoint);\n      this.plugged = plugged;\n    }\n\n    @Override\n    public void process(Exchange exchange) throws Exception {\n      // do nothing\n    }\n\n    @Override\n    protected void doStop() throws Exception {\n      super.doStop();\n\n      //noinspection ConstantConditions\n      if (plugged) {\n        // need to remove self from services since we are ServicePoolAware this will not be handled for us otherwise we\n        // leak memory\n        getEndpoint().getCamelContext().removeService(this);\n      }\n    }\n  }\n\n  @Override\n  protected boolean useJmx() {\n    // only occurs when using JMX as the GC root for the producer is through a ManagedProducer created by the\n    // context.addService() invocation\n    return true;\n  }\n\n  /**\n   * Returns true if verification of state should be performed during the test as opposed to at the end.\n   */\n  public boolean isFailFast() {\n    return false;\n  }\n\n  /**\n   * Returns true if during fast failure we should verify that the service pool remains in the started state.\n   */\n  public boolean isVerifyProducerServicePoolRemainsStarted() {\n    return false;\n  }\n\n  @Override\n  public boolean isUseAdviceWith() {\n    return true;\n  }\n\n  @Test\n  public void testForMemoryLeak() throws Exception {\n    registerLeakyComponent();\n\n    final AtomicLongMap<String> references = AtomicLongMap.create();\n\n    // track LeakySieveProducer lifecycle\n    context.addLifecycleStrategy(new LifecycleStrategySupport() {\n      @Override\n      public void onServiceAdd(CamelContext context, Service service, Route route) {\n        if (service instanceof LeakySieveProducer) {\n          references.incrementAndGet(((LeakySieveProducer) service).getEndpoint().getEndpointKey());\n        }\n      }\n\n      @Override\n      public void onServiceRemove(CamelContext context, Service service, Route route) {\n        if (service instanceof LeakySieveProducer) {\n          references.decrementAndGet(((LeakySieveProducer) service).getEndpoint().getEndpointKey());\n        }\n      }\n    });\n\n    context.addRoutes(new RouteBuilder() {\n      @Override\n      public void configure() throws Exception {\n        from(""direct:sieve-transient"")\n            .id(""sieve-transient"")\n            .to(LEAKY_SIEVE_TRANSIENT);\n\n        from(""direct:sieve-stable"")\n            .id(""sieve-stable"")\n            .to(LEAKY_SIEVE_STABLE);\n      }\n    });\n\n    context.start();\n\n    for (int i = 0; i < 1000; i++) {\n      ServiceSupport service = (ServiceSupport) context.getProducerServicePool();\n      assertEquals(ServiceStatus.Started, service.getStatus());\n      if (isFailFast()) {\n        assertEquals(2, context.getProducerServicePool().size());\n        assertEquals(1, references.get(LEAKY_SIEVE_TRANSIENT));\n        assertEquals(1, references.get(LEAKY_SIEVE_STABLE));\n      }\n\n      context.stopRoute(""sieve-transient"");\n\n      if (isFailFast()) {\n        assertEquals(""Expected no service references to remain"", 0, references.get(LEAKY_SIEVE_TRANSIENT));\n      }\n\n      if (isFailFast()) {\n        // looks like we cleared more than just our route, we\'ve stopped and cleared the global ProducerServicePool\n        // since SendProcessor.stop() invokes ServiceHelper.stopServices(producerCache, producer); which in turn invokes\n        // ServiceHelper.stopAndShutdownService(pool);.\n        //\n        // Whilst stop on the SharedProducerServicePool is a NOOP shutdown is not and effects a stop of the pool.\n\n        if (isVerifyProducerServicePoolRemainsStarted()) {\n         assertEquals(ServiceStatus.Started, service.getStatus());\n        }\n        assertEquals(""Expected one stable producer to remain pooled"", 1, context.getProducerServicePool().size());\n        assertEquals(""Expected one stable producer to remain as service"", 1, references.get(LEAKY_SIEVE_STABLE));\n      }\n\n      // Send a body to verify behaviour of send producer after another route has been stopped\n      sendBody(""direct:sieve-stable"", """");\n\n      if (isFailFast()) {\n        // shared pool is used despite being \'Stopped\'\n        if (isVerifyProducerServicePoolRemainsStarted()) {\n          assertEquals(ServiceStatus.Started, service.getStatus());\n        }\n\n        assertEquals(""Expected only stable producer in pool"", 1, context.getProducerServicePool().size());\n        assertEquals(""Expected no references to transient producer"", 0, references.get(LEAKY_SIEVE_TRANSIENT));\n        assertEquals(""Expected reference to stable producer"", 1, references.get(LEAKY_SIEVE_STABLE));\n      }\n\n      context.startRoute(""sieve-transient"");\n\n      // ok, back to normal\n      assertEquals(ServiceStatus.Started, service.getStatus());\n      if (isFailFast()) {\n        assertEquals(""Expected both producers in pool"", 2, context.getProducerServicePool().size());\n        assertEquals(""Expected one transient producer as service"", 1, references.get(LEAKY_SIEVE_TRANSIENT));\n        assertEquals(""Expected one stable producer as service"", 1, references.get(LEAKY_SIEVE_STABLE));\n      }\n    }\n\n    if (!isFailFast()) {\n      assertEquals(""Expected both producers in pool"", 2, context.getProducerServicePool().size());\n\n      // if not fixed these will equal the number of iterations in the loop + 1\n      assertEquals(""Expected one transient producer as service"", 1, references.get(LEAKY_SIEVE_TRANSIENT));\n      assertEquals(""Expected one stable producer as service"", 1, references.get(LEAKY_SIEVE_STABLE));\n    }\n  }\n\n  private void registerLeakyComponent() {\n    // register leaky component\n    context.addComponent(""leaky"", new LeakySieveComponent());\n  }\n}\n{code}'}"
camel,bugs-dot-jar_CAMEL-9217_e7ac45b6,"{'BugID': 'CAMEL-9217', 'Summary': 'URI validation verifies usage of & char incorrectly', 'Description': 'Hello Camel team,\n\nI have faced a URI validation issue that does not allow me to use a correct file producer URI if it does not contain parameters. My file endpoint URI looks like this:\n{code}\nraw form:             file:D:\\camel_test\\test&run\nin encoded format:  file:D%3A%5Ccamel_test%5Ctest%26run\n{code}\n\nAs you can see this is a simple file endpoint URI which does not contain any parameters, please note that the target folder name contains \'&\' char.\n\nWhen I try to start a route for this endpoint I get the following error:\n{code}\nCaused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Invalid uri syntax: no ? marker however the uri has & parameter separators. Check the uri if its missing a ? marker.\nat org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:547)\nat org.apache.camel.util.CamelContextHelper.getMandatoryEndpoint(CamelContextHelper.java:72)\nat org.apache.camel.model.RouteDefinition.resolveEndpoint(RouteDefinition.java:202)\nat org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:107)\nat org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:113)\nat org.apache.camel.model.SendDefinition.resolveEndpoint(SendDefinition.java:61)\nat org.apache.camel.model.SendDefinition.createProcessor(SendDefinition.java:55)\nat org.apache.camel.model.ProcessorDefinition.makeProcessor(ProcessorDefinition.java:500)\nat org.apache.camel.model.ProcessorDefinition.addRoutes(ProcessorDefinition.java:213)\nat org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:942)\nCaused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest&run due to: Invalid uri syntax: no ? marker however the uri has & parameter separators. Check the uri if its missing a ? marker.\nat org.apache.camel.impl.DefaultComponent.validateURI(DefaultComponent.java:210)\nat org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:115)\nat org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:527)\n{code}\n\nThe issue is that I have an \'&\' char in my folder path and do not have URI parameters. As a result the DefaultComponent.validateURI() function throws the exception at this point:\n{code}\nif (uri.contains(""&"") && !uri.contains(""?"")) {\n            throw new ResolveEndpointFailedException(uri, ""Invalid uri syntax: no ? marker however the uri ""\n                + ""has & parameter separators. Check the uri if its missing a ? marker."");\n        }\n{code}\n\nI think the issue is that for some reason & char at this point is in decoded form which is incorrect because it should be URI encoded here. I know that Camel has issues with + and & chars and in case of URI parameters we can use RAW() wrapper as a workaround. But as far as I can see we cannot do the same in base endpoint URI.\n\nPlease note that if I add an URI parameter (any parameter works) the issue disappears and the event target works fine.\nFor example:\n{code}\nraw form:             file:D:\\camel_test\\test&run?forceWrites=true\nin encoded format:  file:D%3A%5Ccamel_test%5Ctest%26run?forceWrites=true\n{code}\n\nI also found that Camel cannot normally handle URIs with ? char in base URI string. For example in case of File endpoint it tries to handle the second part of the base URI (which follows the ? char) as a parameter which is incorrect because ? char was correctly encoded and should not be used as the point where parameters string starts. If I try to use ? char in bucket name for S3 endpoint the part after ? is simply ignored.\nFor example the following URI produces an error:\n{code}\nraw form:             file:D:\\camel_test\\test?run?forceWrites=true\nin encoded format:  file:D%3A%5Ccamel_test%5Ctest%3Frun?forceWrites=true\n{code}\nError:\n{code}\n2015-10-13 13:26:38,285 INFO [com.informatica.saas.infaagentv3.agentcore.TomcatManager] - Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file://D:%5Ccamel_test%5Ctest?run%3FforceWrites=true due to: There are 1 parameters that couldn\'t be set on the endpoint. Check the uri if the parameters are spelt correctly and that they are properties of the endpoint. Unknown parameters=[{run?forceWrites=true}]\nat org.apache.camel.impl.DefaultComponent.validateParameters(DefaultComponent.java:192)\nat org.apache.camel.impl.DefaultComponent.createEndpoint(DefaultComponent.java:137)\nat org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:527)\n{code}\n\nAnd yes I know that ? char in a file path is not the best idea, I used the above example to illustrate a global camel issue.'}"
camel,bugs-dot-jar_CAMEL-9238_169b981e,"{'BugID': 'CAMEL-9238', 'Summary': 'NPE while GenericFile.changeFileName', 'Description': 'If a relative file path is specified for the {{move}}\xa0or {{moveFailed}} Attribute of the file2 component, a NullPointerException is thrown while processing the onCompletion commit resp. rollback strategy.\n\nAnd because the processed file cannot be moved away, the processing is restarted again and so on...\n\nWrong code line (GenericFile.java:203 in camel-core V2.15.3):\n{code:java}\nObjectHelper.after(newFileName, newEndpointPath + File.separatorChar);\n{code}\nwhen {{newFileName}}\xa0and {{newEndpointPath}} are both relative paths.\n\n\nStacktrace:\n{code:java}\njava.lang.NullPointerException\n\tat java.io.File.<init>(File.java:277) ~[?:1.8.0_60]\n\tat org.apache.camel.component.file.GenericFile.changeFileName(GenericFile.java:207) ~[camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.strategy.GenericFileExpressionRenamer.renameFile(GenericFileExpressionRenamer.java:41) ~[camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.strategy.GenericFileRenameProcessStrategy.commit(GenericFileRenameProcessStrategy.java:87) ~[camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileOnCompletion.processStrategyCommit(GenericFileOnCompletion.java:124) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileOnCompletion.onCompletion(GenericFileOnCompletion.java:80) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileOnCompletion.onComplete(GenericFileOnCompletion.java:54) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.util.UnitOfWorkHelper.doneSynchronizations(UnitOfWorkHelper.java:104) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.impl.DefaultUnitOfWork.done(DefaultUnitOfWork.java:229) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.util.UnitOfWorkHelper.doneUow(UnitOfWorkHelper.java:65) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.processor.CamelInternalProcessor$UnitOfWorkProcessorAdvice.after(CamelInternalProcessor.java:637) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.processor.CamelInternalProcessor$UnitOfWorkProcessorAdvice.after(CamelInternalProcessor.java:605) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.processor.CamelInternalProcessor$InternalCallback.done(CamelInternalProcessor.java:239) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.processor.Pipeline.process(Pipeline.java:106) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:190) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:439) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:211) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:175) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.impl.ScheduledPollConsumer.doRun(ScheduledPollConsumer.java:174) [camel-core-2.15.3.jar:2.15.3]\n\tat org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:101) [camel-core-2.15.3.jar:2.15.3]\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_60]\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_60]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_60]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_60]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_60]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_60]\n{code}'}"
camel,bugs-dot-jar_CAMEL-9243_1957a828,"{'BugID': 'CAMEL-9243', 'Summary': 'Invocation of Bean fails when Bean extends and abstract which implements the actual method', 'Description': 'The issue described here does NOT exist in 2.15.2 and only manifests in 2.15.3.\n\nWith the following definition of a Bean:\n\n{code}\n    public interface MyBaseInterface {\n        @Handler\n        String hello(@Body String hi);\n    }\n\n    public abstract static class MyAbstractBean implements MyBaseInterface {\n        public String hello(@Body String hi) {\n            return ""Hello "" + hi;\n        }\n        public String doCompute(String input) {\n            fail(""Should not invoke me"");\n            return null;\n        }\n    }\n\n    public static class MyConcreteBean extends MyAbstractBean {\n    }\n\n{code}\n\nThe following test case will fail to invoke the proper method:\n\n{code}\npublic class BeanHandlerMethodTest extends ContextTestSupport {\n\n    public void testInterfaceBeanMethod() throws Exception {\n        BeanInfo info = new BeanInfo(context, MyConcreteBean.class);\n\n        Exchange exchange = new DefaultExchange(context);\n        MyConcreteBean pojo = new MyConcreteBean();\n        MethodInvocation mi = info.createInvocation(pojo, exchange);\n        assertNotNull(mi);\n        assertEquals(""hello"", mi.getMethod().getName());\n    }\n{code}\n\nThe issue is how BeanInfo.introspect determines which methods are available to be invoked.\n\nAt line 344, if the class is public, the interface methods are added to the list:\n\n{code}\n        if (Modifier.isPublic(clazz.getModifiers())) {\n            // add additional interface methods\n            List<Method> extraMethods = getInterfaceMethods(clazz);\n            for (Method target : extraMethods) {\n                for (Method source : methods) {\n                    if (ObjectHelper.isOverridingMethod(source, target, false)) {\n                        overrides.add(target);\n                    }\n                }\n            }\n            // remove all the overrides methods\n            extraMethods.removeAll(overrides);\n            methods.addAll(extraMethods);\n        }\n{code}\n\nHowever, all the methods from the interface are ""abstract"".  Later, when the real implementation is encountered as the code crawls up the tree, the abstract method is not replaced:\n\nLine 390:\n\n{code}\n        MethodInfo existingMethodInfo = overridesExistingMethod(methodInfo);\n        if (existingMethodInfo != null) {\n            LOG.trace(""This method is already overridden in a subclass, so the method from the sub class is preferred: {}"", existingMethodInfo);\n            return existingMethodInfo;\n        }\n{code}\n\nFinally, during the invocation, the following was added as part of 2.15.3 release:\n\nLine 561:\n\n{code}\n        removeAllAbstractMethods(localOperationsWithBody);\n        removeAllAbstractMethods(localOperationsWithNoBody);\n        removeAllAbstractMethods(localOperationsWithCustomAnnotation);\n        removeAllAbstractMethods(localOperationsWithHandlerAnnotation);\n{code}\n\nAs a result, the abstract method is removed and not invoked.\n\nI think the fix should be to see if the existingMethodInfo references an ""abstract\' method and if it does and methodInfo does not, replace the existingMethodInfo with methodInfo in the collection.\n\nThis would preserve the preferences implied with the rest of the code while properly replacing the abstract method with their proper implementations.\n'}"
camel,bugs-dot-jar_CAMEL-9269_62b2042b,"{'BugID': 'CAMEL-9269', 'Summary': 'NotifyBuilder.fromRoute() does not work for some endpoint types', 'Description': ""{{NotifyBuilder.fromRoute()}} does not work if the endpoint uri in the {{from()}} clause for a route does not match the actual endpoint uri the exchange was sent to. Because we also have the route id itself available in the exchange, we can use that as a fallback when the match on from endpoint uri doesn't work.""}"
camel,bugs-dot-jar_CAMEL-9340_1cab39f6,"{'BugID': 'CAMEL-9340', 'Summary': 'FileIdempotentRepository fails to create fileStore when no path is specified', 'Description': 'I create a FileIdempotentRepository like this:\n\n{code}\n.idempotentConsumer(fileIdempotentRepository(new File(\'ids\'))) {\n    it.in.body.id\n}\n{code}\n\nI get an error, and I traced it to:\n{noformat}\nCaused by: java.lang.NullPointerException: null\n\tat org.apache.camel.processor.idempotent.mpotentRepository.loadStore(FileIdempotentRepository.java:293) ~[camel-core-2.16.0.jar:2.16.0]\n\tat org.apache.camel.processor.idempotent.FileIdempotentRepository.doStart(FileIdempotentRepository.java:328) ~[camel-core-2.16.0.jar:2.16.0]\n{noformat}\n\nThe FileIdempotentRepository is trying to create the parent directory of the file that was specified for the file store. If a path to the file is not specified, then getParentFile() returns null. Calling .mkdirs() on that bombs.\n\nThis route works the second time it runs because then the file exists. It also works if I specify my file name as ""./ids"" instead of ""ids"".'}"
camel,bugs-dot-jar_CAMEL-9444_baece126,"{'BugID': 'CAMEL-9444', 'Summary': 'Incorrect exceptions handling from Splitter', 'Description': 'Steps to reproduce:\n1. Create global onException handler\n{code}\n<onException>\n    <exception>java.lang.Exception</exception>\n    <handled>\n        <constant>false</constant>\n    </handled>\n    <log message=""SOME MESSAGE""/>\n</onException>\n{code}\n\n2. Create 2 routes with Splitter (set shareUnitOfWork to TRUE, important)\n{code}\n<route>\n    <from uri=""timer://foo?repeatCount=1""/>\n\n    <!-- Add some value list to body here -->\n\n    <split shareUnitOfWork=""true"" stopOnException=""true"">\n        <simple>${body}</simple>\n        <to uri=""direct:handleSplit""/>\n    </split>\n</route>\n\n<route>\n    <from uri=""direct:handleSplit""/>\n    <throwException ref=""myException""/>\n</route>\n{code}\n\nExpected: string ""SOME MESSAGE"" is logged\nActual:  <log message=""SOME MESSAGE""/> is not executed at all '}"
camel,bugs-dot-jar_CAMEL-9480_0ead2cac,"{'BugID': 'CAMEL-9480', 'Summary': 'IdempotentConsumer - If exception from repo it should be able to handle by onException', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Exception-from-idempotentConsumer-not-propagating-to-onException-tp5775779.html'}"
camel,bugs-dot-jar_CAMEL-9641_9a6e6d8a,"{'BugID': 'CAMEL-9641', 'Summary': 'Simple backwards parser bug if using file', 'Description': 'See nabble\nhttp://camel.465427.n5.nabble.com/Unknown-File-Language-Syntax-tp5778208.html'}"
camel,bugs-dot-jar_CAMEL-9666_da035952,"{'BugID': 'CAMEL-9666', 'Summary': ""Safe copy of DefaultExchange does not propagate 'fault' property "", 'Description': '{{fault}} property should be copied in the following places:\n\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L100\nhttps://github.com/apache/camel/blob/master/camel-core/src/main/java/org/apache/camel/impl/DefaultExchange.java#L107\n\nConsequences:\n{{DefaultExchange#isFault()}} does not work if {{exception}} property is not set.'}"
camel,bugs-dot-jar_CAMEL-9672_84922699,"{'BugID': 'CAMEL-9672', 'Summary': 'ClassCastException with interceptFrom', 'Description': 'If a statement like\n\n{code:java}\ninterceptFrom().when(simple(""${header.foo} == \'bar\'"")).to(""mock:intercepted"");\n{code}\n\nis available in a route builder with JMX enabled the startup will fail in Camel 2.16.2 (and the current 2.17-SNAPSHOT) with a ClassCastException in line 310 of DefaultManagementObjectStrategy.\n\nThe generated processor is a FilterProcessor, but the resulting definition is a WhenDefinition not a FilterDefinition.\n\nThe reason is that CAMEL-8992 introduced a too precise class check for this.\n\nThe attached patch relexes the class constraint on the definition.'}"
camel,bugs-dot-jar_CAMEL-9673_7944093f,"{'BugID': 'CAMEL-9673', 'Summary': 'doTry .. doFinally should run the finally block for fault messages also', 'Description': 'If a message has fault flag, then a doFinally block is only executed the first processor. We should ensure the entire block is processed like we do if an exception was thrown. The same kind of logic should apply for fault.'}"
camel,bugs-dot-jar_CAMEL-9700_4d03e9de,"{'BugID': 'CAMEL-9700', 'Summary': 'seda - discardIfNoConsumers=true do not call on completions ', 'Description': 'See SO\nhttp://stackoverflow.com/questions/35938139/how-to-release-file-lock-with-camel-when-not-consuming-from-seda-queue/35940850#35940850'}"
commons-math,bugs-dot-jar_MATH-1005_91d280b7,"{'BugID': 'MATH-1005', 'Summary': 'ArrayIndexOutOfBoundsException in MathArrays.linearCombination', 'Description': 'When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line:\n\ndouble prodHighNext = prodHigh[1];\n\nlinearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.'}"
commons-math,bugs-dot-jar_MATH-1045_a4ffd393,"{'BugID': 'MATH-1045', 'Summary': 'EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity', 'Description': ""EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.\n\nThe patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.\n\n(What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)""}"
commons-math,bugs-dot-jar_MATH-1045_c979a6f0,"{'BugID': 'MATH-1045', 'Summary': 'EigenDecomposition.Solver should consider tiny values 0 for purposes of determining singularity', 'Description': ""EigenDecomposition.Solver tests for singularity by comparing eigenvalues to 0 for exact equality. Elsewhere in the class and in the code, of course, very small values are considered 0. This causes the solver to consider some singular matrices as non-singular.\n\nThe patch here includes a test as well showing the behavior -- the matrix is clearly singular but isn't considered as such since one eigenvalue are ~1e-14 rather than exactly 0.\n\n(What I am not sure of is whether we should really be evaluating the *norm* of the imaginary eigenvalues rather than real/imag components separately. But the javadoc says the solver only supports real eigenvalues anyhow, so it's kind of moot since imag=0 for all eigenvalues.)""}"
commons-math,bugs-dot-jar_MATH-1051_bda25b40,"{'BugID': 'MATH-1051', 'Summary': 'EigenDecomposition may not converge for certain matrices', 'Description': 'Jama-1.0.3 contains a bugfix for certain matrices where the original code goes into an infinite loop.\n\nThe commons-math translations would throw a MaxCountExceededException, so fails to compute the eigen decomposition.\n\nPort the fix from jama to CM.'}"
commons-math,bugs-dot-jar_MATH-1058_4ebd967c,"{'BugID': 'MATH-1058', 'Summary': 'Beta, LogNormalDistribution, WeibullDistribution give slightly wrong answer for extremely small args due to log/exp inaccuracy', 'Description': ""Background for those who aren't familiar: math libs like Math and FastMath have two mysterious methods, log1p and expm1. log1p(x) = log(1+x) and expm1(x) = exp(x)-1 mathetmatically, but can return a correct answer even when x was small, where floating-point error due to the addition/subtraction introduces a relatively large error.\n\nThere are three instances in the code that can employ these specialized methods and gain a measurable improvement in accuracy. See patch and tests for an example -- try the tests without the code change to see the error.""}"
commons-math,bugs-dot-jar_MATH-1065_996c0c16,"{'BugID': 'MATH-1065', 'Summary': 'EnumeratedRealDistribution.inverseCumulativeProbability returns values not in the samples set', 'Description': 'The method EnumeratedRealDistribution.inverseCumulativeProbability() sometimes returns values that are not in the initial samples domain...\nI will attach a test to exploit this bug.\n'}"
commons-math,bugs-dot-jar_MATH-1067_aff82362,"{'BugID': 'MATH-1067', 'Summary': 'Stack overflow in Beta.regularizedBeta', 'Description': 'In org.apache.commons.math3.special.Beta.regularizedBeta(double,double,double,double,int), the case\n\n } else if (x > (a + 1.0) / (a + b + 2.0)) {\n      ret = 1.0 - regularizedBeta(1.0 - x, b, a, epsilon, maxIterations);\n} \n\nis prone to infinite recursion: If x is approximately the tested value, then 1-x is approximately the tested value in the recursion. Thus, due to loss of precision after the subtraction, this condition can be true for the recursive call as well.\n\nExample:\ndouble x= Double.longBitsToDouble(4597303555101269224L);\ndouble a= Double.longBitsToDouble(4634227472812299606L);\ndouble b = Double.longBitsToDouble(4642050131540049920L);\nSystem.out.println(x > (a + 1.0) / (a + b + 2.0));\nSystem.out.println(1-x>(b + 1.0) / (b + a + 2.0));\nSystem.out.println(1-(1-x)>(a + 1.0) / (a + b + 2.0));\n\nPossible solution: change the condition to\nx > (a + 1.0) / (a + b + 2.0) && 1-x<=(b + 1.0) / (b + a + 2.0)'}"
commons-math,bugs-dot-jar_MATH-1068_b12610d3,"{'BugID': 'MATH-1068', 'Summary': 'KendallsCorrelation suffers from integer overflow for large arrays.', 'Description': 'For large array size (say, over 5,000), numPairs > 10 million.\nin line 258, (numPairs - tiedXPairs) * (numPairs - tiedYPairs) possibly > 100 billion, which will cause an integer overflow, resulting in a negative number, which will result in the end result in a NaN since the square-root of that number is calculated.\nThis can easily be solved by changing line 163 to\nfinal long numPairs = ((long)n) * (n - 1) / 2; // to avoid overflow'}"
commons-math,bugs-dot-jar_MATH-1070_8e5867ed,"{'BugID': 'MATH-1070', 'Summary': 'Incorrect rounding of float', 'Description': 'package org.apache.commons.math3.util \nexample of usage of round functions of Precision class:\n\nPrecision.round(0.0f, 2, BigDecimal.ROUND_UP) = 0.01\nPrecision.round((float)0.0, 2, BigDecimal.ROUND_UP) = 0.01\nPrecision.round((float) 0.0, 2) = 0.0\nPrecision.round(0.0, 2, BigDecimal.ROUND_UP) = 0.0\n\nSeems the reason is usage of extending float to double inside round functions and getting influence of memory trash as value.\n\nI think, same problem will be found at usage of other round modes.\n'}"
commons-math,bugs-dot-jar_MATH-1080_b285f170,"{'BugID': 'MATH-1080', 'Summary': 'The LinearConstraintSet shall return its constraints in a deterministic way', 'Description': 'As previously discussed on the mailinglist, the LinearConstraintSet should return its internally stored LinearConstraints in the same iteration order as they have been provided via its constructor.\n\nThis ensures that the execution of the same linear problem results in the same results each time it is executed. This is especially important when linear problems are loaded from a file, e.g. mps format, and makes it simpler to debug problems and compare with other solvers which do the same thing.'}"
commons-math,bugs-dot-jar_MATH-1088_63d88c74,"{'BugID': 'MATH-1088', 'Summary': 'MultidimensionalCounter does not throw ""NoSuchElementException""', 'Description': 'The iterator should throw when ""next()"" is called even though ""hasNext()"" would return false.\n'}"
commons-math,bugs-dot-jar_MATH-1089_e91d0f05,"{'BugID': 'MATH-1089', 'Summary': 'Precision.round() returns different results when provided negative zero as double or float', 'Description': 'Precision.round(-0.0d, x) = 0.0\nPrecision.round(-0.0f, x) = -0.0\n\nAfter discussion on the mailinglist, the result should always be -0.0.'}"
commons-math,bugs-dot-jar_MATH-1093_7cfbc0da,"{'BugID': 'MATH-1093', 'Summary': 'arcs set split covers full circle instead of being empty', 'Description': 'When splitting an arcs set using an arc very close to one of the boundaries (but not at the boundary), the algorithm confuses cases for which end - start = 2pi from cases for which end - start = epsilon.\n\nThe following test case shows such a failure:\n{code}\n    @Test\n    public void testSplitWithinEpsilon() {\n        double epsilon = 1.0e-10;\n        double a = 6.25;\n        double b = a - 0.5 * epsilon;\n        ArcsSet set = new ArcsSet(a - 1, a, epsilon);\n        Arc arc = new Arc(b, b + FastMath.PI, epsilon);\n        ArcsSet.Split split = set.split(arc);\n        Assert.assertEquals(set.getSize(), split.getPlus().getSize(),  epsilon);\n        Assert.assertNull(split.getMinus());\n    }\n{code}\n\nThe last assertion (split.getMinus() being null) fails, as with current code split.getMinus() covers the full circle from 0 to 2pi.'}"
commons-math,bugs-dot-jar_MATH-1096_19c1c3bb,"{'BugID': 'MATH-1096', 'Summary': 'implementation of smallest enclosing ball algorithm sometime fails', 'Description': 'The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.\n\nIn some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.\n\nSome very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are\n\n* WelzlEncloser2DTest.testReducingBall\n* WelzlEncloser2DTest.testLargeSamples\n* WelzlEncloser3DTest.testInfiniteLoop\n* WelzlEncloser3DTest.testLargeSamples'}"
commons-math,bugs-dot-jar_MATH-1096_faf99727,"{'BugID': 'MATH-1096', 'Summary': 'implementation of smallest enclosing ball algorithm sometime fails', 'Description': 'The algorithm for finding the smallest ball is designed in such a way the radius should be strictly increasing at each iteration.\n\nIn some cases, it is not true and one iteration has a smaller ball. In most cases, there is no consequence, there is just one or two more iterations. However, in rare cases discovered while testing 3D, this generates an infinite loop.\n\nSome very short offending cases have already been identified and added to the test suite. These cases are currently deactivated in the main repository while I am already working on them. The test cases are\n\n* WelzlEncloser2DTest.testReducingBall\n* WelzlEncloser2DTest.testLargeSamples\n* WelzlEncloser3DTest.testInfiniteLoop\n* WelzlEncloser3DTest.testLargeSamples'}"
commons-math,bugs-dot-jar_MATH-1103_a6f96306,"{'BugID': 'MATH-1103', 'Summary': 'Convergence Checker Fixes', 'Description': None}"
commons-math,bugs-dot-jar_MATH-1106_e2dc384d,"{'BugID': 'MATH-1106', 'Summary': 'LevenburgMaquardt switched evaluation and iterations', 'Description': None}"
commons-math,bugs-dot-jar_MATH-1115_2a6c6409,"{'BugID': 'MATH-1115', 'Summary': 'Constructor of PolyhedronsSet throws NullPointerException', 'Description': 'The following statement throws a NullPointerException:\nnew org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet(0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d);\n\nI found that other numbers also produce that effect. The stack trace:\njava.lang.NullPointerException\n        at org.apache.commons.math3.geometry.partitioning.BSPTree.fitToCell(BSPTree.java:297)\n        at org.apache.commons.math3.geometry.partitioning.BSPTree.insertCut(BSPTree.java:155)\n        at org.apache.commons.math3.geometry.partitioning.RegionFactory.buildConvex(RegionFactory.java:55)\n        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.buildBoundary(PolyhedronsSet.java:119)\n        at org.apache.commons.math3.geometry.euclidean.threed.PolyhedronsSet.<init>(PolyhedronsSet.java:97)'}"
commons-math,bugs-dot-jar_MATH-1117_f4c926ea,"{'BugID': 'MATH-1117', 'Summary': 'twod.PolygonsSet.getSize produces NullPointerException if BSPTree has no nodes', 'Description': 'org.apache.commons.math3.geometry.euclidean.twod.PolygonsSet.getSize() uses a tree internally:\n\nfinal BSPTree<Euclidean2D> tree = getTree(false);\n\nHowever, if that tree contains no data, it seems that the reference returned is null, which causes a subsequent NullPointerException.\n\nProbably an exception with a message (""tree has no data"") would clarify that this is an API usage error.'}"
commons-math,bugs-dot-jar_MATH-1121_5a6ccd58,"{'BugID': 'MATH-1121', 'Summary': ""Brent optimizer doesn't use the Base optimizer iteration counter"", 'Description': 'BrentOptimizer uses ""iter"" defined in ""doOptimize""  to count iterations.\nIt should ideally use the iteration counter defined for the BaseOptimizer.'}"
commons-math,bugs-dot-jar_MATH-1123_a197ba85,"{'BugID': 'MATH-1123', 'Summary': 'NPE in BSPTree#fitToCell()', 'Description': 'Hello, \nI faced a NPE using  BSPTree#fitToCell() from the SVN trunk. I fixed the problem using a small patch I will attach to the ticket.\n'}"
commons-math,bugs-dot-jar_MATH-1127_ba62c59d,"{'BugID': 'MATH-1127', 'Summary': '2.0 equal to -2.0', 'Description': 'The following test fails:\n\n{code}\n    @Test\n    public void testMath1127() {\n        Assert.assertFalse(Precision.equals(2.0, -2.0, 1));\n    }\n{code}\n'}"
commons-math,bugs-dot-jar_MATH-1129_d4f978dd,"{'BugID': 'MATH-1129', 'Summary': 'Percentile Computation errs', 'Description': 'In the following test, the 75th percentile is _smaller_ than the 25th percentile, leaving me with a negative interquartile range.\n\n{code:title=Bar.java|borderStyle=solid}\n@Test public void negativePercentiles(){\n\n        double[] data = new double[]{\n                -0.012086732064244697, \n                -0.24975668704012527, \n                0.5706168483164684, \n                -0.322111769955327, \n                0.24166759508327315, \n                Double.NaN, \n                0.16698443218942854, \n                -0.10427763937565114, \n                -0.15595963093172435, \n                -0.028075857595882995, \n                -0.24137994506058857, \n                0.47543170476574426, \n                -0.07495595384947631, \n                0.37445697625436497, \n                -0.09944199541668033\n        };\n        DescriptiveStatistics descriptiveStatistics = new DescriptiveStatistics(data);\n\n        double threeQuarters = descriptiveStatistics.getPercentile(75);\n        double oneQuarter = descriptiveStatistics.getPercentile(25);\n\n        double IQR = threeQuarters - oneQuarter;\n        \n        System.out.println(String.format(""25th percentile %s 75th percentile %s"", oneQuarter, threeQuarters ));\n        \n        assert IQR >= 0;\n        \n    }\n{code}'}"
commons-math,bugs-dot-jar_MATH-1135_a7363a2a,"{'BugID': 'MATH-1135', 'Summary': 'Bug in MonotoneChain: a collinear point landing on the existing boundary should be dropped (patch)', 'Description': 'The is a bug on the code in MonotoneChain.java that attempts to handle the case of a point on the line formed by the previous last points and the last point of the chain being constructed. When `includeCollinearPoints` is false, the point should be dropped entirely. In common-math 3,3, the point is added, which in some cases can cause a `ConvergenceException` to be thrown.\n\nIn the patch below, the data points are from a case that showed up in testing before we went to production.\n\n{code:java}\nIndex: src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\n===================================================================\n--- src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\t(revision 1609491)\n+++ src/main/java/org/apache/commons/math3/geometry/euclidean/twod/hull/MonotoneChain.java\t(working copy)\n@@ -160,8 +160,8 @@\n                 } else {\n                     if (distanceToCurrent > distanceToLast) {\n                         hull.remove(size - 1);\n+                        hull.add(point);\n                     }\n-                    hull.add(point);\n                 }\n                 return;\n             } else if (offset > 0) {\nIndex: src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\n===================================================================\n--- src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\t(revision 1609491)\n+++ src/test/java/org/apache/commons/math3/geometry/euclidean/twod/hull/ConvexHullGenerator2DAbstractTest.java\t(working copy)\n@@ -204,6 +204,24 @@\n     }\n \n     @Test\n+    public void testCollinnearPointOnExistingBoundary() {\n+        final Collection<Vector2D> points = new ArrayList<Vector2D>();\n+        points.add(new Vector2D(7.3152, 34.7472));\n+        points.add(new Vector2D(6.400799999999997, 34.747199999999985));\n+        points.add(new Vector2D(5.486399999999997, 34.7472));\n+        points.add(new Vector2D(4.876799999999999, 34.7472));\n+        points.add(new Vector2D(4.876799999999999, 34.1376));\n+        points.add(new Vector2D(4.876799999999999, 30.48));\n+        points.add(new Vector2D(6.0959999999999965, 30.48));\n+        points.add(new Vector2D(6.0959999999999965, 34.1376));\n+        points.add(new Vector2D(7.315199999999996, 34.1376));\n+        points.add(new Vector2D(7.3152, 30.48));\n+\n+        final ConvexHull2D hull = generator.generate(points);\n+        checkConvexHull(points, hull);\n+    }\n+\n+    @Test\n     public void testIssue1123() {\n \n         List<Vector2D> points = new ArrayList<Vector2D>();\n{code}'}"
commons-math,bugs-dot-jar_MATH-1136_cc4ab51e,"{'BugID': 'MATH-1136', 'Summary': 'BinomialDistribution deals with degenerate cases incorrectly', 'Description': 'The following calculation returns false results:\n\n{{new BinomialDistribution(0, 0.01).logProbability(0)}}\n\nIt evaluates to Double.NaN when it should be 0 (cf., for example, ""dbinom(0, 0, 0.01, log=T)"" in R).\n\nI attach a patch dealing with the problem. The patch also adds a test for this bug.'}"
commons-math,bugs-dot-jar_MATH-1141_2f2a2dda,"{'BugID': 'MATH-1141', 'Summary': 'UniformIntegerDistribution should make constructer a exclusive bound or made parameter check more relax', 'Description': 'UniformIntegerDistribution constructer  public UniformIntegerDistribution(RandomGenerator rng,\n                                      int lower,\n                                      int upper) \nthe lower and the upper all inclusive. but the parameter check made a   if (lower >= upper) {\n            throw new NumberIsTooLargeException(\n                            LocalizedFormats.LOWER_BOUND_NOT_BELOW_UPPER_BOUND,\n                            lower, upper, false);\ncheck, i think it is too strict\nto construct UniformIntegerDistribution (0,0) \nthis should make it possible'}"
commons-math,bugs-dot-jar_MATH-1148_4080feff,"{'BugID': 'MATH-1148', 'Summary': 'MonotoneChain handling of collinear points drops low points in a near-column', 'Description': 'This code\n{code}\nval points = List(\n  new Vector2D(\n    16.078200000000184,\n    -36.52519999989808\n  ),\n  new Vector2D(\n    19.164300000000186,\n    -36.52519999989808\n  ),\n  new Vector2D(\n    19.1643,\n    -25.28136477910407\n  ),\n  new Vector2D(\n    19.1643,\n    -17.678400000004157\n  )\n)\nnew hull.MonotoneChain().generate(points.asJava)\n{code}\n\nresults in the exception:\n{code}\norg.apache.commons.math3.exception.ConvergenceException: illegal state: convergence failed\n\tat org.apache.commons.math3.geometry.euclidean.twod.hull.AbstractConvexHullGenerator2D.generate(AbstractConvexHullGenerator2D.java:106)\n\tat org.apache.commons.math3.geometry.euclidean.twod.hull.MonotoneChain.generate(MonotoneChain.java:50)\n\tat .<init>(<console>:13)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:11)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:704)\n\tat scala.tools.nsc.interpreter.IMain$Request$$anonfun$14.apply(IMain.scala:920)\n\tat scala.tools.nsc.interpreter.Line$$anonfun$1.apply$mcV$sp(Line.scala:43)\n\tat scala.tools.nsc.io.package$$anon$2.run(package.scala:25)\n\tat java.lang.Thread.run(Thread.java:662)\n{code}\n\nThis will be tricky to fix. Not only is the point (19.164300000000186, -36.52519999989808) is being dropped incorrectly, but any point dropped in one hull risks creating a kink when combined with the other hull.\n\n'}"
commons-math,bugs-dot-jar_MATH-1165_596ccd59,"{'BugID': 'MATH-1165', 'Summary': 'Rare case for updateMembershipMatrix() in FuzzyKMeansClusterer', 'Description': 'The function updateMembershipMatrix() in FuzzyKMeansClusterer assigns the points to the cluster with the highest membership. Consider the following case:\n\nIf the distance between a point and the cluster center is zero, then we will have a cluster membership of one, and all other membership values will be zero.\n\nSo the if condition:\nif (membershipMatrix[i][j] > maxMembership) {\n                    maxMembership = membershipMatrix[i][j];\n                    newCluster = j;\n}\nwill never be true during the for loop and newCluster will remain -1. This will throw an exception because of the line:\nclusters.get(newCluster)\n                    .addPoint(point);\n\nAdding the following condition can solve the problem:\ndouble d;\nif (sum == 0)\nd = 1;\nelse\nd = 1.0/sum;'}"
commons-math,bugs-dot-jar_MATH-1203_4aa4c6d3,"{'BugID': 'MATH-1203', 'Summary': 'getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution', 'Description': ""After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.\n\nSee: https://issues.apache.org/jira/browse/MATH-984""}"
commons-math,bugs-dot-jar_MATH-1203_b148046a,"{'BugID': 'MATH-1203', 'Summary': 'getKernel fails for buckets with only multiple instances of the same value in random.EmpiricalDistribution', 'Description': ""After loading a set of values into an EmpericalDistribution, assume that there's a case where a single bin ONLY contains multiple instances of the same value.  In this case the standard deviation will equal zero.  This will fail when getKernel attempts to create a NormalDistribution.  The other case where stddev=0 is when there is only a single value in the bin, and this is handled by returning a ConstantRealDistribution rather than a NormalDistrbution.\n\nSee: https://issues.apache.org/jira/browse/MATH-984""}"
commons-math,bugs-dot-jar_MATH-1204_a56d4998,"{'BugID': 'MATH-1204', 'Summary': 'bracket function gives up too early ', 'Description': 'In UnivariateSolverUtils.bracket(...) the search ends prematurely if a = lowerBound, which ignores some roots in the interval. '}"
commons-math,bugs-dot-jar_MATH-1208_ce2badf0,"{'BugID': 'MATH-1208', 'Summary': 'EmpiricalDistribution cumulativeProbability can return NaN when evaluated within a constant bin', 'Description': 'If x belongs to a bin with no variance or to which a ConstantRealDistribution kernel has been assigned, cumulativeProbability(x) can return NaN.'}"
commons-math,bugs-dot-jar_MATH-1211_a06a1584,"{'BugID': 'MATH-1211', 'Summary': 'PolyhedronsSet.firstIntersection(Vector3D point, Line line) sometimes reports intersections on wrong end of line', 'Description': '\nI constructed a PolyhedronsSet from a list of triangular faces representing an icosphere (using the instructions found at https://mail-archives.apache.org/mod_mbox/commons-user/201208.mbox/<5039FE35.2090307@free.fr>).  This seems to produce correct INSIDE/OUTSIDE results for randomly chosen points.  I think my mesh triangles are defined appropriately.\n\nHowever, using PolyhedronsSet.firstIntersection(Vector3D point, Line line) to shoot randomly oriented rays from the origin sometimes gives a wrong mesh intersection point ""behind"" the origin.  The intersection algorithm is sometimes picking up faces of the sphere-shaped mesh on the wrong semi-infinite portion of the line, i.e. meshIntersectionPoint.subtract(point).dotProduct(line.getDirection())<0 where point is the Vector3D at center of the sphere and line extends outward through the mesh.\n\nI think the dot product above should always be positive. If multiple intersections exist along a ""whole"" line then the first one in ""front"" of the line\'s origin should be returned. This makes ray tracing with a PolyhedronsSet possible.\n'}"
commons-math,bugs-dot-jar_MATH-1226_c44bfe00,"{'BugID': 'MATH-1226', 'Summary': 'Exception thrown in ode for a pair of close events', 'Description': 'When two discrete events occur closer to each other than the convergence threshold used for locating them, this sometimes triggers a NumberIsTooLargeException.\n\nThe exception happens because the EventState class think the second event is simply a numerical artifact (a repetition of the already triggerred first event) and tries to skip past it. If there are no other event in the same step later on, one interval boundary finally reach step end and the interval bounds are reversed.'}"
commons-math,bugs-dot-jar_MATH-1230_96eb80ef,"{'BugID': 'MATH-1230', 'Summary': 'SimplexSolver returning wrong answer from optimize', 'Description': 'SimplexSolver fails for the following linear program:\n\nmin 2x1 +15x2 +18x3\n\nSubject to\n\n  -x1 +2x2  -6x3 <=-10\n            x2  +2x3 <= 6\n   2x1      +10x3 <= 19\n    -x1  +x2       <= -2\n    x1,x2,x3 >= 0\n\nSolution should be\nx1 = 7\nx2 = 0\nx3 = 1/2\nObjective function = 23\n\nInstead, it is returning\nx1 = 9.5\nx2 = 1/8\nx3 = 0\nObjective function = 20.875\n\nConstraint number 1 is violated by this answer'}"
commons-math,bugs-dot-jar_MATH-1232_8f35fcb8,"{'BugID': 'MATH-1232', 'Summary': 'UknownParameterException message prints {0} instead of parameter name', 'Description': 'The constructor for UnknownParameterException stores the\nparameter name internally but does not forward it to the base class which creates the error message.'}"
commons-math,bugs-dot-jar_MATH-1241_471e6b07,"{'BugID': 'MATH-1241', 'Summary': 'Digamma calculation produces SOE on NaN argument', 'Description': ""Digamma doesn't work particularly well with NaNs.\n\nHow to reproduce: call Gamma.digamma(Double.NaN)\n\nExpected outcome: returns NaN or throws a meaningful exception\n\nReal outcome: crashes with StackOverflowException, as digamma enters infinite recursion.""}"
commons-math,bugs-dot-jar_MATH-1252_09fe956a,"{'BugID': 'MATH-1252', 'Summary': 'ResizableDoubleArray does not work with double array of size 1', 'Description': 'When attempting to create a ResizableDoubleArray with an array of a single value (e.g. {4.0}), the constructor creates an internal array with 16 entries that are all 0.0\n\nBug looks like it might be on line 414 of ResizableDoubleArray.java:\n\n        if (data != null && data.length > 1) {\n'}"
commons-math,bugs-dot-jar_MATH-1256_41f29780,"{'BugID': 'MATH-1256', 'Summary': 'Interval class upper and lower check', 'Description': 'In class Interval, which is in the package org.apache.commons.math4.geometry.euclidean.oned it is possible to pass the value for variable upper  less than the value of variable lower, which is logically incorrect and  also causes the method getSize() to return negative value.\n\nFor example:\n\n @Test\n  public void test1()  throws Throwable  {\n      Interval interval0 = new Interval(0.0, (-1.0));\n      double double0 = interval0.getSize();\n      assertEquals((-1.0), double0, 0.01D);\n  }\n\n'}"
commons-math,bugs-dot-jar_MATH-1257_03178c8b,"{'BugID': 'MATH-1257', 'Summary': 'NormalDistribution.cumulativeProbability() suffers from cancellation', 'Description': 'I see the following around line 194:\n{noformat}\n        return 0.5 * (1 + Erf.erf(dev / (standardDeviation * SQRT2)));\n{noformat}\n\nWhen erf() returns a very small value, this cancels in the addition with the ""1.0"" which leads to poor precision in the results.\n\nI would suggest changing this line to read more like:\n{noformat}\nreturn 0.5 * Erf.erfc( -dev / standardDeviation * SQRT2 );\n{noformat} \n\nShould you want some test cases for ""extreme values"" (one might argue that within 10 standard deviations isn\'t all that extreme) then you can check the following: http://www.jstatsoft.org/v52/i07/ then look in the v52i07-xls.zip at replication-01-distribution-standard-normal.xls\n\nI think you will also find that evaluation of expressions such as {noformat}NormalDistribution( 0, 1 ).cumulativeProbability( -10.0 );{noformat}\nare pretty far off.'}"
commons-math,bugs-dot-jar_MATH-1261_4c4b3e2e,"{'BugID': 'MATH-1261', 'Summary': 'Overflow checks in Fraction multiply(int) / divide(int)', 'Description': 'The member methods multiply(int) / divide(int) in the class org.apache.commons.math3.fraction.Fraction do not have overflow checks.\n\n{code:java}\nreturn new Fraction(numerator * i, denominator);\n{code}\n\nshould be\n\n{code:java}\nreturn new Fraction(ArithmeticUtils.mulAndCheck(numerator, i), denominator);\n{code}\n\nor, considering the case gcd(i, denominator) > 1,\n\n{code:java}\nreturn multiply(new Fraction(i));\n{code}'}"
commons-math,bugs-dot-jar_MATH-1269_a94ff90a,"{'BugID': 'MATH-1269', 'Summary': 'FastMath.exp may return NaN for non-NaN arguments', 'Description': 'I have observed that FastMath.exp(709.8125) returns NaN. However, the exponential function must never return NaN (if the argument is not NaN). The result must always be non-negative or positive infinity.'}"
commons-math,bugs-dot-jar_MATH-1272_26e878ab,"{'BugID': 'MATH-1272', 'Summary': 'FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE', 'Description': 'FastMath.pow(double, long) enters an infinite loop with Long.MIN_VALUE. It cannot be negated, so unsigned shift (>>>) is required instead of a signed one (>>).'}"
commons-math,bugs-dot-jar_MATH-1277_fb007815,"{'BugID': 'MATH-1277', 'Summary': 'Incorrect Kendall Tau calc due to data type mistmatch', 'Description': ""The Kendall Tau calculation returns a number from -1.0 to 1.0\n\ndue to a mixing of ints and longs, a mistake occurs on large size columns (arrays) passed to the function. an array size of > 50350 triggers the condition in my case - although it may be data dependent\n\nthe ver 3.5 library returns 2.6 as a result (outside of the defined range of Kendall Tau)\n\nwith the cast to long below - the result reutns to its expected value\n\n\ncommons.math3.stat.correlation.KendallsCorrelation.correlation\n\n\nhere's the sample code I used:\nI added the cast to long of swaps in the \n\n\t\t\tint swaps = 1077126315;\n\t\t\t final long numPairs = sum(50350 - 1);\n\t\t\t    long tiedXPairs = 0;\n\t\t        long tiedXYPairs = 0;\n\t\t        long tiedYPairs = 0;\n\t\t        \n\t\t  final long concordantMinusDiscordant = numPairs - tiedXPairs - tiedYPairs + tiedXYPairs - 2 * (long) swaps;\n\t        final double nonTiedPairsMultiplied = 1.6e18;\n\t        double myTest = concordantMinusDiscordant / FastMath.sqrt(nonTiedPairsMultiplied);\n""}"
commons-math,bugs-dot-jar_MATH-1283_9e0c5ad4,"{'BugID': 'MATH-1283', 'Summary': 'Gamma function computation', 'Description': 'In the gamma method, when handling the case ""absX > 20"", the computation of gammaAbs should replace ""x"" (see code below with x in bold) by ""absX"".\nFor large negative values of x, the function returns with the wrong sign.\n\nfinal double gammaAbs = SQRT_TWO_PI / *x* *\n                                     FastMath.pow(y, absX + 0.5) *\n                                     FastMath.exp(-y) * lanczos(absX);'}"
commons-math,bugs-dot-jar_MATH-1297_56434517,"{'BugID': 'MATH-1297', 'Summary': 'multistep integrator start failure triggers NPE', 'Description': 'Multistep ODE integrators like Adams-Bashforth and Adams-Moulton require a starter procedure.\nIf the starter integrator is not configured properly, it will not create the necessary number of initial points and the multistep integrator will not be initialized correctly. This results in NullPointErException when the scaling array is referenced later on.\n\nThe following test case (with an intentionally wrong starter configuration) shows the problem.\n\n{code}\n@Test\npublic void testStartFailure() {\n\n     TestProblem1 pb = new TestProblem1();\n      double minStep = 0.0001 * (pb.getFinalTime() - pb.getInitialTime());\n      double maxStep = pb.getFinalTime() - pb.getInitialTime();\n      double scalAbsoluteTolerance = 1.0e-6;\n      double scalRelativeTolerance = 1.0e-7;\n\n      MultistepIntegrator integ =\n          new AdamsBashforthIntegrator(4, minStep, maxStep,\n                                                            scalAbsoluteTolerance,\n                                                            scalRelativeTolerance);\n      integ.setStarterIntegrator(new DormandPrince853Integrator(0.2 * (pb.getFinalTime() - pb.getInitialTime()),\n                                                                pb.getFinalTime() - pb.getInitialTime(),\n                                                                0.1, 0.1));\n      TestProblemHandler handler = new TestProblemHandler(pb, integ);\n      integ.addStepHandler(handler);\n      integ.integrate(pb,\n                             pb.getInitialTime(), pb.getInitialState(),\n                             pb.getFinalTime(), new double[pb.getDimension()]);\n\n    }\n{code}\n\nFailure to start the integrator should be detected and an appropriate exception should be triggered.'}"
commons-math,bugs-dot-jar_MATH-1300_1d635088,"{'BugID': 'MATH-1300', 'Summary': 'BitsStreamGenerator#nextBytes(byte[]) is wrong', 'Description': 'Sequential calls to the BitsStreamGenerator#nextBytes(byte[]) must generate the same sequence of bytes, no matter by chunks of which size it was divided. This is also how java.util.Random#nextBytes(byte[]) works.\n\nWhen nextBytes(byte[]) is called with a bytes array of length multiple of 4 it makes one unneeded call to next(int) method. This is wrong and produces an inconsistent behavior of classes like MersenneTwister.\n\nI made a new implementation of the BitsStreamGenerator#nextBytes(byte[]) see attached code.'}"
commons-math,bugs-dot-jar_MATH-286_dbdff075,"{'BugID': 'MATH-286', 'Summary': 'SimplexSolver not working as expected?', 'Description': 'I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints...\n\nConsider this LP:\n\nmax: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5;\nr1: x0 + x2 + x4 = 23.0;\nr2: x1 + x3 + x5 = 23.0;\nr3: x0 >= 10.0;\nr4: x2 >= 8.0;\nr5: x4 >= 5.0;\n\nLPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0;\n\nThe same LP expressed in Apache commons math is:\n\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 }, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 23.0));\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 0, 0, 0, 0 }, Relationship.GEQ, 10.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 0, 1, 0, 0, 0 }, Relationship.GEQ, 8.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 0, 0, 0, 1, 0 }, Relationship.GEQ, 5.0));\n\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);\n\nthat returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0;\n\nIs it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied...\n\nAm I using the interface wrongly?'}"
commons-math,bugs-dot-jar_MATH-288_38983e82,"{'BugID': 'MATH-288', 'Summary': 'SimplexSolver not working as expected 2', 'Description': ""SimplexSolver didn't find the optimal solution.\n\nProgram for Lpsolve:\n=====================\n/* Objective function */\nmax: 7 a 3 b;\n\n/* Constraints */\nR1: +3 a -5 c <= 0;\nR2: +2 a -5 d <= 0;\nR3: +2 b -5 c <= 0;\nR4: +3 b -5 d <= 0;\nR5: +3 a +2 b <= 5;\nR6: +2 a +3 b <= 5;\n\n/* Variable bounds */\na <= 1;\nb <= 1;\n=====================\nResults(correct): a = 1, b = 1, value = 10\n\n\nProgram for SimplexSolve:\n=====================\nLinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[]{7, 3, 0, 0}, 0);\nCollection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>();\npodmienky.add(new LinearConstraint(new double[]{1, 0, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{0, 1, 0, 0}, Relationship.LEQ, 1));\npodmienky.add(new LinearConstraint(new double[]{3, 0, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{2, 0, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 2, -5, 0}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{0, 3, 0, -5}, Relationship.LEQ, 0));\npodmienky.add(new LinearConstraint(new double[]{3, 2, 0, 0}, Relationship.LEQ, 5));\npodmienky.add(new LinearConstraint(new double[]{2, 3, 0, 0}, Relationship.LEQ, 5));\nSimplexSolver solver = new SimplexSolver();\nRealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true);\n=====================\nResults(incorrect): a = 1, b = 0.5, value = 8.5\n\nP.S. I used the latest software from the repository (including MATH-286 fix).""}"
commons-math,bugs-dot-jar_MATH-290_b01fcc31,"{'BugID': 'MATH-290', 'Summary': 'NullPointerException in SimplexTableau.initialize', 'Description': 'SimplexTableau throws a NullPointerException when no solution can be found instead of a NoFeasibleSolutionException\n\nHere is the code that causes the NullPointerException:\n\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 1, 5 }, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] { 2, 0 }, Relationship.GEQ, -1.0));\n\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MINIMIZE, true);\n\nNote: Tested both with Apache Commons Math 2.0 release and SVN trunk'}"
commons-math,bugs-dot-jar_MATH-293_59a0da9c,"{'BugID': 'MATH-293', 'Summary': 'Matrix\'s ""OutOfBoundException"" in SimplexSolver', 'Description': 'Hi all,\nThis bug is somehow related to incident MATH-286, but not necessarily...\n\nLet\'s say I have an LP and I solve it using SimplexSolver. Then I create a second LP similar to the first one, but with ""stronger"" constraints. The second LP has the following properties:\n* the only point in the feasible region for the second LP is the solution returned for the first LP\n* the solution returned for the first LP is also the (only possible) solution to the second LP\n\nThis shows the problem:\n\n{code:borderStyle=solid}\nLinearObjectiveFunction f = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );\nCollection<LinearConstraint> constraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));\nconstraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, 10.0));\nconstraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, 10.0));\nconstraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, 10.0));\n\nRealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);\n\ndouble valA = 0.8 * solution.getPoint()[0] + 0.2 * solution.getPoint()[1];\ndouble valB = 0.7 * solution.getPoint()[2] + 0.3 * solution.getPoint()[3];\ndouble valC = 0.4 * solution.getPoint()[4] + 0.6 * solution.getPoint()[5];\n\nf = new LinearObjectiveFunction(new double[] { 0.8, 0.2, 0.7, 0.3, 0.4, 0.6}, 0 );\nconstraints = new ArrayList<LinearConstraint>();\nconstraints.add(new LinearConstraint(new double[] { 1, 0, 1, 0, 1, 0 }, Relationship.EQ, 30.0));\nconstraints.add(new LinearConstraint(new double[] { 0, 1, 0, 1, 0, 1 }, Relationship.EQ, 30.0));\nconstraints.add(new LinearConstraint(new double[] { 0.8, 0.2, 0.0, 0.0, 0.0, 0.0 }, Relationship.GEQ, valA));\nconstraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.7, 0.3, 0.0, 0.0 }, Relationship.GEQ, valB));\nconstraints.add(new LinearConstraint(new double[] { 0.0, 0.0, 0.0, 0.0, 0.4, 0.6 }, Relationship.GEQ, valC));\n\nsolution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true);\n{code} \n\nInstead of returning the solution, SimplexSolver throws an Exception:\n\n{noformat} Exception in thread ""main"" org.apache.commons.math.linear.MatrixIndexException: no entry at indices (0, 7) in a 6x7 matrix\n\tat org.apache.commons.math.linear.Array2DRowRealMatrix.getEntry(Array2DRowRealMatrix.java:356)\n\tat org.apache.commons.math.optimization.linear.SimplexTableau.getEntry(SimplexTableau.java:408)\n\tat org.apache.commons.math.optimization.linear.SimplexTableau.getBasicRow(SimplexTableau.java:258)\n\tat org.apache.commons.math.optimization.linear.SimplexTableau.getSolution(SimplexTableau.java:336)\n\tat org.apache.commons.math.optimization.linear.SimplexSolver.doOptimize(SimplexSolver.java:182)\n\tat org.apache.commons.math.optimization.linear.AbstractLinearOptimizer.optimize(AbstractLinearOptimizer.java:106){noformat} \n\nI was too optimistic with the bug MATH-286 ;-)'}"
commons-math,bugs-dot-jar_MATH-294_2c8a114f,"{'BugID': 'MATH-294', 'Summary': 'RandomDataImpl.nextPoisson fails for means in range 6.0 - 19.99', 'Description': ""math.random.RandomDataImpl.nextPoisson(double mean) fails frequently (but not always) for values of mean between 6.0 and 19.99 inclusive. For values below 6.0 (where I see there is a branch in the logic) and above 20.0 it seems to be okay (though I've only randomly sampled the space and run a million trials for the values I've tried)\n\nWhen it fails, the exception is as follows (this for a mean of 6.0)\n\norg.apache.commons.math.MathRuntimeException$4: must have n >= 0 for n!, got n = -2\n\tat org.apache.commons.math.MathRuntimeException.createIllegalArgumentException(MathRuntimeException.java:282)\n\tat org.apache.commons.math.util.MathUtils.factorialLog(MathUtils.java:561)\n\tat org.apache.commons.math.random.RandomDataImpl.nextPoisson(RandomDataImpl.java:434) \n\nie MathUtils.factorialLog is being called with a negative input\n\nTo reproduce:\n\n    JDKRandomGenerator random = new JDKRandomGenerator();\n    random.setSeed(123456);\n    RandomData randomData = new RandomDataImpl(random);\n\n    for (int i=0; i< 1000000; i++){\n        randomData.nextPoisson(6.0);\n    }\n""}"
commons-math,bugs-dot-jar_MATH-305_ef9b639a,"{'BugID': 'MATH-305', 'Summary': 'NPE in  KMeansPlusPlusClusterer unittest', 'Description': 'When running this unittest, I am facing this NPE:\njava.lang.NullPointerException\n\tat org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91)\n\nThis is the unittest:\n\n\npackage org.fao.fisheries.chronicles.calcuation.cluster;\n\nimport static org.junit.Assert.assertEquals;\nimport static org.junit.Assert.assertTrue;\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Random;\n\nimport org.apache.commons.math.stat.clustering.Cluster;\nimport org.apache.commons.math.stat.clustering.EuclideanIntegerPoint;\nimport org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer;\nimport org.fao.fisheries.chronicles.input.CsvImportProcess;\nimport org.fao.fisheries.chronicles.input.Top200Csv;\nimport org.junit.Test;\n\npublic class ClusterAnalysisTest {\n\n\n\t@Test\n\tpublic void testPerformClusterAnalysis2() {\n\t\tKMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(\n\t\t\t\tnew Random(1746432956321l));\n\t\tEuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1959, 325100 }),\n\t\t\t\tnew EuclideanIntegerPoint(new int[] { 1960, 373200 }), };\n\t\tList<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);\n\t\tassertEquals(1, clusters.size());\n\n\t}\n\n}\n'}"
commons-math,bugs-dot-jar_MATH-309_0596e314,"{'BugID': 'MATH-309', 'Summary': 'nextExponential parameter check bug - patch supplied', 'Description': 'Index: src/main/java/org/apache/commons/math/random/RandomDataImpl.java\n===================================================================\n--- src/main/java/org/apache/commons/math/random/RandomDataImpl.java\t(revision 830102)\n+++ src/main/java/org/apache/commons/math/random/RandomDataImpl.java\t(working copy)\n@@ -462,7 +462,7 @@\n      * @return the random Exponential value\n      */\n     public double nextExponential(double mean) {\n-        if (mean < 0.0) {\n+        if (mean <= 0.0) {\n             throw MathRuntimeException.createIllegalArgumentException(\n                   ""mean must be positive ({0})"", mean);\n         }'}"
commons-math,bugs-dot-jar_MATH-318_83f18d52,"{'BugID': 'MATH-318', 'Summary': 'wrong result in eigen decomposition', 'Description': 'Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0\n{code}\n    public void testMathpbx02() {\n\n        double[] mainTridiagonal = {\n        \t  7484.860960227216, 18405.28129035345, 13855.225609560746,\n        \t 10016.708722343366, 559.8117399576674, 6750.190788301587, \n        \t    71.21428769782159\n        };\n        double[] secondaryTridiagonal = {\n        \t -4175.088570476366,1975.7955858241994,5193.178422374075, \n        \t  1995.286659169179,75.34535882933804,-234.0808002076056\n        };\n\n        // the reference values have been computed using routine DSTEMR\n        // from the fortran library LAPACK version 3.2.1\n        double[] refEigenValues = {\n        \t\t20654.744890306974412,16828.208208485466457,\n        \t\t6893.155912634994820,6757.083016675340332,\n        \t\t5887.799885688558788,64.309089923240379,\n        \t\t57.992628792736340\n        };\n        RealVector[] refEigenVectors = {\n        \t\tnew ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),\n        \t\tnew ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),\n        \t\tnew ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),\n        \t\tnew ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),\n        \t\tnew ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),\n        \t\tnew ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),\n        \t\tnew ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})\n        };\n\n        // the following line triggers the exception\n        EigenDecomposition decomposition =\n            new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);\n\n        double[] eigenValues = decomposition.getRealEigenvalues();\n        for (int i = 0; i < refEigenValues.length; ++i) {\n            assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);\n            if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {\n                assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            } else {\n                assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);\n            }\n        }\n\n    }\n{code}'}"
commons-math,bugs-dot-jar_MATH-320_b2f3f6db,"{'BugID': 'MATH-320', 'Summary': 'NaN singular value from SVD', 'Description': ""The following jython code\nStart code\n\nfrom org.apache.commons.math.linear import *\n \nAlist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]\n \nA = Array2DRowRealMatrix(Alist)\n \ndecomp = SingularValueDecompositionImpl(A)\n \nprint decomp.getSingularValues()\n\nEnd code\n\nprints\narray('d', [11.218599757513008, 0.3781791648535976, nan])\nThe last singular value should be something very close to 0 since the matrix\nis rank deficient.  When i use the result from getSolver() to solve a system, i end \nup with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.\n\nDoes this SVD implementation require that the matrix be full rank?  If so, then i would expect\nan exception to be thrown from the constructor or one of the methods.\n\n\n""}"
commons-math,bugs-dot-jar_MATH-320_c06cc933,"{'BugID': 'MATH-320', 'Summary': 'NaN singular value from SVD', 'Description': ""The following jython code\nStart code\n\nfrom org.apache.commons.math.linear import *\n \nAlist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]]\n \nA = Array2DRowRealMatrix(Alist)\n \ndecomp = SingularValueDecompositionImpl(A)\n \nprint decomp.getSingularValues()\n\nEnd code\n\nprints\narray('d', [11.218599757513008, 0.3781791648535976, nan])\nThe last singular value should be something very close to 0 since the matrix\nis rank deficient.  When i use the result from getSolver() to solve a system, i end \nup with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution.\n\nDoes this SVD implementation require that the matrix be full rank?  If so, then i would expect\nan exception to be thrown from the constructor or one of the methods.\n\n\n""}"
commons-math,bugs-dot-jar_MATH-326_ce185345,"{'BugID': 'MATH-326', 'Summary': 'getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)', 'Description': 'the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries.\n\nThe current implementation in ArrayRealVector has a typo:\n\n{code}\n    public double getLInfNorm() {\n        double max = 0;\n        for (double a : data) {\n            max += Math.max(max, Math.abs(a));\n        }\n        return max;\n    }\n{code}\n\nthe += should just be an =.\n\nThere is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness).\n\nWorse, the implementation in OpenMapRealVector is not even positive semi-definite:\n\n{code}   \n    public double getLInfNorm() {\n        double max = 0;\n        Iterator iter = entries.iterator();\n        while (iter.hasNext()) {\n            iter.advance();\n            max += iter.value();\n        }\n        return max;\n    }\n{code}\n\nI would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():\n\n{code}\n  public double getLInfNorm() {\n    double norm = 0;\n    Iterator<Entry> it = sparseIterator();\n    Entry e;\n    while(it.hasNext() && (e = it.next()) != null) {\n      norm = Math.max(norm, Math.abs(e.getValue()));\n    }\n    return norm;\n  }\n{code}\n\nUnit tests with negative valued vectors would be helpful to check for this kind of thing in the future.'}"
commons-math,bugs-dot-jar_MATH-327_262fe4c0,"{'BugID': 'MATH-327', 'Summary': ' Maximal number of iterations (540) exceeded', 'Description': ""I have a matrix of size 49x19 and when I apply SVD on this matrix it raises the following exception. The problem which I am facing is that SVD works for some matrix and doesn't work for others. I have no clue what is the possible reason.\n\nException::\nCorrespondenceAnalysis: org.apache.commons.math.MaxIterationsExceededException: Maximal number of iterations (540) exceeded \n[org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:881), org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:651), org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:243), org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:202), org.apache.commons.math.linear.SingularValueDecompositionImpl.<init>(SingularValueDecompositionImpl.java:114),\n\n\nRealMatrix m = [[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.99107143, 1.00000000, 1.00000000, 1.00000000, 1.00000000, 0.94450431, 1.00000000, 1.00000000, 0.99107143, 0.95238096, 1.00000000, 1.00000000, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573],[1.00000000, 1.00000000, 0.94999999, 0.95833331, 1.00000000, 1.00000000, 0.99107143, 0.94583333, 1.00000000, 0.95000000, 0.98333333, 0.92106681, 0.97368419, 1.00000000, 0.95357142, 0.95238096, 1.00000000, 0.93333334, 0.96428573]]\n\nRealMatrix rcp = MatrixUtils.createRealMatrix(CP);\t\nSingularValueDecomposition svd = new SingularValueDecompositionImpl(rcp);\t\t\n\nRealMatrix U = svd.getU();\nRealMatrix S = svd.getS();\nRealMatrix Vt = svd.getVT();\ndouble[] singularValues = svd.getSingularValues();""}"
commons-math,bugs-dot-jar_MATH-329_6dd3724b,"{'BugID': 'MATH-329', 'Summary': 'In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable) ', 'Description': 'Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change\n\nFrequency.java\n\n   /**\n      * Returns the percentage of values that are equal to v\n     * @deprecated replaced by {@link #getPct(Comparable)} as of 2.0\n     */\n    @Deprecated\n    public double getPct(Object v) {\n        return getCumPct((Comparable<?>) v);\n    }'}"
commons-math,bugs-dot-jar_MATH-338_8dd22390,"{'BugID': 'MATH-338', 'Summary': 'Wrong parameter for first step size guess for Embedded Runge Kutta methods', 'Description': 'In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator.\n\nHere, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...)\n\nThe problem comes from the array ""scale"" that is used as a parameter in the call off initializeStep(..)\n\nFollowing the theory described by Hairer in his book ""Solving Ordinary Differential Equations 1 : Nonstiff Problems"", the scaling should be :\n\nsci = Atol i + |y0i| * Rtoli\n\nWhereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli\n\nNote that the Gragg-Bulirsch-Stoer integrator uses the good implementation ""sci = Atol i + |y0i| * Rtoli  "" when he performs the call to the same method initializeStep(..)\n\nIn the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user.\nBut in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...)\n\n\nTo fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator\nFor exemple :\n\n final double[] scale= new double[y0.length];;\n          \n          if (vecAbsoluteTolerance == null) {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;\n              }\n            } else {\n              for (int i = 0; i < scale.length; ++i) {\n                final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));\n                scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;\n              }\n            }\n          \n          hNew = initializeStep(equations, forward, getOrder(), scale,\n                           stepStart, y, yDotK[0], yTmp, yDotK[1]);\n\n\n\nSorry for the length of this message, looking forward to hearing from you soon\n\nVincent Morand\n\n\n\n\n'}"
commons-math,bugs-dot-jar_MATH-343_f6dd42b4,"{'BugID': 'MATH-343', 'Summary': ""Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign"", 'Description': 'Javadoc for ""public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)"" claims that ""if the values of the function at the three points have the same sign"" an IllegalArgumentException is thrown. This case isn\'t even checked.'}"
commons-math,bugs-dot-jar_MATH-344_a0b4b4b7,"{'BugID': 'MATH-344', 'Summary': 'Brent solver returns the wrong value if either bracket endpoint is root', 'Description': 'The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.'}"
commons-math,bugs-dot-jar_MATH-349_4cc9a49d,"{'BugID': 'MATH-349', 'Summary': 'Dangerous code in ""PoissonDistributionImpl""', 'Description': 'In the following excerpt from class ""PoissonDistributionImpl"":\n\n{code:title=PoissonDistributionImpl.java|borderStyle=solid}\n    public PoissonDistributionImpl(double p, NormalDistribution z) {\n        super();\n        setNormal(z);\n        setMean(p);\n    }\n{code}\n\n(1) Overridable methods are called within the constructor.\n(2) The reference ""z"" is stored and modified within the class.\n\nI\'ve encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the ""setter"" methods inside the constructor but I think that a more elegant solution would be to remove the ""setters"" altogether (i.e. make the classes immutable).\nProblem (2) can also create unexpected behaviour. Is it really necessary to pass the ""NormalDistribution"" object; can\'t it be always created within the class?\n'}"
commons-math,bugs-dot-jar_MATH-358_061f5017,"{'BugID': 'MATH-358', 'Summary': 'ODE integrator goes past specified end of integration range', 'Description': 'End of integration range in ODE solving is handled as an event.\nIn some cases, numerical accuracy in events detection leads to error in events location.\nThe following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.\n{code}\n  public void testMissedEvent() throws IntegratorException, DerivativeException {\n          final double t0 = 1878250320.0000029;\n          final double t =  1878250379.9999986;\n          FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {\n            \n            public int getDimension() {\n                return 1;\n            }\n            \n            public void computeDerivatives(double t, double[] y, double[] yDot)\n                throws DerivativeException {\n                yDot[0] = y[0] * 1.0e-6;\n            }\n        };\n\n        DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,\n                                                                               1.0e-10, 1.0e-10);\n\n        double[] y = { 1.0 };\n        integrator.setInitialStepSize(60.0);\n        double finalT = integrator.integrate(ode, t0, y, t, y);\n        Assert.assertEquals(t, finalT, 1.0e-6);\n    }\n\n{code}'}"
commons-math,bugs-dot-jar_MATH-367_3a15d8ce,"{'BugID': 'MATH-367', 'Summary': 'AbstractRealVector.sparseIterator fails when vector has exactly one non-zero entry', 'Description': 'The following program:\n===\nimport java.util.Iterator;\nimport org.apache.commons.math.linear.*;\n\npublic class SparseIteratorTester\n{\n    public static void main(String[] args) {\n        double vdata[] = { 0.0, 1.0, 0.0 };\n        RealVector v = new ArrayRealVector(vdata);\n        Iterator<RealVector.Entry> iter = v.sparseIterator();\n        while(iter.hasNext()) {\n            RealVector.Entry entry = iter.next();\n            System.out.printf(""%d: %f\\n"", entry.getIndex(), entry.getValue());\n        }   \n    }       \n} \n===\ngenerates this output:\n\n1: 1.000000\nException in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1\n\tat org.apache.commons.math.linear.ArrayRealVector.getEntry(ArrayRealVector.java:995)\n\tat org.apache.commons.math.linear.AbstractRealVector$EntryImpl.getValue(AbstractRealVector.java:850)\n\tat test.SparseIteratorTester.main(SparseIteratorTester.java:13)\n===\n\nThis patch fixes it, and simplifies AbstractRealVector.SparseEntryIterator  (sorry, i don\'t see any form entry for attaching a file)\n===\nIndex: src/main/java/org/apache/commons/math/linear/AbstractRealVector.java\n===================================================================\n--- src/main/java/org/apache/commons/math/linear/AbstractRealVector.java\t(revision 936985)\n+++ src/main/java/org/apache/commons/math/linear/AbstractRealVector.java\t(working copy)\n@@ -18,6 +18,7 @@\n package org.apache.commons.math.linear;\n \n import java.util.Iterator;\n+import java.util.NoSuchElementException;\n \n import org.apache.commons.math.FunctionEvaluationException;\n import org.apache.commons.math.MathRuntimeException;\n@@ -875,40 +876,25 @@\n         /** Dimension of the vector. */\n         private final int dim;\n \n-        /** Temporary entry (reused on each call to {@link #next()}. */\n-        private EntryImpl tmp = new EntryImpl();\n-\n-        /** Current entry. */\n+        /** Last entry returned by #next(). */\n         private EntryImpl current;\n \n-        /** Next entry. */\n+        /** Next entry for #next() to return. */\n         private EntryImpl next;\n \n         /** Simple constructor. */\n         protected SparseEntryIterator() {\n             dim = getDimension();\n             current = new EntryImpl();\n-            if (current.getValue() == 0) {\n-                advance(current);\n-            }\n-            if(current.getIndex() >= 0){\n-                // There is at least one non-zero entry\n-                next = new EntryImpl();\n-                next.setIndex(current.getIndex());\n+            next = new EntryImpl();\n+            if(next.getValue() == 0)\n                 advance(next);\n-            } else {\n-                // The vector consists of only zero entries, so deny having a next\n-                current = null;\n-            }\n         }\n \n-        /** Advance an entry up to the next non null one.\n+        /** Advance an entry up to the next nonzero value.\n          * @param e entry to advance\n          */\n         protected void advance(EntryImpl e) {\n-            if (e == null) {\n-                return;\n-            }\n             do {\n                 e.setIndex(e.getIndex() + 1);\n             } while (e.getIndex() < dim && e.getValue() == 0);\n@@ -919,22 +905,17 @@\n \n         /** {@inheritDoc} */\n         public boolean hasNext() {\n-            return current != null;\n+            return next.getIndex() >= 0;\n         }\n \n         /** {@inheritDoc} */\n         public Entry next() {\n-            tmp.setIndex(current.getIndex());\n-            if (next != null) {\n-                current.setIndex(next.getIndex());\n-                advance(next);\n-                if (next.getIndex() < 0) {\n-                    next = null;\n-                }\n-            } else {\n-                current = null;\n-            }\n-            return tmp;\n+            int index = next.getIndex();\n+            if(index < 0)\n+                throw new NoSuchElementException();\n+            current.setIndex(index);\n+            advance(next);\n+            return current;\n         }\n \n         /** {@inheritDoc} */\n'}"
commons-math,bugs-dot-jar_MATH-369_f4a4464b,"{'BugID': 'MATH-369', 'Summary': 'BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException', 'Description': 'Method \n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)  \n\ninvokes \n\n    BisectionSolver.solve(double min, double max) \n\nwhich throws NullPointerException, as member variable\n\n    UnivariateRealSolverImpl.f \n\nis null.\n\nInstead the method:\n\n    BisectionSolver.solve(final UnivariateRealFunction f, double min, double max)\n\nshould be called.\n\nSteps to reproduce:\n\ninvoke:\n\n     new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5);\n\nNullPointerException will be thrown.\n\n\n'}"
commons-math,bugs-dot-jar_MATH-370_495f04bc,"{'BugID': 'MATH-370', 'Summary': 'NaN in ""equals"" methods', 'Description': 'In ""MathUtils"", some ""equals"" methods will return true if both argument are NaN.\nUnless I\'m mistaken, this contradicts the IEEE standard.\n\nIf nobody objects, I\'m going to make the changes.\n'}"
commons-math,bugs-dot-jar_MATH-371_bb005b56,"{'BugID': 'MATH-371', 'Summary': 'PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon', 'Description': 'Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that.\n\nIn MATH-201, the problem was described as such:\n> So in essence, the p-value returned by TTestImpl.tTest() is:\n> \n> 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t))\n> \n> For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When \n> cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because:\n> \n> 1.0 - 1.0 + 0.0 = 0.0\n\nThe solution in MATH-201 was to modify the p-value calculation to this:\n> p = 2.0 * cumulativeProbability(-t)\n\nHere, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():\n  p = 2 * (1 - tDistribution.cumulativeProbability(t));\n\nDirectly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:\n  p = 2 * (tDistribution.cumulativeProbability(-t));\n\n\n\n\n'}"
commons-math,bugs-dot-jar_MATH-373_bfe4623c,"{'BugID': 'MATH-373', 'Summary': 'StatUtils.sum returns NaN for zero-length arrays', 'Description': 'StatUtils.sum returns NaN for zero-length arrays, which is:\n\n1. inconsistent with the mathematical notion of sum: in maths, sum_{i=0}^{N-1} a_i will be 0 for N=0. In particular, the identity\n\nsum_{i=0}^{k-1} a_i + sum_{i=k}^{N-1} = sum_{i=0}^{N-1}\n\nis broken for k = 0, since NaN + x = NaN, not x.\n\n2. introduces hard to debug erros (returning a NaN is one of the worst forms of reporting an exceptional condition, as NaNs propagate silently and require manual tracing during the debugging)\n\n3. enforces ""special case"" handling when the user expects that the summed array can have a zero length.\n\nThe correct behaviour is, in my opinion, to return 0.0, not NaN in the above case.'}"
commons-math,bugs-dot-jar_MATH-377_c640932d,"{'BugID': 'MATH-377', 'Summary': 'weight versus sigma in AbstractLeastSquares', 'Description': 'In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.\n\n Once corrected, getRMS() can even reduce\n\n public double getRMS() {return Math.sqrt(getChiSquare()/rows);}'}"
commons-math,bugs-dot-jar_MATH-393_d4b02f6a,"{'BugID': 'MATH-393', 'Summary': 'Method ""getResult()"" in ""MultiStartUnivariateRealOptimizer""', 'Description': 'In ""MultiStartUnivariateRealOptimizer"" (package ""optimization""), the method ""getResult"" returns the result of the last run of the ""underlying"" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the ""optimize"" method. This is confusing and does not seem very useful. I think that ""getResult"" should be defined as\n{code} \npublic double getResult() {\n    return optima[0];\n}\n{code}\nand similarly\n{code}\npublic double getFunctionValue() {\n    return optimaValues[0];\n}\n{code}\n'}"
commons-math,bugs-dot-jar_MATH-395_962315ba,"{'BugID': 'MATH-395', 'Summary': 'Bugs in ""BrentOptimizer""', 'Description': 'I apologize for having provided a buggy implementation of Brent\'s optimization algorithm (class ""BrentOptimizer"" in package ""optimization.univariate"").\nThe unit tests didn\'t show that there was something wrong, although (from the ""changes.xml"" file) I discovered that, at the time, Luc had noticed something weird in the implementation\'s behaviour.\nComparing with an implementation in Python, I could figure out the fixes. I\'ll modify ""BrentOptimizer"" and add a test. I also propose to change the name of the unit test class from ""BrentMinimizerTest"" to ""BrentOptimizerTest"".\n'}"
commons-math,bugs-dot-jar_MATH-405_784e4f69,"{'BugID': 'MATH-405', 'Summary': 'Inconsistent result from Levenberg-Marquardt', 'Description': 'Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost'}"
commons-math,bugs-dot-jar_MATH-413_51aa6e6c,"{'BugID': 'MATH-413', 'Summary': 'Miscellaneous issues concerning the ""optimization"" package', 'Description': 'Revision 990792 contains changes triggered the following issues:\n* [MATH-394|https://issues.apache.org/jira/browse/MATH-394]\n* [MATH-397|https://issues.apache.org/jira/browse/MATH-397]\n* [MATH-404|https://issues.apache.org/jira/browse/MATH-404]\n\nThis issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):\n# ""BrentOptimizer"": a specific convergence checker must be used. ""LevenbergMarquardtOptimizer"" also has specific convergence checks.\n# Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):\n ** See ""BrentOptimizer"" and ""LevenbergMarquardtOptimizer"", the algorithm passes ""points"" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker).\n ** In ""PowellOptimizer"" the line search (""BrentOptimizer"") tolerances depend on the tolerances within the main algorithm. Since tolerances come with ""ConvergenceChecker"" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals (""BrentOptimizer"" field) to the enclosing class (""PowellOptimizer"").\n# Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all.\n# Class ""DirectSearchOptimizer"" (in package ""optimization.direct"") inherits from class ""AbstractScalarOptimizer"" (in package ""optimization.general"").\n# Some interfaces are defined in package ""optimization"" but their base implementations (abstract class that contain the boiler-plate code) are in package ""optimization.general"" (e.g. ""DifferentiableMultivariateVectorialOptimizer"" and ""BaseAbstractVectorialOptimizer"").\n# No check is performed to ensure the the convergence checker has been set (see e.g. ""BrentOptimizer"" and ""PowellOptimizer""); if it hasn\'t there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker.\n# ""NonLinearConjugateGradientOptimizer"": Ugly workaround for the checked ""ConvergenceException"".\n# Everywhere, we trail the checked ""FunctionEvaluationException"" although it is never used.\n# There remains some duplicate code (such as the ""multi-start loop"" in the various ""MultiStart..."" implementations).\n# The ""ConvergenceChecker"" interface is very general (the ""converged"" method can take any number of ""...PointValuePair""). However there remains a ""semantic"" problem: One cannot be sure that the list of points means the same thing for the caller of ""converged"" and within the implementation of the ""ConvergenceChecker"" that was independently set.\n# It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In ""LevenbergMarquartdOptimizer"" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations.\n# In ""AbstractLeastSquaresOptimizer"" and ""LevenbergMarquardtOptimizer"", occurences of ""OptimizationException"" were replaced by the unchecked ""ConvergenceException"" but in some cases it might not be the most appropriate one.\n# ""MultiStartUnivariateRealOptimizer"": in the other classes (""MultiStartMultivariate..."") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval).\n# The Javadoc utility raises warnings (see output of ""mvn site"") which I couldn\'t figure out how to correct.\n# Some previously existing classes and interfaces have become no more than a specialisation of new ""generics"" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.\n'}"
commons-math,bugs-dot-jar_MATH-414_5fe9b36c,"{'BugID': 'MATH-414', 'Summary': 'ConvergenceException in NormalDistributionImpl.cumulativeProbability()', 'Description': 'I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity.\nFor instance in the following code:\n\n\t@Test\n\tpublic void testCumulative() {\n\t\tfinal NormalDistribution nd = new NormalDistributionImpl();\n\t\tfor (int i = 0; i < 500; i++) {\n\t\t\tfinal double val = Math.exp(i);\n\t\t\ttry {\n\t\t\t\tSystem.out.println(""val = "" + val + "" cumulative = "" + nd.cumulativeProbability(val));\n\t\t\t} catch (MathException e) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail();\n\t\t\t}\n\t\t}\n\t}\n\nIn version 2.0, I get no exception. \n\nMy suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.\n'}"
commons-math,bugs-dot-jar_MATH-434_133cbc2d,"{'BugID': 'MATH-434', 'Summary': 'SimplexSolver returns unfeasible solution', 'Description': 'The SimplexSolver is returning an unfeasible solution:\n\nimport java.util.ArrayList;\nimport java.text.DecimalFormat;\nimport org.apache.commons.math.linear.ArrayRealVector;\nimport org.apache.commons.math.optimization.GoalType;\nimport org.apache.commons.math.optimization.OptimizationException;\nimport org.apache.commons.math.optimization.linear.*;\n\npublic class SimplexSolverBug {\n    \n    public static void main(String[] args) throws OptimizationException {\n        \n        LinearObjectiveFunction c = new LinearObjectiveFunction(new double[]{0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d}, 0.0d);\n        \n        ArrayList<LinearConstraint> cnsts = new ArrayList<LinearConstraint>(5);\n        LinearConstraint cnst;\n        cnst = new LinearConstraint(new double[] {1.0d, -0.1d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.EQ, -0.1d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, -1e-18d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 1.0d, 0.0d, 0.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 1.0d, 0.0d, -0.0128588d, 1e-5d}, Relationship.EQ, 0.0d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 0.0d, 0.0d, 1.0d, 1e-5d, -0.0128586d}, Relationship.EQ, 1e-10d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, -1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 1.0d, 0.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, -1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);\n        cnsts.add(cnst);\n        cnst = new LinearConstraint(new double[] {0.0d, 0.0d, 1.0d, 0.0d, 1.0d, 0.0d, 0.0d}, Relationship.GEQ, 0.0d);\n        cnsts.add(cnst);\n                \n        DecimalFormat df = new java.text.DecimalFormat(""0.#####E0"");\n        \n        System.out.println(""Constraints:"");\n        for(LinearConstraint con : cnsts) {\n            for (int i = 0; i < con.getCoefficients().getDimension(); ++i)\n                System.out.print(df.format(con.getCoefficients().getData()[i]) + "" "");\n            System.out.println(con.getRelationship() + "" "" + con.getValue());\n        }\n        \n        SimplexSolver simplex = new SimplexSolver(1e-7);\n        double[] sol = simplex.optimize(c, cnsts, GoalType.MINIMIZE, false).getPointRef();\n        System.out.println(""Solution:\\n"" + new ArrayRealVector(sol));\n        System.out.println(""Second constraint is violated!"");\n    }\n}\n\n\nIt\'s an odd problem, but something I ran across.  I tracked the problem to the getPivotRow routine in SimplexSolver.  It was choosing a pivot that resulted in a negative right-hand-side.  I recommend a fix by replacing\n                ...\n                if (MathUtils.equals(ratio, minRatio, epsilon)) {\n                ...\nwith\n                ...\n                if (MathUtils.equals(ratio, minRatio, Math.abs(epsilon/entry))) {\n                ...\n\nI believe this would be more appropriate (and at least resolves this particular problem).\n\nAlso, you may want to consider making a change in getPivotColumn to replace\n            ...\n            if (MathUtils.compareTo(tableau.getEntry(0, i), minValue, epsilon) < 0) {\n            ...\nwith\n            ...\n            if (tableau.getEntry(0, i) < minValue) \n            ...\nbecause I don\'t see the point of biasing earlier columns when multiple entries are within epsilon of each other.  Why not pick the absolute smallest.  I don\'t know that any problem can result from doing it the other way, but the latter may be a safer bet.\n\nVERY IMPORTANT: I discovered another bug that occurs when not restricting to non-negatives.  In SimplexTableu::getSolution(), \n          ...          \n          if (basicRows.contains(basicRow)) \n              // if multiple variables can take a given value\n              // then we choose the first and set the rest equal to 0\n              coefficients[i] = 0;\n          ...\nshould be\n          ...          \n          if (basicRows.contains(basicRow)) {\n              // if multiple variables can take a given value\n              // then we choose the first and set the rest equal to 0\n              coefficients[i] = (restrictToNonNegative ? 0 : -mostNegative);\n          ...\nIf necessary, I can give an example of where this bug causes a problem, but it should be fairly obvious why this was wrong.\n'}"
commons-math,bugs-dot-jar_MATH-471_a4b1948b,"{'BugID': 'MATH-471', 'Summary': 'MathUtils.equals(double, double) does not work properly for floats', 'Description': 'MathUtils.equals(double, double) does not work properly for floats.\n\nThere is no equals(float,float) so float parameters are automatically promoted to double. However, that is not necessarily appropriate, given that the ULP for a double is much smaller than the ULP for a float.\n\nSo for example:\n\n{code}\ndouble oneDouble = 1.0d;\nassertTrue(MathUtils.equals(oneDouble, Double.longBitsToDouble(1 + Double.doubleToLongBits(oneDouble)))); // OK\nfloat oneFloat = 1.0f;\nassertTrue(MathUtils.equals(oneFloat, Float.intBitsToFloat(1 + Float.floatToIntBits(oneFloat)))); // FAILS\nfloat  f1 = 333.33334f;\ndouble d1 = 333.33334d;\nassertTrue(MathUtils.equals(d1, f1)); // FAILS\n{code}\n\nI think the equals() methods need to be duplicated with the appropriate changes for floats to avoid any problems with the promotion of floats.\n'}"
commons-math,bugs-dot-jar_MATH-482_6d6649ef,"{'BugID': 'MATH-482', 'Summary': 'FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f', 'Description': 'FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f.\n\nThis is because the wrong variable is returned.\n\nThe bug was not detected by the test case ""testMinMaxFloat()"" because that has a bug too - it tests doubles, not floats.'}"
commons-math,bugs-dot-jar_MATH-519_26a61077,"{'BugID': 'MATH-519', 'Summary': 'GaussianFitter Unexpectedly Throws NotStrictlyPositiveException', 'Description': ""Running the following:\n\n    \tdouble[] observations = \n    \t{ \n    \t\t\t1.1143831578403364E-29, \n    \t\t\t 4.95281403484594E-28, \n    \t\t\t 1.1171347211930288E-26, \n    \t\t\t 1.7044813962636277E-25, \n    \t\t\t 1.9784716574832164E-24, \n    \t\t\t 1.8630236407866774E-23, \n    \t\t\t 1.4820532905097742E-22, \n    \t\t\t 1.0241963854632831E-21, \n    \t\t\t 6.275077366673128E-21, \n    \t\t\t 3.461808994532493E-20, \n    \t\t\t 1.7407124684715706E-19, \n    \t\t\t 8.056687953553974E-19, \n    \t\t\t 3.460193945992071E-18, \n    \t\t\t 1.3883326374011525E-17, \n    \t\t\t 5.233894983671116E-17, \n    \t\t\t 1.8630791465263745E-16, \n    \t\t\t 6.288759227922111E-16, \n    \t\t\t 2.0204433920597856E-15, \n    \t\t\t 6.198768938576155E-15, \n    \t\t\t 1.821419346860626E-14, \n    \t\t\t 5.139176445538471E-14, \n    \t\t\t 1.3956427429045787E-13, \n    \t\t\t 3.655705706448139E-13, \n    \t\t\t 9.253753324779779E-13, \n    \t\t\t 2.267636001476696E-12, \n    \t\t\t 5.3880460095836855E-12, \n    \t\t\t 1.2431632654852931E-11 \n    \t};\n  \n    \tGaussianFitter g = \n    \t\tnew GaussianFitter(new LevenbergMarquardtOptimizer());\n    \t\n    \tfor (int index = 0; index < 27; index++)\n    \t{\n    \t\tg.addObservedPoint(index, observations[index]);\n    \t}\n       \tg.fit();\n\nResults in:\n\norg.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)\n\tat org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129)\n\n\nI'm guessing the initial guess for sigma is off.  ""}"
commons-math,bugs-dot-jar_MATH-546_b6bf8f41,"{'BugID': 'MATH-546', 'Summary': 'Truncation issue in KMeansPlusPlusClusterer', 'Description': ""The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable\n  int sum = 0;\nThis variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1.\n\nAs an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.""}"
commons-math,bugs-dot-jar_MATH-554_fbbb96eb,"{'BugID': 'MATH-554', 'Summary': 'Vector3D.crossProduct is sensitive to numerical cancellation', 'Description': 'Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:\n{code}\nVector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1);\nVector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1);\nSystem.out.println(Vector3D.crossProduct(v1, v2));\n{code}\n\nThe previous code displays { -1, 2, 0 } instead of the correct answer { -1, 2, 1 }'}"
commons-math,bugs-dot-jar_MATH-555_328513f3,"{'BugID': 'MATH-555', 'Summary': 'MathUtils round method should propagate rather than wrap Runitme exceptions', 'Description': 'MathUtils.round(double, int, int) can generate IllegalArgumentException or ArithmeticException.  Instead of wrapping these exceptions in MathRuntimeException, the conditions under which these exceptions can be thrown should be documented and the exceptions should be propagated directly to the caller.'}"
commons-math,bugs-dot-jar_MATH-559_fc409e88,"{'BugID': 'MATH-559', 'Summary': 'Remove ""assert"" from ""MathUtils.equals""', 'Description': 'The ""assert"" in methods ""equals(double,double,int)"" and ""equals(float,float,int)"" is not necessary.\n'}"
commons-math,bugs-dot-jar_MATH-618_2123f780,"{'BugID': 'MATH-618', 'Summary': 'Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same', 'Description': 'For both Complex add and subtract, the javadoc states that\n\n{code}\n     * If either this or <code>rhs</code> has a NaN value in either part,\n     * {@link #NaN} is returned; otherwise Inifinite and NaN values are\n     * returned in the parts of the result according to the rules for\n     * {@link java.lang.Double} arithmetic\n{code}\n\nSubtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).\n'}"
commons-math,bugs-dot-jar_MATH-631_334c01e6,"{'BugID': 'MATH-631', 'Summary': '""RegulaFalsiSolver"" failure', 'Description': 'The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing ""PegasusSolver"", the answer is found after 17 evaluations.\n'}"
commons-math,bugs-dot-jar_MATH-631_c0b49542,"{'BugID': 'MATH-631', 'Summary': '""RegulaFalsiSolver"" failure', 'Description': 'The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing ""PegasusSolver"", the answer is found after 17 evaluations.\n'}"
commons-math,bugs-dot-jar_MATH-631_ebc61de9,"{'BugID': 'MATH-631', 'Summary': '""RegulaFalsiSolver"" failure', 'Description': 'The following unit test:\n{code}\n@Test\npublic void testBug() {\n    final UnivariateRealFunction f = new UnivariateRealFunction() {\n            @Override\n            public double value(double x) {\n                return Math.exp(x) - Math.pow(Math.PI, 3.0);\n            }\n        };\n\n    UnivariateRealSolver solver = new RegulaFalsiSolver();\n    double root = solver.solve(100, f, 1, 10);\n}\n{code}\nfails with\n{noformat}\nillegal state: maximal count (100) exceeded: evaluations\n{noformat}\n\nUsing ""PegasusSolver"", the answer is found after 17 evaluations.\n'}"
commons-math,bugs-dot-jar_MATH-639_8b418000,"{'BugID': 'MATH-639', 'Summary': 'numerical problems in rotation creation', 'Description': ""building a rotation from the following vector pairs leads to NaN:\nu1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377\nu2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10\nv1 = 1, 0, 0\nv2 = 0, 0, 1\n\nThe constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:\n <v1'|v1'> == <u1|u1>\n <v2'|v2'> == <u2|u2>\n <u1 |u2>  == <v1'|v2'>\n\nOnce the (v1', v2') pair has been computed, we compute the cross product:\n  k = (v1' - u1)^(v2' - u2)\n\nand the scalar product:\n  c = <k | (u1^u2)>\n\nBy construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)].\nc should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm.\n\nHowever, there are numerical problems with the vector above with the way these computations are done, as shown\nby the following comparisons, showing the result we get from our Java code and the result we get from manual\ncomputation with the same formulas but with enhanced precision:\n\ncommons math:   k = 38514476.5,            -84.,                           -1168590144\nhigh precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208...\n\nand it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get:\ncommons math    c = -1.2397173627587605E20\nhigh precision: c =  558382746168463196.7079627...\n\nWe have lost ALL significant digits in cancellations, and even the sign is wrong!\n""}"
commons-math,bugs-dot-jar_MATH-640_98556fed,"{'BugID': 'MATH-640', 'Summary': 'AbstractRandomGenerator nextInt() and nextLong() default implementations generate only positive values', 'Description': 'The javadoc for these methods (and what is specified in the RandomGenerator interface) says that all int / long values should be in the range of these methods.  The default implementations provided in this class do not generate negative values.'}"
commons-math,bugs-dot-jar_MATH-657_32b0f733,"{'BugID': 'MATH-657', 'Summary': 'Division by zero', 'Description': 'In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].\n'}"
commons-math,bugs-dot-jar_MATH-657_97b440fc,"{'BugID': 'MATH-657', 'Summary': 'Division by zero', 'Description': 'In class {{Complex}}, division by zero always returns NaN. I think that it should return NaN only when the numerator is also {{ZERO}}, otherwise the result should be {{INF}}. See [here|http://en.wikipedia.org/wiki/Riemann_sphere#Arithmetic_operations].\n'}"
commons-math,bugs-dot-jar_MATH-679_5e638976,"{'BugID': 'MATH-679', 'Summary': 'Integer overflow in OpenMapRealMatrix', 'Description': 'computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information).\n\nWorkaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.'}"
commons-math,bugs-dot-jar_MATH-691_118f0cc0,"{'BugID': 'MATH-691', 'Summary': 'Statistics.setVarianceImpl makes getStandardDeviation produce NaN', 'Description': 'Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:\n\n{code:java}\nint[] scores = {1, 2, 3, 4};\nSummaryStatistics stats = new SummaryStatistics();\nstats.setVarianceImpl(new Variance(false)); //use ""population variance""\nfor(int i : scores) {\n  stats.addValue(i);\n}\ndouble sd = stats.getStandardDeviation();\nSystem.out.println(sd);\n{code}\n\nA workaround suggested by Mikkel is:\n{code:java}\n  double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());\n{code}'}"
commons-math,bugs-dot-jar_MATH-695_7980a242,"{'BugID': 'MATH-695', 'Summary': 'Incomplete reinitialization with some events handling', 'Description': ""I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way.\nI can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.""}"
commons-math,bugs-dot-jar_MATH-699_b2e24119,"{'BugID': 'MATH-699', 'Summary': 'inverseCumulativeDistribution fails with cumulative distribution having a plateau', 'Description': 'This bug report follows MATH-692. The attached unit test fails. As required by the definition in MATH-692, the lower-bound of the interval on which the cdf is constant should be returned. This is not so at the moment.'}"
commons-math,bugs-dot-jar_MATH-704_3f645310,"{'BugID': 'MATH-704', 'Summary': 'One of Variance.evaluate() methods does not work correctly', 'Description': 'The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset.\nSimilar method in Mean class seems to work.\nI did not check other methods taking the part of the array; they may have the same problem.\n\nWorkaround: I had to shrink my arrays and use the method without the length.'}"
commons-math,bugs-dot-jar_MATH-705_645d642b,"{'BugID': 'MATH-705', 'Summary': 'DormandPrince853 integrator leads to revisiting of state events', 'Description': 'See the attached ReappearingEventTest.java. It has two unit tests, which use either the DormandPrince853 or the GraggBulirschStoer integrator, on the same ODE problem. It is a problem starting at time 6.0, with 7 variables, and 1 state event. The state event was previously detected at time 6.0, which is why I start there now. I provide and end time of 10.0. Since I start at the state event, I expect to integrate all the way to the end (10.0). For the GraggBulirschStoer this is what happens (see attached ReappearingEventTest.out). For the DormandPrince853Integerator, it detects a state event and stops integration at 6.000000000000002.\n\nI think the problem becomes clear by looking at the output in ReappearingEventTest.out, in particular these lines:\n\n{noformat}\ncomputeDerivatives: t=6.0                  y=[2.0                 , 2.0                 , 2.0                 , 4.0                 , 2.0                 , 7.0                 , 15.0                ]\n(...)\ng                 : t=6.0                  y=[1.9999999999999996  , 1.9999999999999996  , 1.9999999999999996  , 4.0                 , 1.9999999999999996  , 7.0                 , 14.999999999999998  ]\n(...)\nfinal result      : t=6.000000000000002    y=[2.0000000000000013  , 2.0000000000000013  , 2.0000000000000013  , 4.000000000000002   , 2.0000000000000013  , 7.000000000000002   , 15.0                ]\n{noformat}\n\nThe initial value of the last variable in y, the one that the state event refers to, is 15.0. However, the first time it is given to the g function, the value is 14.999999999999998. This value is less than 15, and more importantly, it is a value from the past (as all functions are increasing), *before* the state event. This makes that the state event re-appears immediately, and integration stops at 6.000000000000002 because of the detected state event.\n\nI find it puzzling that for the DormandPrince853Integerator the y array that is given to the first evaluation of the g function, has different values than the y array that is the input to the problem. For GraggBulirschStoer is can be seen that the y arrays have identical values.'}"
commons-math,bugs-dot-jar_MATH-713_f656676e,"{'BugID': 'MATH-713', 'Summary': 'Negative value with restrictNonNegative', 'Description': 'Problem: commons-math-2.2 SimplexSolver.\n\nA variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call:\nSimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true);\n\nFunction\n1 * x + 1 * y + 0\n\nConstraints:\n1 * x + 0 * y = 1\n\nResult:\nx = 1; y = -1;\n\nProbably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.'}"
commons-math,bugs-dot-jar_MATH-716_faa77857,"{'BugID': 'MATH-716', 'Summary': 'BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary', 'Description': 'In some cases, the aging feature in BracketingNthOrderBrentSolver fails.\nIt attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket.\nIn the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).'}"
commons-math,bugs-dot-jar_MATH-718_3a08bfa6,"{'BugID': 'MATH-718', 'Summary': 'inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.', 'Description': ""The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem.\n\n{{System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5));}}\n\nThis returns 499525, though it should be 499999.\n\nI'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.""}"
commons-math,bugs-dot-jar_MATH-722_95d15eff,"{'BugID': 'MATH-722', 'Summary': '[math] Complex Tanh for ""big"" numbers', 'Description': 'Hi,\n\nIn Complex.java the tanh is computed with the following formula:\n\ntanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i\n\nThe problem that I\'m finding is that as soon as ""a"" is a ""big"" number,\nboth sinh(2a) and cosh(2a) are infinity and then the method tanh returns in\nthe real part NaN (infinity/infinity) when it should return 1.0.\n\nWouldn\'t it be appropiate to add something as in the FastMath library??:\n\nif (real>20.0){\n      return createComplex(1.0, 0.0);\n}\nif (real<-20.0){\n      return createComplex(-1.0, 0.0);\n}\n\n\nBest regards,\n\nJBB\n'}"
commons-math,bugs-dot-jar_MATH-723_1352a70f,"{'BugID': 'MATH-723', 'Summary': 'BitStreamGenerators (MersenneTwister, Well generators) do not clear normal deviate cache on setSeed', 'Description': 'The BitStream generators generate normal deviates (for nextGaussian) in pairs, caching the last value generated. When reseeded, the cache should be cleared; otherwise seeding two generators with the same value is not guaranteed to generate the same sequence.'}"
commons-math,bugs-dot-jar_MATH-724_9c8bb934,"{'BugID': 'MATH-724', 'Summary': 'RandomDataImpl.nextInt does not distribute uniformly for negative lower bound', 'Description': 'When using the RandomDataImpl.nextInt function to get a uniform sample in a [lower, upper] interval, when the lower value is less than zero, the output is not uniformly distributed, as the lowest value is practically never returned.\n\nSee the attached NextIntUniformTest.java file. It uses a [-3, 5] interval. For several values between 0 and 1, testNextIntUniform1 prints the return value of RandomDataImpl.nextInt (as double and as int). We see that -2 through 5 are returned several times. The -3 value however, is only returned for 0.0, and is thus under-respresented in the integer samples. The output of test method testNextIntUniform2 also clearly shows that value -3 is never sampled.'}"
commons-math,bugs-dot-jar_MATH-727_69273dca,"{'BugID': 'MATH-727', 'Summary': 'too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)', 'Description': 'Adaptive step size integrators compute the first step size by themselves if it is not provided.\nFor embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.'}"
commons-math,bugs-dot-jar_MATH-727_d2777388,"{'BugID': 'MATH-727', 'Summary': 'too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)', 'Description': 'Adaptive step size integrators compute the first step size by themselves if it is not provided.\nFor embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.'}"
commons-math,bugs-dot-jar_MATH-738_f64b6a90,"{'BugID': 'MATH-738', 'Summary': 'Incomplete beta function I(x, a, b) is inaccurate for large values of a and/or b', 'Description': 'This was first reported in MATH-718. The result of the current implementation of the incomplete beta function I(x, a, b) is inaccurate when a and/or b are large-ish. \r\n\r\nI\'ve skimmed through [slatec|http://www.netlib.org/slatec/fnlib/betai.f], GSL, [Boost|http://www.boost.org/doc/libs/1_38_0/libs/math/doc/sf_and_dist/html/math_toolkit/special/sf_beta/ibeta_function.html] as well as NR. At first sight, neither uses the same method to compute this function. I think [TOMS-708|http://www.netlib.org/toms/708] is probably the best option.\r\n\r\n_Issue moved from MATH project on January 27, 2018 (concerned implementation was moved to module {{commons-numbers-gamma}} of ""Commons Numbers"")._'}"
commons-math,bugs-dot-jar_MATH-744_8a83581e,"{'BugID': 'MATH-744', 'Summary': 'BigFraction.doubleValue() returns Double.NaN for large numerators or denominators', 'Description': ""The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range.\n\nI have a patch to fix it, if I can figure out how to attach it here I will.""}"
commons-math,bugs-dot-jar_MATH-757_76b7413d,"{'BugID': 'MATH-757', 'Summary': 'ResizableDoubleArray is not thread-safe yet has some synch. methods', 'Description': 'ResizableDoubleArray has several synchronised methods, but is not thread-safe, because class variables are not always accessed using the lock.\n\nIs the class supposed to be thread-safe?\n\nIf so, all accesses (read and write) need to be synch.\n\nIf not, the synch. qualifiers could be dropped.\n\nIn any case, the protected fields need to be made private.\n'}"
commons-math,bugs-dot-jar_MATH-776_b9ca51f0,"{'BugID': 'MATH-776', 'Summary': 'Need range checks for elitismRate in ElitisticListPopulation constructors.', 'Description': ""There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.""}"
commons-math,bugs-dot-jar_MATH-778_5b9302d5,"{'BugID': 'MATH-778', 'Summary': 'Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)', 'Description': 'In class {{org.apache.commons.math3.Dfp}},  the method {{multiply(int n)}} is limited to {{0 <= n <= 9999}}. This is not consistent with the general contract of {{FieldElement.multiply(int n)}}, where there should be no limitation on the values of {{n}}.'}"
commons-math,bugs-dot-jar_MATH-779_ebadb558,"{'BugID': 'MATH-779', 'Summary': 'ListPopulation Iterator allows you to remove chromosomes from the population.', 'Description': 'Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.'}"
commons-math,bugs-dot-jar_MATH-780_dd6cefb0,"{'BugID': 'MATH-780', 'Summary': 'BSPTree class and recovery of a Euclidean 3D BRep', 'Description': 'New to the work here. Thanks for your efforts on this code.\n\nI create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem.\n\nAny ideas?\n'}"
commons-math,bugs-dot-jar_MATH-781_3c4cb189,"{'BugID': 'MATH-781', 'Summary': 'SimplexSolver gives bad results', 'Description': 'Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0\nin a simple test problem. It works well in commons-math-2.2. '}"
commons-math,bugs-dot-jar_MATH-789_621bbb8f,"{'BugID': 'MATH-789', 'Summary': 'Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix', 'Description': 'The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples):\n\nArray2DRowRealMatrix{\n{0.0,0.0,0.0,0.0,0.0},\n{0.0,0.013445532,0.01039469,0.009881156,0.010499559},\n{0.0,0.01039469,0.023006616,0.008196856,0.010732709},\n{0.0,0.009881156,0.008196856,0.019023866,0.009210099},\n{0.0,0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data1)\n   V1 V2 V3 V4 V5\nV1 0 0.000000000 0.00000000 0.000000000 0.000000000\nV2 0 0.013383931 0.01034401 0.009913271 0.010506733\nV3 0 0.010344006 0.02309479 0.008374730 0.010759306\nV4 0 0.009913271 0.00837473 0.019005488 0.009187287\nV5 0 0.010506733 0.01075931 0.009187287 0.019021483\n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.0,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.0,0.008196856,0.010732709},\n{0.0,0.0,0.0,0.0,0.0},\n{0.009881156,0.008196856,0.0,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.0,0.009210099,0.019107243}}\n\n> cov(data2)\n            V1 V2 V3 V4 V5\nV1 0.006922905 0.010507692 0 0.005817399 0.010330529\nV2 0.010507692 0.023428918 0 0.008273152 0.010735568\nV3 0.000000000 0.000000000 0 0.000000000 0.000000000\nV4 0.005817399 0.008273152 0 0.004929843 0.009048759\nV5 0.010330529 0.010735568 0 0.009048759 0.018683544 \n\nArray2DRowRealMatrix{\n{0.013445532,0.01039469,0.009881156,0.010499559},\n{0.01039469,0.023006616,0.008196856,0.010732709},\n{0.009881156,0.008196856,0.019023866,0.009210099},\n{0.010499559,0.010732709,0.009210099,0.019107243}}\n\n> cov(data3)\n            V1          V2          V3          V4\nV1 0.013445047 0.010478862 0.009955904 0.010529542\nV2 0.010478862 0.022910522 0.008610113 0.011046353\nV3 0.009955904 0.008610113 0.019250975 0.009464442\nV4 0.010529542 0.011046353 0.009464442 0.019260317\n\n\nI\'ve traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0},{0.0759577418122063,0.0876125188474239,0.0,0.0,0.0},{0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0},{0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0},{0.13822895138139477,0.0,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 5\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0},{0.07764443622513505,0.13029949164628746,0.0},{0.0,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.0},{0.13822895138139477,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 3\n\nCorrelatedRandomVectorGenerator.getRootMatrix() = \nArray2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785},{0.07764443622513505,0.13029949164628746,0.0,0.0},{0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0},{0.13822895138139477,0.0,0.0,0.0}}\nCorrelatedRandomVectorGenerator.getRank() = 4\n\nClearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don\'t know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the ""rectangular"" variant (also not at the links provided in the javadoc).'}"
commons-math,bugs-dot-jar_MATH-801_118e94b5,"{'BugID': 'MATH-801', 'Summary': 'Quaternion not normalized after construction', 'Description': ""The use of the Rotation(Vector3D u1,Vector3D u2,Vector3D v1,Vector3D v2) constructor with normalized angle can apparently lead to un-normalized quaternion.\nThis case appeared to me with the following data :\nu1 = (0.9999988431610581, -0.0015210774290851095, 0.0)\nu2 = (0.0, 0.0, 1.0)\nand \nv1 = (0.9999999999999999, 0.0, 0.0)\nv2 = (0.0, 0.0, -1.0)\n\nThis lead to the following quaternion :\nq0 = 225783.35177064248\nq1 = 0.0\nq2 = 0.0\nq3 = -3.3684446110762543E-9\n\nI was expecting to have a normalized quaternion, as input vector's are normalized. Does the quaternion shouldn't be normalized ?\nI've joined the corresponding piece of code as JUnit Test case""}"
commons-math,bugs-dot-jar_MATH-812_607c9ec6,"{'BugID': 'MATH-812', 'Summary': 'In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators', 'Description': 'In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.\n\nProposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.\n\nSame issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.\n\nAnother option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.'}"
commons-math,bugs-dot-jar_MATH-812_6eb46555,"{'BugID': 'MATH-812', 'Summary': 'In RealVector, dotProduct and outerProduct return wrong results due to misuse of sparse iterators', 'Description': 'In class {{RealVector}}, the default implementation of {{RealMatrix outerProduct(RealVector)}} uses sparse iterators on the entries of the two vectors. The rationale behind this is that {{0d * x == 0d}} is {{true}} for all {{double x}}. This assumption is in fact false, since {{0d * NaN == NaN}}.\n\nProposed fix is to loop through *all* entries of both vectors. This can have a significant impact on the CPU cost, but robustness should probably be preferred over speed in default implementations.\n\nSame issue occurs with {{double dotProduct(RealVector)}}, which uses sparse iterators for {{this}} only.\n\nAnother option would be to through an exception if {{isNaN()}} is {{true}}, in which case caching could be used for both {{isNaN()}} and {{isInfinite()}}.'}"
commons-math,bugs-dot-jar_MATH-828_a49e443c,"{'BugID': 'MATH-828', 'Summary': 'Not expected UnboundedSolutionException', 'Description': ""SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables.\n\nIn order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions.\nFirst iteration is runned with predefined set of input data with which the Solver gives back an appropriate result.\n\nThe problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values.\n\nWhat is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem.\n\nThe problem is formulated as\nmin(1*t + 0*L) (for every r-th subject)\ns.t.\n-q(r) + QL >= 0\nx(r)t - XL >= 0\nL >= 0\nwhere \nr = 1..R, \nL = {l(1), l(2), ..., l(R)} (vector of R rows and 1 column),\nQ - coefficients matrix MxR\nX - coefficients matrix NxR ""}"
commons-math,bugs-dot-jar_MATH-835_63a48705,"{'BugID': 'MATH-835', 'Summary': 'Fraction percentageValue rare overflow', 'Description': 'The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value.\n\nThe patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.'}"
commons-math,bugs-dot-jar_MATH-836_d7c0f27e,"{'BugID': 'MATH-836', 'Summary': 'Fraction(double, int) constructor strange behaviour', 'Description': 'The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100\'000s), two distinct bugs can manifest:\n\n1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value\n\n2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831.\n\nI have, as of yet, not found a solution. The constructor looks like this:\n\npublic Fraction(double value, int maxDenominator)\n        throws FractionConversionException\n    {\n       this(value, 0, maxDenominator, 100);\n    }\n\nIncreasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest. \n\nThe problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find.\n\nThis bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.\n\n* It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that ""since fractions are always in lowest terms, numerators and can be compared directly for equality"", so it seems like this is the intention. '}"
commons-math,bugs-dot-jar_MATH-844_7994d3ee,"{'BugID': 'MATH-844', 'Summary': '""HarmonicFitter.ParameterGuesser"" sometimes fails to return sensible values', 'Description': 'The inner class ""ParameterGuesser"" in ""HarmonicFitter"" (package ""o.a.c.m.optimization.fitting"") fails to compute a usable guess for the ""amplitude"" parameter.\n'}"
commons-math,bugs-dot-jar_MATH-848_ad252a8c,"{'BugID': 'MATH-848', 'Summary': 'EigenDecomposition fails for certain matrices', 'Description': 'The Schurtransformation of the following matrix fails, which is a preliminary step for the Eigendecomposition:\n\nRealMatrix m = MatrixUtils.DEFAULT_FORMAT.parse(""{{0.184944928,-0.0646971046,0.0774755812,-0.0969651755,-0.0692648806,0.3282344352,-0.0177423074,0.206313634},{-0.0742700134,-0.028906303,-0.001726946,-0.0375550146,-0.0487737922,-0.2616837868,-0.0821201295,-0.2530000167},{0.2549910127,0.0995733692,-0.0009718388,0.0149282808,0.1791878897,-0.0823182816,0.0582629256,0.3219545182},{-0.0694747557,-0.1880649148,-0.2740630911,0.0720096468,-0.1800836914,-0.3518996425,0.2486747833,0.6257938167},{0.0536360918,-0.1339297778,0.2241579764,-0.0195327484,-0.0054103808,0.0347564518,0.5120802482,-0.0329902864},{-0.5933332356,-0.2488721082,0.2357173629,0.0177285473,0.0856630593,-0.35671263,-0.1600668126,-0.1010899621},{-0.0514349819,-0.0854319435,0.1125050061,0.006345356,-0.2250000688,-0.220934309,0.1964623477,-0.1512329924},{0.0197395947,-0.1997170581,-0.1425959019,-0.274947791,-0.0969467073,0.060368852,-0.2826905192,0.1794315473}}"");\n'}"
commons-math,bugs-dot-jar_MATH-855_350f726c,"{'BugID': 'MATH-855', 'Summary': '""BrentOptimizer"" not always reporting the best point', 'Description': '{{BrentOptimizer}} (package ""o.a.c.m.optimization.univariate"") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.'}"
commons-math,bugs-dot-jar_MATH-855_ac597cc1,"{'BugID': 'MATH-855', 'Summary': '""BrentOptimizer"" not always reporting the best point', 'Description': '{{BrentOptimizer}} (package ""o.a.c.m.optimization.univariate"") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.'}"
commons-math,bugs-dot-jar_MATH-859_66dece12,"{'BugID': 'MATH-859', 'Summary': 'Fix and then deprecate isSupportXxxInclusive in RealDistribution interface', 'Description': 'The conclusion from [1] was never implemented. We should deprecate these\nproperties from the RealDistribution interface, but since removal\nwill have to wait until 4.0, we should agree on a precise\ndefinition and fix the code to match it in the mean time.\n\nThe definition that I propose is that isSupportXxxInclusive means\nthat when the density function is applied to the upper or lower\nbound of support returned by getSupportXxxBound, a finite (i.e. not\ninfinite), not NaN value is returned.\n\n[1] http://markmail.org/message/dxuxh7eybl7xejde\n'}"
commons-math,bugs-dot-jar_MATH-864_abe53a53,"{'BugID': 'MATH-864', 'Summary': 'CMAESOptimizer does not enforce bounds', 'Description': 'The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.'}"
commons-math,bugs-dot-jar_MATH-865_b55e0206,"{'BugID': 'MATH-865', 'Summary': 'Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function', 'Description': 'If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.'}"
commons-math,bugs-dot-jar_MATH-867_bfbb156d,"{'BugID': 'MATH-867', 'Summary': 'CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound. ', 'Description': 'When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.'}"
commons-math,bugs-dot-jar_MATH-880_2a9cbbab,"{'BugID': 'MATH-880', 'Summary': 'Polygon difference produces erronious results in some cases', 'Description': 'The 2D polygon difference method is returning incorrect\nresults.  Below is a test case of subtracting two polygons (Sorry,\nthis is the simplest case that I could find that duplicates the\nproblem).  \n\nThere are three problems with the result. The first is that the first\npoint of the first set of vertices is null (and the first point of the\nsecond set is also null).  The second is that, even if the first null\npoints are ignored,  the returned polygon is not the correct result.\nThe first and last points are way off, and the remaining points do not\nmatch the original polygon boundaries.  Additionally, there are two\nholes that are returned in the results.  This subtraction case should\nnot have holes.\n\n{code:title=""Complex Polygon Difference Test""}\npublic void testComplexDifference() {\n        Vector2D[][] vertices1 = new Vector2D[][] {\n            new Vector2D[] {\n                    new Vector2D( 90.08714908223715,  38.370299337260235),\n                    new Vector2D( 90.08709517675004,  38.3702895991413),\n                    new Vector2D( 90.08401538704919,  38.368849330127944),\n                    new Vector2D( 90.08258210430711,  38.367634558585564),\n                    new Vector2D( 90.08251455106665,  38.36763409247078),\n                    new Vector2D( 90.08106599752608,  38.36761621664249),\n                    new Vector2D( 90.08249585300035,  38.36753627557965),\n                    new Vector2D( 90.09075743352184,  38.35914647644972),\n                    new Vector2D( 90.09099945896571,  38.35896264724079),\n                    new Vector2D( 90.09269383800086,  38.34595756121246),\n                    new Vector2D( 90.09638631543191,  38.3457988093121),\n                    new Vector2D( 90.09666417351019,  38.34523360999418),\n                    new Vector2D( 90.1297082145872,  38.337670454923625),\n                    new Vector2D( 90.12971687748956,  38.337669827794684),\n                    new Vector2D( 90.1240820219179,  38.34328502001131),\n                    new Vector2D( 90.13084259656404,  38.34017811765017),\n                    new Vector2D( 90.13378567942857,  38.33860579180606),\n                    new Vector2D( 90.13519557833206,  38.33621054663689),\n                    new Vector2D( 90.13545616732307,  38.33614965452864),\n                    new Vector2D( 90.13553111202748,  38.33613962818305),\n                    new Vector2D( 90.1356903436448,  38.33610227127048),\n                    new Vector2D( 90.13576283227428,  38.33609255422783),\n                    new Vector2D( 90.13595870833188,  38.33604606376991),\n                    new Vector2D( 90.1361556630693,  38.3360024198866),\n                    new Vector2D( 90.13622408795709,  38.335987048115726),\n                    new Vector2D( 90.13696189099994,  38.33581914328681),\n                    new Vector2D( 90.13746655304897,  38.33616706665265),\n                    new Vector2D( 90.13845973716064,  38.33650776167099),\n                    new Vector2D( 90.13950901827667,  38.3368469456463),\n                    new Vector2D( 90.14393814424852,  38.337591835857495),\n                    new Vector2D( 90.14483839716831,  38.337076122362475),\n                    new Vector2D( 90.14565474433601,  38.33769000964429),\n                    new Vector2D( 90.14569421179482,  38.3377117256905),\n                    new Vector2D( 90.14577067124333,  38.33770883625908),\n                    new Vector2D( 90.14600350631684,  38.337714326520995),\n                    new Vector2D( 90.14600355139731,  38.33771435193319),\n                    new Vector2D( 90.14600369112401,  38.33771443882085),\n                    new Vector2D( 90.14600382486884,  38.33771453466096),\n                    new Vector2D( 90.14600395205912,  38.33771463904344),\n                    new Vector2D( 90.14600407214999,  38.337714751520764),\n                    new Vector2D( 90.14600418462749,  38.337714871611695),\n                    new Vector2D( 90.14600422249327,  38.337714915811034),\n                    new Vector2D( 90.14867838361471,  38.34113888210675),\n                    new Vector2D( 90.14923750157374,  38.341582537502575),\n                    new Vector2D( 90.14877083250991,  38.34160685841391),\n                    new Vector2D( 90.14816667319519,  38.34244232585684),\n                    new Vector2D( 90.14797696744586,  38.34248455284745),\n                    new Vector2D( 90.14484318014337,  38.34385573215269),\n                    new Vector2D( 90.14477919958296,  38.3453797747614),\n                    new Vector2D( 90.14202393306448,  38.34464324839456),\n                    new Vector2D( 90.14198920640195,  38.344651155237216),\n                    new Vector2D( 90.14155207025175,  38.34486424263724),\n                    new Vector2D( 90.1415196143314,  38.344871730519),\n                    new Vector2D( 90.14128611910814,  38.34500196593859),\n                    new Vector2D( 90.14047850603913,  38.34600084496253),\n                    new Vector2D( 90.14045907000337,  38.34601860032171),\n                    new Vector2D( 90.14039496493928,  38.346223030432384),\n                    new Vector2D( 90.14037626063737,  38.346240203360026),\n                    new Vector2D( 90.14030005823724,  38.34646920000705),\n                    new Vector2D( 90.13799164754806,  38.34903093011013),\n                    new Vector2D( 90.11045289492762,  38.36801537312368),\n                    new Vector2D( 90.10871471476526,  38.36878044144294),\n                    new Vector2D( 90.10424901707671,  38.374300101757),\n                    new Vector2D( 90.10263482039932,  38.37310041316073),\n                    new Vector2D( 90.09834601753448,  38.373615053823414),\n                    new Vector2D( 90.0979455456843,  38.373578376172475),\n                    new Vector2D( 90.09086514328669,  38.37527884194668),\n                    new Vector2D( 90.09084931407364,  38.37590801712463),\n                    new Vector2D( 90.09081227075944,  38.37526295920463),\n                    new Vector2D( 90.09081378927135,  38.375193883266434)\n            }\n        };\n        PolygonsSet set1 = buildSet(vertices1);\n\n        Vector2D[][] vertices2 = new Vector2D[][] {\n            new Vector2D[] {\n                    new Vector2D( 90.13067558880044,  38.36977255037573),\n                    new Vector2D( 90.12907570488,  38.36817308242706),\n                    new Vector2D( 90.1342774136516,  38.356886880294724),\n                    new Vector2D( 90.13090330629757,  38.34664392676211),\n                    new Vector2D( 90.13078571364593,  38.344904617518466),\n                    new Vector2D( 90.1315602208914,  38.3447185040846),\n                    new Vector2D( 90.1316336226821,  38.34470643148342),\n                    new Vector2D( 90.134020944832,  38.340936644972885),\n                    new Vector2D( 90.13912536387306,  38.335497255122334),\n                    new Vector2D( 90.1396178806582,  38.334878075552126),\n                    new Vector2D( 90.14083049696671,  38.33316530644106),\n                    new Vector2D( 90.14145252901329,  38.33152722916191),\n                    new Vector2D( 90.1404779335565,  38.32863516047786),\n                    new Vector2D( 90.14282712131586,  38.327504432532066),\n                    new Vector2D( 90.14616669875488,  38.3237354115015),\n                    new Vector2D( 90.14860976050608,  38.315714862457924),\n                    new Vector2D( 90.14999277782437,  38.3164932507504),\n                    new Vector2D( 90.15005207194997,  38.316534677663356),\n                    new Vector2D( 90.15508513859612,  38.31878731691609),\n                    new Vector2D( 90.15919938519221,  38.31852743183782),\n                    new Vector2D( 90.16093758658837,  38.31880662005153),\n                    new Vector2D( 90.16099420184912,  38.318825953291594),\n                    new Vector2D( 90.1665411125756,  38.31859497874757),\n                    new Vector2D( 90.16999653861313,  38.32505772048029),\n                    new Vector2D( 90.17475243391698,  38.32594398441148),\n                    new Vector2D( 90.17940844844992,  38.327427213761325),\n                    new Vector2D( 90.20951909541378,  38.330616833491774),\n                    new Vector2D( 90.2155400467941,  38.331746223670336),\n                    new Vector2D( 90.21559881391778,  38.33175551425302),\n                    new Vector2D( 90.21916646426041,  38.332584299620805),\n                    new Vector2D( 90.23863749852285,  38.34778978875795),\n                    new Vector2D( 90.25459855175802,  38.357790570608984),\n                    new Vector2D( 90.25964298227257,  38.356918010203174),\n                    new Vector2D( 90.26024593994703,  38.361692743151366),\n                    new Vector2D( 90.26146187570015,  38.36311080550837),\n                    new Vector2D( 90.26614159359622,  38.36510808579902),\n                    new Vector2D( 90.26621342936448,  38.36507942500333),\n                    new Vector2D( 90.26652190211962,  38.36494042196722),\n                    new Vector2D( 90.26621240678867,  38.365113172030874),\n                    new Vector2D( 90.26614057102057,  38.365141832826794),\n                    new Vector2D( 90.26380080055299,  38.3660381760273),\n                    new Vector2D( 90.26315345241,  38.36670658276421),\n                    new Vector2D( 90.26251574942881,  38.367490323488084),\n                    new Vector2D( 90.26247873448426,  38.36755266444749),\n                    new Vector2D( 90.26234628016698,  38.36787989125406),\n                    new Vector2D( 90.26214559424784,  38.36945909356126),\n                    new Vector2D( 90.25861728442555,  38.37200753430875),\n                    new Vector2D( 90.23905557537864,  38.375405314295904),\n                    new Vector2D( 90.22517251874075,  38.38984691662256),\n                    new Vector2D( 90.22549955153215,  38.3911564273979),\n                    new Vector2D( 90.22434386063355,  38.391476432092134),\n                    new Vector2D( 90.22147729457276,  38.39134652252034),\n                    new Vector2D( 90.22142070120117,  38.391349167741964),\n                    new Vector2D( 90.20665060751588,  38.39475580900313),\n                    new Vector2D( 90.20042268367109,  38.39842558622888),\n                    new Vector2D( 90.17423771242085,  38.402727751805344),\n                    new Vector2D( 90.16756796257476,  38.40913898597597),\n                    new Vector2D( 90.16728283954308,  38.411255399912875),\n                    new Vector2D( 90.16703538220418,  38.41136059866693),\n                    new Vector2D( 90.16725865657685,  38.41013618805954),\n                    new Vector2D( 90.16746107640665,  38.40902614307544),\n                    new Vector2D( 90.16122795307462,  38.39773101873203)\n            }\n        };\n        PolygonsSet set2 = buildSet(vertices2);\n        PolygonsSet set  = (PolygonsSet) new\nRegionFactory<Euclidean2D>().difference(set1.copySelf(),\n\n              set2.copySelf());\n\n        Vector2D[][] verticies = set.getVertices();\n        Assert.assertTrue(verticies[0][0] != null);\n        Assert.assertEquals(1, verticies.length);\n    }\n{code}'}"
commons-math,bugs-dot-jar_MATH-891_2b852d79,"{'BugID': 'MATH-891', 'Summary': 'SpearmansCorrelation fails when using NaturalRanking together with NaNStrategy.REMOVED', 'Description': 'As reported by Martin Rosellen on the users mailinglist:\n\nUsing a NaturalRanking with a REMOVED NaNStrategy can result in an exception when NaN are contained in the input arrays.\n\nThe current implementation just removes the NaN values where they occur, without taken care to remove the corresponding values in the other array.'}"
commons-math,bugs-dot-jar_MATH-899_ce126bdb,"{'BugID': 'MATH-899', 'Summary': 'A random crash of MersenneTwister random generator', 'Description': 'There is a very small probability that MersenneTwister generator gives a following error: \njava.lang.ArrayIndexOutOfBoundsException: 624\nin MersenneTwister.java line 253\nThe error is completely random and its probability is about 1e-8.\n\nUPD: The problem most probably arises only in multy-thread mode.'}"
commons-math,bugs-dot-jar_MATH-904_6844aba9,"{'BugID': 'MATH-904', 'Summary': 'FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53 ', 'Description': 'As reported by Jeff Hain:\n\npow(double,double):\nMath.pow(-1.0,5.000000000000001E15) = -1.0\nFastMath.pow(-1.0,5.000000000000001E15) = 1.0\n===> This is due to considering that power is an even\ninteger if it is >= 2^52, while you need to test\nthat it is >= 2^53 for it.\n===> replace\n""if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)""\nwith\n""if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)""\nand that solves it.'}"
commons-math,bugs-dot-jar_MATH-924_2836a6f9,"{'BugID': 'MATH-924', 'Summary': 'new multivariate vector optimizers cannot be used with large number of weights', 'Description': 'When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.\n\nThis happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.'}"
commons-math,bugs-dot-jar_MATH-924_b07ecae3,"{'BugID': 'MATH-924', 'Summary': 'new multivariate vector optimizers cannot be used with large number of weights', 'Description': 'When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large.\n\nThis happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.'}"
commons-math,bugs-dot-jar_MATH-927_185e3033,"{'BugID': 'MATH-927', 'Summary': 'GammaDistribution cloning broken', 'Description': ""Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples.\n\nCause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'.\n\nSolution: Make BitStreamGenerator implement Serializable as well.\n\nThis probably affects other distributions as well.""}"
commons-math,bugs-dot-jar_MATH-929_cedf0d27,"{'BugID': 'MATH-929', 'Summary': 'MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd', 'Description': 'To reproduce:\n{code}\nAssert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);\n{code}'}"
commons-math,bugs-dot-jar_MATH-934_724795b5,"{'BugID': 'MATH-934', 'Summary': 'Complex.ZERO.reciprocal() returns NaN but should return INF.', 'Description': 'Complex.ZERO.reciprocal() returns NaN but should return INF.\n\nClass: org.apache.commons.math3.complex.Complex;\nMethod: reciprocal()\n@version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $\n'}"
commons-math,bugs-dot-jar_MATH-935_48dde378,"{'BugID': 'MATH-935', 'Summary': 'DerivativeStructure.atan2(y,x) does not handle special cases properly', 'Description': 'The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.'}"
commons-math,bugs-dot-jar_MATH-938_73605560,"{'BugID': 'MATH-938', 'Summary': 'Line.revert() is imprecise', 'Description': ""Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction.\n\nAlso, is there a reason why Line is not immutable? It is just comprised of two vectors.""}"
commons-math,bugs-dot-jar_MATH-939_49444ee6,"{'BugID': 'MATH-939', 'Summary': 'stat.correlation.Covariance should allow one-column matrices', 'Description': ""Currently (rev 1453206), passing 1-by-M matrix to the Covariance constructor throws IllegalArgumentException. For consistency, the Covariance class should work for a single-column matrix (i.e., for a N-dimensional random variable with N=1) and it should return 1-by-1 covariance matrix with the variable's variance in its only element.""}"
commons-math,bugs-dot-jar_MATH-942_0d057fc6,"{'BugID': 'MATH-942', 'Summary': 'DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type', 'Description': 'Creating an array with {{Array.newInstance(singletons.get(0).getClass(), sampleSize)}} in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:\n* {{singleons.get(0)}} is of type T1, an sub-class of T, and\n* {{DiscreteDistribution.sample()}} returns an object which is of type T, but not of type T1.\n\nTo reproduce:\n{code}\nList<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>();\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(0)));\nlist.add(new Pair<Object, Double>(new Object() {}, new Double(1)));\nnew DiscreteDistribution<Object>(list).sample(1);\n{code}\n\nAttaching a patch.'}"
commons-math,bugs-dot-jar_MATH-949_f83bbc1d,"{'BugID': 'MATH-949', 'Summary': 'LevenbergMarquardtOptimizer reports 0 iterations', 'Description': ""The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount()\n\nI've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.\n\n{noformat}\n    @Test\n    public void testGetIterations() {\n        // setup\n        LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();\n\n        // action\n        otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),\n                new Weight(new double[] { 1 }), new InitialGuess(\n                        new double[] { 3 }), new ModelFunction(\n                        new MultivariateVectorFunction() {\n                            @Override\n                            public double[] value(double[] point)\n                                    throws IllegalArgumentException {\n                                return new double[] { FastMath.pow(point[0], 4) };\n                            }\n                        }), new ModelFunctionJacobian(\n                        new MultivariateMatrixFunction() {\n                            @Override\n                            public double[][] value(double[] point)\n                                    throws IllegalArgumentException {\n                                return new double[][] { { 0.25 * FastMath.pow(\n                                        point[0], 3) } };\n                            }\n                        }));\n\n        // verify\n        assertThat(otim.getEvaluations(), greaterThan(1));\n        assertThat(otim.getIterations(), greaterThan(1));\n    }\n\n{noformat}""}"
commons-math,bugs-dot-jar_MATH-950_424cbd20,"{'BugID': 'MATH-950', 'Summary': 'event state not updated if an unrelated event triggers a RESET_STATE during ODE integration', 'Description': 'When an ODE solver manages several different event types, there are some unwanted side effects.\n\nIf one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return.\n\nAs a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20.\n\nThe attached file reproduces the problem.\n'}"
commons-math,bugs-dot-jar_MATH-957_9aabf587,"{'BugID': 'MATH-957', 'Summary': 'Use analytical function for UniformRealDistribution.inverseCumulativeProbability', 'Description': ""The inverse CDF is currently solved by a root finding function. It would be much simpler (and faster) to use the analytical expression. This would save the user from having to set the inverseCumAccuracy correctly.\n\nI've attached a patch that implements this.""}"
commons-math,bugs-dot-jar_MATH-988_d270055e,"{'BugID': 'MATH-988', 'Summary': 'NPE when calling SubLine.intersection() with non-intersecting lines', 'Description': 'When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations.\n\nThe attached patch fixes both implementations and adds the required test cases.\n\n'}"
commons-math,bugs-dot-jar_MATH-996_86545dab,"{'BugID': 'MATH-996', 'Summary': 'Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception', 'Description': 'An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple\nfraction.  For example:\n\ndouble d = 0.5000000001;\nFraction f = new Fraction(d, 10);\n\nPatch with unit test on way.'}"
flink,bugs-dot-jar_FLINK-1133_02c08456,"{'BugID': 'FLINK-1133', 'Summary': 'Type extractor cannot determine type of function', 'Description': 'This function fails in the type extractor.\n\n{code}\npublic static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> {\n\t\t\n\t@Override\n\tpublic Tuple2<T, T> map(Tuple1<T> vertex) {\n\t\treturn new Tuple2<T, T>(vertex.f0, vertex.f0);\n\t}\n}\n{code}'}"
flink,bugs-dot-jar_FLINK-1133_27e40205,"{'BugID': 'FLINK-1133', 'Summary': 'Type extractor cannot determine type of function', 'Description': 'This function fails in the type extractor.\n\n{code}\npublic static final class DuplicateValue<T> implements MapFunction<Tuple1<T>, Tuple2<T, T>> {\n\t\t\n\t@Override\n\tpublic Tuple2<T, T> map(Tuple1<T> vertex) {\n\t\treturn new Tuple2<T, T>(vertex.f0, vertex.f0);\n\t}\n}\n{code}'}"
flink,bugs-dot-jar_FLINK-1145_22c370d9,"{'BugID': 'FLINK-1145', 'Summary': 'POJO Type extractor bug with type variables', 'Description': 'The following program incorrectly states that there are duplicate getters/setters.\n\n{code}\n\tpublic static class Vertex<K, V> {\n\t\t\n\t\tprivate K key1;\n\t\tprivate K key2;\n\t\tprivate V value;\n\t\t\n\t\tpublic Vertex() {}\n\t\t\n\t\tpublic Vertex(K key, V value) {\n\t\t\tthis.key1 = key;\n\t\t\tthis.key2 = key;\n\t\t\tthis.value = value;\n\t\t}\n\t\t\n\t\tpublic Vertex(K key1, K key2, V value) {\n\t\t\tthis.key1 = key1;\n\t\t\tthis.key2 = key2;\n\t\t\tthis.value = value;\n\t\t}\n\n\t\tpublic void setKey1(K key1) {\n\t\t\tthis.key1 = key1;\n\t\t}\n\t\t\n\t\tpublic void setKey2(K key2) {\n\t\t\tthis.key2 = key2;\n\t\t}\n\t\t\n\t\tpublic K getKey1() {\n\t\t\treturn key1;\n\t\t}\n\t\t\n\t\tpublic K getKey2() {\n\t\t\treturn key2;\n\t\t}\n\t\t\n\t\tpublic void setValue(V value) {\n\t\t\tthis.value = value;\n\t\t}\n\t\t\n\t\tpublic V getValue() {\n\t\t\treturn value;\n\t\t}\n\t}\n\t\n\tpublic static void main(String[] args) throws Exception {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\n\t\tDataSet<Vertex<Long, Double>> set = env.fromElements(new Vertex<Long, Double>(0L, 3.0), new Vertex<Long, Double>(1L, 1.0));\n\t\t\n\t\tset.print();\n\t\t\n\t\tenv.execute();\n\t}\n{code}\n\nThe exception is\n{code}\nException in thread ""main"" java.lang.IllegalStateException: Detected more than one getters\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.isValidPojoField(TypeExtractor.java:981)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.analyzePojo(TypeExtractor.java:1025)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:937)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForClass(TypeExtractor.java:863)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.privateGetForObject(TypeExtractor.java:1146)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.getForObject(TypeExtractor.java:1116)\n\tat org.apache.flink.api.java.ExecutionEnvironment.fromElements(ExecutionEnvironment.java:466)\n\tat test.Test.main(Test.java:74)\n\n{code}'}"
flink,bugs-dot-jar_FLINK-1167_259f10c0,"{'BugID': 'FLINK-1167', 'Summary': 'CompilerException caused by NullPointerException', 'Description': 'Run into it during working on my code. Seems not caused by my plan, or anyway the compiler should have a NullPointer isssue:\n\norg.apache.flink.compiler.CompilerException: An error occurred while translating the optimized plan to a nephele JobGraph: Error translating node \'Union ""Union"" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]\': null\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:543)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:95)\n\tat org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:170)\n\tat org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196)\n\tat org.apache.flink.compiler.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:196)\n\tat org.apache.flink.compiler.plan.OptimizedPlan.accept(OptimizedPlan.java:165)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.compileJobGraph(NepheleJobGraphGenerator.java:163)\n\tat org.apache.flink.client.program.Client.getJobGraph(Client.java:218)\n\tat org.apache.flink.client.program.Client.run(Client.java:290)\n\tat org.apache.flink.client.program.Client.run(Client.java:285)\n\tat org.apache.flink.client.program.Client.run(Client.java:230)\n\tat org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:347)\n\tat org.apache.flink.client.CliFrontend.run(CliFrontend.java:334)\n\tat org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:1001)\n\tat org.apache.flink.client.CliFrontend.main(CliFrontend.java:1025)\nCaused by: org.apache.flink.compiler.CompilerException: Error translating node \'Union ""Union"" : UNION [[ GlobalProperties [partitioning=HASH_PARTITIONED, on fields [0]] ]] [[ LocalProperties [ordering=null, grouped=null, unique=null] ]]\': null\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:338)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:95)\n\tat org.apache.flink.compiler.plan.DualInputPlanNode.accept(DualInputPlanNode.java:162)\n\tat org.apache.flink.compiler.plan.WorksetIterationPlanNode.acceptForStepFunction(WorksetIterationPlanNode.java:196)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.postVisit(NepheleJobGraphGenerator.java:398)\n\t... 14 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.flink.runtime.operators.util.TaskConfig.setDriver(TaskConfig.java:307)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.createDualInputVertex(NepheleJobGraphGenerator.java:793)\n\tat org.apache.flink.compiler.plantranslate.NepheleJobGraphGenerator.preVisit(NepheleJobGraphGenerator.java:286)\n\t... 18 more\n'}"
flink,bugs-dot-jar_FLINK-1214_6ecd0f82,"{'BugID': 'FLINK-1214', 'Summary': 'Prevent partitioning pushdown unless partitions fields match exactly', 'Description': 'Consider an operation grouped on fields (A, B), followed by an operation grouped on field (A).\n\nRight now, the optimizer can push down the partitioning on (A), which serves both operations (the first step locally still groups by A and B). This may however by a bad idea for the cases where the field A has a low cardinality, or the value distribution is skewed.\n\nSince we cannot determine that robustly yet, I suggest to disable this optimization for now.'}"
flink,bugs-dot-jar_FLINK-1290_45fb6d82,"{'BugID': 'FLINK-1290', 'Summary': 'Optimizer prunes all candidates when unable to reuse sort properties', 'Description': 'Programs fail with an exception that no plan could be created.\nThe bug can be reproduced by the following code:\n\n{code}\nval data : DataSet[(Long, Long)] = ...\n\ndata.distinct(0, 1).groupBy(0).reduceGroup(...)\n{code}'}"
flink,bugs-dot-jar_FLINK-1311_94c8e3fa,"{'BugID': 'FLINK-1311', 'Summary': 'Auxiliary nodes in iterations are not correctly identified as ""dynamic"" or ""static""', 'Description': 'The static/dynamic path tagger starts on the original roots of the step functions, ignoring possible auxiliary nodes that we need to attach to the root (such as NoOps, when the root is a union)'}"
flink,bugs-dot-jar_FLINK-1333_63ef8e86,"{'BugID': 'FLINK-1333', 'Summary': 'Getter/Setter recognition for POJO fields with generics is not working', 'Description': 'Fields like\n{code}\nprivate List<Contributors> contributors;\n{code}\n\nAre not recognized correctly, even if they have getters and setters.\nWorkaround: make them public.'}"
flink,bugs-dot-jar_FLINK-1382_9cd96df7,"{'BugID': 'FLINK-1382', 'Summary': 'Void is not added to TypeInfoParser', 'Description': 'List l = Arrays.asList(new Tuple2<Void,Long>(null, 1L));\nTypeInformation t = TypeInfoParser.parse(""Tuple2<Void,Long>"");\nDataSet<Tuple2<Void,Long>> data = env.fromCollection(l, t);\ndata.print();\nThrows:\nException in thread ""main"" java.lang.IllegalArgumentException: String could not be parsed: Class \'Void\' could not be found for use as custom object. Please note that inner classes must be declared static.\nat org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:90)\nat org.apache.flink.hadoopcompatibility.mapreduce.example.ParquetOutput.main(ParquetOutput.java:92)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)\nCaused by: java.lang.IllegalArgumentException: Class \'Void\' could not be found for use as custom object. Please note that inner classes must be declared static.\nat org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:290)\nat org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:133)\nat org.apache.flink.api.java.typeutils.TypeInfoParser.parse(TypeInfoParser.java:88)\n... 6 more'}"
flink,bugs-dot-jar_FLINK-1437_fb7ce0e3,"{'BugID': 'FLINK-1437', 'Summary': ""Bug in PojoSerializer's copy() method"", 'Description': ""The PojoSerializer's {{copy()}} method does not work properly with {{null}} values. An exception could look like:\n\n{code}\nCaused by: java.io.IOException: Thread 'SortMerger spilling thread' terminated due to an exception: null\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:792)\nCaused by: java.io.EOFException\n\tat org.apache.flink.runtime.io.disk.RandomAccessInputView.nextSegment(RandomAccessInputView.java:83)\n\tat org.apache.flink.runtime.memorymanager.AbstractPagedInputView.advance(AbstractPagedInputView.java:159)\n\tat org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readByte(AbstractPagedInputView.java:270)\n\tat org.apache.flink.runtime.memorymanager.AbstractPagedInputView.readUnsignedByte(AbstractPagedInputView.java:277)\n\tat org.apache.flink.types.StringValue.copyString(StringValue.java:839)\n\tat org.apache.flink.api.common.typeutils.base.StringSerializer.copy(StringSerializer.java:83)\n\tat org.apache.flink.api.java.typeutils.runtime.PojoSerializer.copy(PojoSerializer.java:261)\n\tat org.apache.flink.runtime.operators.sort.NormalizedKeySorter.writeToOutput(NormalizedKeySorter.java:449)\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$SpillingThread.go(UnilateralSortMerger.java:1303)\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:788)\n{code}\n\nI'm working on a fix for that...""}"
flink,bugs-dot-jar_FLINK-1458_91f9bfc7,"{'BugID': 'FLINK-1458', 'Summary': 'Interfaces and abstract classes are not valid types', 'Description': 'I don\'t know whether this is by design or is a bug, but I am having trouble working with DataSet and traits in scala which is a major limitation.  A simple example is shown below.  \n\nCompile time warning is \'Type Main.SimpleTrait has no fields that are visible from Scala Type analysis. Falling back to Java Type Analysis...\'\n\nRun time error is \'Interfaces and abstract classes are not valid types: interface Main$SimpleTrait\'\n\nRegards, John\n\n\n val env = ExecutionEnvironment.getExecutionEnvironment\n\n  trait SimpleTrait {\n    def contains(x: String): Boolean\n  }\n\n  class SimpleClass extends SimpleTrait {\n    def contains(x: String) = true\n  }\n\n  val data: DataSet[Double] = env.fromElements(1.0, 2.0, 3.0, 4.0)\n\n  def f(data: DataSet[Double]): DataSet[SimpleTrait] = {\n\n    data.mapPartition(iterator => {\n      Iterator(new SimpleClass)\n    })\n  }\n\n\n  val g = f(data)\n  g.print()\n\n\n  env.execute(""Simple example"")'}"
flink,bugs-dot-jar_FLINK-1471_d033fa8f,"{'BugID': 'FLINK-1471', 'Summary': 'Allow KeySelectors to implement ResultTypeQueryable', 'Description': 'See https://github.com/apache/flink/pull/354'}"
flink,bugs-dot-jar_FLINK-1496_0a4c7694,"{'BugID': 'FLINK-1496', 'Summary': 'Events at unitialized input channels are lost', 'Description': 'If a program sends an event backwards to the producer task, it might happen that some of it input channels have not been initialized yet (UnknownInputChannel). In that case, the events are lost and will never be received at the producer.'}"
flink,bugs-dot-jar_FLINK-1531_21f47d9c,"{'BugID': 'FLINK-1531', 'Summary': 'Custom Kryo Serializer fails in itertation scenario', 'Description': 'When using iterations with a custom serializer for a domain object, the iteration will fail.\n\n{code:java}\norg.apache.flink.runtime.client.JobExecutionException: com.esotericsoftware.kryo.KryoException: Buffer underflow\n\tat org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.require(NoFetchingInput.java:76)\n\tat com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:355)\n\tat com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:109)\n\tat com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752)\n\tat org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:198)\n\tat org.apache.flink.api.java.typeutils.runtime.KryoSerializer.deserialize(KryoSerializer.java:203)\n\tat org.apache.flink.runtime.io.disk.InputViewIterator.next(InputViewIterator.java:43)\n\tat org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamOutFinalOutputBulk(IterationHeadPactTask.java:404)\n\tat org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:377)\n\tat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360)\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:204)\n\tat java.lang.Thread.run(Thread.java:745)\n{code}'}"
flink,bugs-dot-jar_FLINK-1640_8f321c72,"{'BugID': 'FLINK-1640', 'Summary': ""FileOutputFormat writes to wrong path if path ends with '/'"", 'Description': ""The FileOutputFormat duplicates the last directory of a path, if the path ends  with a slash '/'.\nFor example, if the output path is specified as {{/home/myuser/outputPath/}} the output is written to {{/home/myuser/outputPath/outputPath/}}.\n\nThis bug was introduced by commit 8fc04e4da8a36866e10564205c3f900894f4f6e0""}"
flink,bugs-dot-jar_FLINK-1686_1f726e48,"{'BugID': 'FLINK-1686', 'Summary': 'Streaming iteration heads cannot be instantiated', 'Description': 'It looks that streaming jobs with iterations and dop > 1 do not work currently. From what I see, when the TaskManager tries to instantiate a new RuntimeEnvironment for the iteration head tasks it fails since the following exception is being thrown:\n\njava.lang.Exception: Failed to deploy the task Map (2/8) - execution #0 to slot SimpleSlot (0)(1) - 0e39fcabcab3e8543cc2d8320f9de783 - ALLOCATED/ALIVE: java.lang.Exception: Error setting up runtime environment: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied.\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:174)\n\tat org.apache.flink.runtime.taskmanager.TaskManager.org$apache$flink$runtime$taskmanager$TaskManager$$submitTask(TaskManager.scala:432)\n.....\n.....\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied.\n\tat org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:64)\n\tat org.apache.flink.streaming.api.streamvertex.StreamVertex.registerInputOutput(StreamVertex.java:86)\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.<init>(RuntimeEnvironment.java:171)\n\t... 20 more\nCaused by: java.lang.RuntimeException: Could not register the given element, broker slot is already occupied.\n\tat org.apache.flink.runtime.iterative.concurrent.Broker.handIn(Broker.java:39)\n\tat org.apache.flink.streaming.api.streamvertex.StreamIterationHead.setInputsOutputs(StreamIterationHead.java:62)\n\nThe IterateTest passed since it is using a dop of 1 but for higher parallelism it fails. Also, the IterateExample fails as well if you try to run it. \n\nI will debug this once I find some time so any ideas of what could possible cause this are more than welcome. '}"
flink,bugs-dot-jar_FLINK-1705_5308ac83,"{'BugID': 'FLINK-1705', 'Summary': 'InstanceConnectionInfo returns wrong hostname when no DNS entry exists', 'Description': 'If there is no DNS entry for an address (like 10.4.122.43), then the {{InstanceConnectionInfo}} returns the first octet ({{10}}) as the hostame.\n'}"
flink,bugs-dot-jar_FLINK-1761_380ef878,"{'BugID': 'FLINK-1761', 'Summary': 'IndexOutOfBoundsException when receiving empty buffer at remote channel', 'Description': 'Receiving buffers from remote input channels with size 0 results in an {{IndexOutOfBoundsException}}.\n\n{code}\nCaused by: java.lang.IndexOutOfBoundsException: index: 30 (expected: range(0, 30))\n\tat io.netty.buffer.AbstractByteBuf.checkIndex(AbstractByteBuf.java:1123)\n\tat io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:156)\n\tat io.netty.buffer.PooledUnsafeDirectByteBuf.getBytes(PooledUnsafeDirectByteBuf.java:151)\n\tat io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:179)\n\tat io.netty.buffer.AbstractByteBuf.readBytes(AbstractByteBuf.java:717)\n\tat org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeBufferOrEvent(PartitionRequestClientHandler.java:205)\n\tat org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:164)\n\tat org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:118)\n{code}'}"
flink,bugs-dot-jar_FLINK-1820_39d526e6,"{'BugID': 'FLINK-1820', 'Summary': 'Bug in DoubleParser and FloatParser - empty String is not casted to 0', 'Description': 'Hi,\n\nI found the bug, when I wanted to read a csv file, which had a line like:\n""||\\n""\n\nIf I treat it as a Tuple2<Long,Long>, I get as expected a tuple (0L,0L).\n\nBut if I want to read it into a Double-Tuple or a Float-Tuple, I get the following error:\n\njava.lang.AssertionError: Test failed due to a org.apache.flink.api.common.io.ParseException: Line could not be parsed: \'||\'\nParserError NUMERIC_VALUE_FORMAT_ERROR \n\nThis error can be solved by adding an additional condition for empty strings in the FloatParser / DoubleParser.\n\nWe definitely need the CSVReader to be able to read ""empty values"".\n\nI can fix it like described if there are no better ideas :)\n'}"
flink,bugs-dot-jar_FLINK-1848_7164b2b6,"{'BugID': 'FLINK-1848', 'Summary': 'Paths containing a Windows drive letter cannot be used in FileOutputFormats', 'Description': 'Paths that contain a Windows drive letter such as {{file:///c:/my/directory}} cannot be used as output path for {{FileOutputFormat}}.\n\nIf done, the following exception is thrown:\n\n{code}\nCaused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:c:\n        at org.apache.flink.core.fs.Path.initialize(Path.java:242)\n        at org.apache.flink.core.fs.Path.<init>(Path.java:225)\n        at org.apache.flink.core.fs.Path.<init>(Path.java:138)\n        at org.apache.flink.core.fs.local.LocalFileSystem.pathToFile(LocalFileSystem.java:147)\n        at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:232)\n        at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)\n        at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)\n        at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)\n        at org.apache.flink.core.fs.local.LocalFileSystem.mkdirs(LocalFileSystem.java:233)\n        at org.apache.flink.core.fs.FileSystem.initOutPathLocalFS(FileSystem.java:603)\n        at org.apache.flink.api.common.io.FileOutputFormat.open(FileOutputFormat.java:233)\n        at org.apache.flink.api.java.io.CsvOutputFormat.open(CsvOutputFormat.java:158)\n        at org.apache.flink.runtime.operators.DataSinkTask.invoke(DataSinkTask.java:183)\n        at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)\n        at java.lang.Thread.run(Unknown Source)\nCaused by: java.net.URISyntaxException: Relative path in absolute URI: file:c:\n        at java.net.URI.checkPath(Unknown Source)\n        at java.net.URI.<init>(Unknown Source)\n        at org.apache.flink.core.fs.Path.initialize(Path.java:240)\n        ... 14 more\n{code}'}"
flink,bugs-dot-jar_FLINK-1922_ccd574a4,"{'BugID': 'FLINK-1922', 'Summary': 'Failed task deployment causes NPE on input split assignment', 'Description': 'The input split assignment code is returning {null} if the Task has failed, which is causing a NPE.\n\nWe should improve our error handling / reporting in that situation.\n\n{code}\n13:12:31,002 INFO  org.apache.flink.yarn.ApplicationMaster$$anonfun$2$$anon$1    - Status of job c0b47ce41e9a85a628a628a3977705ef (Flink Java Job at Tue Apr 21 13:10:36 UTC 2015) changed to FAILING Cannot deploy task - TaskManager not responding..\n....\n13:12:47,591 ERROR org.apache.flink.runtime.operators.RegularPactTask            - Error in task code:  CHAIN DataSource (at userMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(UserClass.java:111)) (20/50)\njava.lang.RuntimeException: Requesting the next InputSplit failed.\n\tat org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88)\n\tat org.apache.flink.runtime.operators.DataSourceTask$1.hasNext(DataSourceTask.java:337)\n\tat org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136)\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.NullPointerException\n\tat java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n\tat org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301)\n\tat org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83)\n\t... 4 more\n13:12:47,595 INFO  org.apache.flink.runtime.taskmanager.Task                     - CHAIN DataSource (at SomeMethod (org.apache.flink.api.java.io.AvroInputFormat)) -> FlatMap (FlatMap at main(SomeClass.java:111)) (20/50) switched to FAILED : java.lang.RuntimeException: Requesting the next InputSplit failed.\n\tat org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:88)\n\tat org.apache.flink.runtime.operators.DataSourceTask$1.hasNext(DataSourceTask.java:337)\n\tat org.apache.flink.runtime.operators.DataSourceTask.invoke(DataSourceTask.java:136)\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)\n\tat java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.NullPointerException\n\tat java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)\n\tat org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:301)\n\tat org.apache.flink.runtime.taskmanager.TaskInputSplitProvider.getNextInputSplit(TaskInputSplitProvider.java:83)\n\t... 4 more\n{code}'}"
flink,bugs-dot-jar_FLINK-1930_4dbf030a,"{'BugID': 'FLINK-1930', 'Summary': 'NullPointerException in vertex-centric iteration', 'Description': 'Hello to my Squirrels,\n\nI came across this exception when having a vertex-centric iteration output followed by a group by. \nI\'m not sure if what is causing it, since I saw this error in a rather large pipeline, but I managed to reproduce it with [this code example | https://github.com/vasia/flink/commit/1b7bbca1a6130fbcfe98b4b9b43967eb4c61f309] and a sufficiently large dataset, e.g. [this one | http://snap.stanford.edu/data/com-DBLP.html] (I\'m running this locally).\nIt seems like a null Buffer in RecordWriter.\n\nThe exception message is the following:\n\nException in thread ""main"" org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\nat org.apache.flink.runtime.jobmanager.JobManager$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:319)\nat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\nat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\nat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\nat org.apache.flink.runtime.ActorLogMessages$anon$1.apply(ActorLogMessages.scala:37)\nat org.apache.flink.runtime.ActorLogMessages$anon$1.apply(ActorLogMessages.scala:30)\nat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\nat org.apache.flink.runtime.ActorLogMessages$anon$1.applyOrElse(ActorLogMessages.scala:30)\nat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\nat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:94)\nat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\nat akka.actor.ActorCell.invoke(ActorCell.scala:487)\nat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\nat akka.dispatch.Mailbox.run(Mailbox.scala:221)\nat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\nat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\nat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\nat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\nat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: java.lang.NullPointerException\nat org.apache.flink.runtime.io.network.api.serialization.SpanningRecordSerializer.setNextBuffer(SpanningRecordSerializer.java:93)\nat org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:92)\nat org.apache.flink.runtime.operators.shipping.OutputCollector.collect(OutputCollector.java:65)\nat org.apache.flink.runtime.iterative.task.IterationHeadPactTask.streamSolutionSetToFinalOutput(IterationHeadPactTask.java:405)\nat org.apache.flink.runtime.iterative.task.IterationHeadPactTask.run(IterationHeadPactTask.java:365)\nat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:360)\nat org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:221)\nat java.lang.Thread.run(Thread.java:745)'}"
flink,bugs-dot-jar_FLINK-1951_adb321d6,"{'BugID': 'FLINK-1951', 'Summary': 'NullPointerException in DeltaIteration when no ForwardedFileds', 'Description': 'The following exception is thrown by the Connected Components example, if the @ForwardedFieldsFirst(""*"") annotation from the ComponentIdFilter join is removed:\n\nCaused by: java.lang.NullPointerException\n\tat org.apache.flink.examples.java.graph.ConnectedComponents$ComponentIdFilter.join(ConnectedComponents.java:186)\n\tat org.apache.flink.examples.java.graph.ConnectedComponents$ComponentIdFilter.join(ConnectedComponents.java:1)\n\tat org.apache.flink.runtime.operators.JoinWithSolutionSetSecondDriver.run(JoinWithSolutionSetSecondDriver.java:198)\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496)\n\tat org.apache.flink.runtime.iterative.task.AbstractIterativePactTask.run(AbstractIterativePactTask.java:139)\n\tat org.apache.flink.runtime.iterative.task.IterationIntermediatePactTask.run(IterationIntermediatePactTask.java:92)\n\tat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362)\n\tat org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:217)\n\tat java.lang.Thread.run(Thread.java:745)\n\n[Code | https://github.com/vasia/flink/tree/cc-test] and [dataset | http://snap.stanford.edu/data/com-DBLP.html] to reproduce.'}"
flink,bugs-dot-jar_FLINK-1978_0078c44e,"{'BugID': 'FLINK-1978', 'Summary': 'POJO serialization NPE', 'Description': ""NullPointer on serialization of a Date field:\n\nCaused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread 'SortMerger Reading Thread' terminated due to an exception: null\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607)\n\tat org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1132)\n\tat org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98)\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:464)\n\t... 3 more\nCaused by: java.io.IOException: Thread 'SortMerger Reading Thread' terminated due to an exception: null\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:784)\nCaused by: java.lang.NullPointerException\n\tat org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:72)\n\tat org.apache.flink.api.common.typeutils.base.DateSerializer.deserialize(DateSerializer.java:1)\n\tat org.apache.flink.api.java.typeutils.runtime.PojoSerializer.deserialize(PojoSerializer.java:487)\n\tat org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:136)\n\tat org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30)\n\tat org.apache.flink.runtime.plugable.ReusingDeserializationDelegate.read(ReusingDeserializationDelegate.java:57)\n\tat org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:111)\n\tat org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:64)\n\tat org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34)\n\tat org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59)\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ReadingThread.go(UnilateralSortMerger.java:958)\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:781)""}"
flink,bugs-dot-jar_FLINK-1985_495a5c3c,"{'BugID': 'FLINK-1985', 'Summary': 'Streaming does not correctly forward ExecutionConfig to runtime', 'Description': 'When running streaming jobs you see this log entry:\n""Environment did not contain an ExecutionConfig - using a default config.""\n\nSome parts of the code use an ExecutionConfig at runtime. This will be a default config without registered serializers and other user settings.'}"
flink,bugs-dot-jar_FLINK-2074_6bc6dbec,"{'BugID': 'FLINK-2074', 'Summary': 'Sliding Window Keeps Emitting Elements After Source Stops Producing', 'Description': 'This happens when the source produces some elements, then the source stops for a while and then produces again some elements before stopping again. After this, the window will just keep emitting the last emitted element indefinitely.'}"
flink,bugs-dot-jar_FLINK-2082_0cfa43d7,"{'BugID': 'FLINK-2082', 'Summary': 'Chained stream tasks share the same RuntimeContext', 'Description': 'Chained stream operators currently share the same runtimecontext, this will certainly lead to problems in the future. \n\nWe should create separate runtime contexts for each operator in the chain.'}"
flink,bugs-dot-jar_FLINK-2109_d594d024,"{'BugID': 'FLINK-2109', 'Summary': 'CancelTaskException leads to FAILED task state', 'Description': 'The {{CancelTaskException}} is thrown to trigger canceling of the executing task. It is intended to cause a cancelled status, rather than a failed status.\n\nCurrently, it leads to a {{FAILED}} state instead of the expected {{CANCELED}} state.'}"
flink,bugs-dot-jar_FLINK-2121_03340919,"{'BugID': 'FLINK-2121', 'Summary': 'FileInputFormat.addFilesInDir miscalculates total size', 'Description': ""In FileInputFormat.addFilesInDir, the length variable should start from 0, because the return value is always used by adding it to the length (instead of just assigning). So with the current version, the length before the call will be seen twice in the result.\n\nmvn verify caught this for me now. The reason why this hasn't been seen yet, is because testGetStatisticsMultipleNestedFiles catches this only if it gets the listings of the outer directory in a certain order. Concretely, if the inner directory is seen before the other file in the outer directory, then length is 0 at that point, so the bug doesn't show. But if the other file is seen first, then its size is added twice to the total result.""}"
flink,bugs-dot-jar_FLINK-2294_fef9f115,"{'BugID': 'FLINK-2294', 'Summary': 'Keyed State does not work with DOP=1', 'Description': 'When changing the DOP from 3 to 1 in StatefulOperatorTest.apiTest() the test fails. The reason seems to be that the element is not properly set when chaining is happening.\n\nAlso, requiring this:\n{code}\nheadContext.setNextInput(nextRecord);\nstreamOperator.processElement(nextRecord);\n{code}\n\nto be called seems rather fragile. Why not set the element in {{processElement()}}. This would also make for cleaner encapsulation, since now all outside code must assume that operators have a {{StreamingRuntimeContext}} on which they set the next element.\n\nThe state/keyed state machinery seems dangerously undertested.'}"
flink,bugs-dot-jar_FLINK-2412_a56aad74,"{'BugID': 'FLINK-2412', 'Summary': 'Race leading to IndexOutOfBoundsException when querying for buffer while releasing SpillablePartition', 'Description': 'When running a code as simple as: \n\n{noformat}\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Edge<String, NullValue>> edges = getEdgesDataSet(env);\n\t\tGraph<String, NullValue, NullValue> graph = Graph.fromDataSet(edges, env);\n\n\t\tDataSet<Tuple2<String, Long>> degrees = graph.getDegrees();\ndegrees.writeAsCsv(outputPath, ""\\n"", "" "");\n\t\t\tenv.execute();\n\non the Freindster data set: https://snap.stanford.edu/data/com-Friendster.html; on 30 Wally nodes\n \nI get the following exception:\njava.lang.Exception: The data preparation for task \'CoGroup (CoGroup at inDegrees(Graph.java:701))\' , caused an error: Error obtaining the sorted input: Thread \'SortMerger Reading Thread\' terminated due to an exception: Fatal error at remote task manager \'wally028.cit.tu-berlin.de/130.149.249.38:53730\'.\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:471)\n\tat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: java.lang.RuntimeException: Error obtaining the sorted input: Thread \'SortMerger Reading Thread\' terminated due to an exception: Fatal error at remote task manager \'wally028.cit.tu-berlin.de/130.149.249.38:53730\'.\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger.getIterator(UnilateralSortMerger.java:607)\n\tat org.apache.flink.runtime.operators.RegularPactTask.getInput(RegularPactTask.java:1145)\n\tat org.apache.flink.runtime.operators.CoGroupDriver.prepare(CoGroupDriver.java:98)\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:466)\n\t... 3 more\nCaused by: java.io.IOException: Thread \'SortMerger Reading Thread\' terminated due to an exception: Fatal error at remote task manager \'wally028.cit.tu-berlin.de/130.149.249.38:53730\'.\n\tat org.apache.flink.runtime.operators.sort.UnilateralSortMerger$ThreadBase.run(UnilateralSortMerger.java:784)\nCaused by: org.apache.flink.runtime.io.network.netty.exception.RemoteTransportException: Fatal error at remote task manager \'wally028.cit.tu-berlin.de/130.149.249.38:53730\'.\n\tat org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.decodeMsg(PartitionRequestClientHandler.java:227)\n\tat org.apache.flink.runtime.io.network.netty.PartitionRequestClientHandler.channelRead(PartitionRequestClientHandler.java:162)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:242)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:339)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:324)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:847)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: java.io.IOException: Index: 133, Size: 0\n\n{noformat}\n\nCode works fine for the twitter data set, for instance, which is bigger in size, but contains less vertices.  \n\n'}"
flink,bugs-dot-jar_FLINK-2437_a41bc8cc,"{'BugID': 'FLINK-2437', 'Summary': 'TypeExtractor.analyzePojo has some problems around the default constructor detection', 'Description': 'If a class does have a default constructor, but the user forgot to make it public, then TypeExtractor.analyzePojo still thinks everything is OK, so it creates a PojoTypeInfo. Then PojoSerializer.createInstance blows up.\n\nFurthermore, a ""return null"" seems to be missing from the then case of the if after catching the NoSuchMethodException which would also cause a headache for PojoSerializer.\n\nAn additional minor issue is that the word ""class"" is printed twice in several places, because class.toString also prepends it to the class name.\n\n'}"
flink,bugs-dot-jar_FLINK-2442_30761572,"{'BugID': 'FLINK-2442', 'Summary': 'PojoType fields not supported by field position keys ', 'Description': 'Tuple fields which are Pojos (or any other non-tuple composite type) cannot be selected as keys by field position keys.\n\nSomething like \n\n{code}\nDataSet<Tuple2<Integer, MyPojo>> data = ...\ndata.groupBy(1).reduce(...)\n{code}\n\nfails with an exception.'}"
flink,bugs-dot-jar_FLINK-2447_5546a1ef,"{'BugID': 'FLINK-2447', 'Summary': 'TypeExtractor returns wrong type info when a Tuple has two fields of the same POJO type', 'Description': 'Consider the following code:\n\nDataSet<FooBarPojo> d1 = env.fromElements(new FooBarPojo());\n\t\tDataSet<Tuple2<FooBarPojo, FooBarPojo>> d2 = d1.map(new MapFunction<FooBarPojo, Tuple2<FooBarPojo, FooBarPojo>>() {\n\t\t\t@Override\n\t\t\tpublic Tuple2<FooBarPojo, FooBarPojo> map(FooBarPojo value) throws Exception {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t});\n\nwhere FooBarPojo is the following type:\npublic class FooBarPojo {\n\tpublic int foo, bar;\n\tpublic FooBarPojo() {}\n}\n\nThis should print a tuple type with two identical fields:\nJava Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>>\n\nBut it prints the following instead:\nJava Tuple2<PojoType<FooBarPojo, fields = [bar: Integer, foo: Integer]>, GenericType<FooBarPojo>>\n\nNote, that this problem causes some co-groups in Gelly to crash with ""org.apache.flink.api.common.InvalidProgramException: The pair of co-group keys are not compatible with each other"" when the vertex ID type is a POJO, because the second field of the Edge type gets to be a generic type, but the POJO gets recognized in the Vertex type, and getNumberOfKeyFields returns different numbers for the POJO and the generic type.\n\nThe source of the problem is the mechanism in TypeExtractor that would detect recursive types (see the ""alreadySeen"" field in TypeExtractor), as it mistakes the second appearance of FooBarPojo with a recursive field.\n\nSpecifically the following happens: createTypeInfoWithTypeHierarchy starts to process the Tuple2<FooBarPojo, FooBarPojo> type, and in line 434 it calls itself for the first field, which proceeds into the privateGetForClass case which correctly detects that it is a POJO, and correctly returns a PojoTypeInfo; but in the meantime in line 1191, privateGetForClass adds PojoTypeInfo to ""alreadySeen"". Then the outer createTypeInfoWithTypeHierarchy approaches the second field, goes into privateGetForClass, which mistakenly returns a GenericTypeInfo, as it thinks in line 1187, that a recursive type is being processed.\n\n(Note, that if we comment out the recursive type detection (the lines that do their thing with the alreadySeen field), then the output is correct.)'}"
flink,bugs-dot-jar_FLINK-2460_a17d4e82,"{'BugID': 'FLINK-2460', 'Summary': 'ReduceOnNeighborsWithExceptionITCase failure', 'Description': ""I noticed a build error due to failure on this case. It was on a branch of my fork, which didn't actually have anything to do with the failed test or the runtime system at all.\n\nHere's the error log: https://s3.amazonaws.com/archive.travis-ci.org/jobs/73695554/log.txt""}"
flink,bugs-dot-jar_FLINK-2484_d738430c,"{'BugID': 'FLINK-2484', 'Summary': 'BarrierBuffer does not properly clean up temp files', 'Description': None}"
flink,bugs-dot-jar_FLINK-2515_06e2da35,"{'BugID': 'FLINK-2515', 'Summary': 'CheckpointCoordinator triggers checkpoints even if not all sources are running any more', 'Description': 'When some sources finish early, they will not emit checkpoint barriers any more. That means that pending checkpoint alignments will never be able to complete, locking the flow.'}"
flink,bugs-dot-jar_FLINK-2567_948b6e05,"{'BugID': 'FLINK-2567', 'Summary': 'CsvParser: Quotes cannot be escaped inside quoted fields', 'Description': 'We should allow users to escape the quote character inside a quoted field.\n\nQuoting could be realized through the \\ character like in: {{""This is an \\""escaped\\"" quotation.""}}\n\nMailing list thread: http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/jira-Created-FLINK-2567-CsvParser-Quotes-cannot-be-escaped-inside-quoted-fields-td7654.html'}"
flink,bugs-dot-jar_FLINK-2658_ce68cbd9,"{'BugID': 'FLINK-2658', 'Summary': 'fieldsGrouping for multiple output streams fails', 'Description': 'If a Spout or Bolt declares multiple output streams and another Bolt connects to one of those streams via ""fieldsGrouping"",  the call to {{FlinkTopologyBuilder.createTopology()}} fails with the following exception:\n\n{noformat}\norg.apache.flink.api.common.InvalidProgramException: Specifying keys via field positions is only valid for tuple data types. Type: PojoType<org.apache.flink.stormcompatibility.util.SplitStreamType, fields = [streamId: String, value: GenericType<java.lang.Object>]>\n\tat org.apache.flink.api.java.operators.Keys$ExpressionKeys.<init>(Keys.java:209)\n\tat org.apache.flink.api.java.operators.Keys$ExpressionKeys.<init>(Keys.java:203)\n\tat org.apache.flink.streaming.api.datastream.DataStream.groupBy(DataStream.java:285)\n\tat org.apache.flink.stormcompatibility.api.FlinkTopologyBuilder.createTopology(FlinkTopologyBuilder.java:200)\n\tat org.apache.flink.stormcompatibility.api.FlinkTopologyBuilderTest.testFieldsGroupingOnMultipleBoltOutputStreams(FlinkTopologyBuilderTest.java:73)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\n{noformat}\n\nFix: either introduce a mapper, that ""flattens"" the {{SplitStreamType}} in to regular tuple type that is nested inside or provide a custom {{KeySelector}}.'}"
flink,bugs-dot-jar_FLINK-2707_3e233a38,"{'BugID': 'FLINK-2707', 'Summary': 'Set state checkpointer before default state for PartitionedStreamOperatorState', 'Description': 'Currently the default state is set before the passed StateCheckpointer instance for operator states.\n\nWhat currently happens because of this is that the default value is serialized with Java serialization and then deserialized on the opstate.value() call using the StateCheckpointer most likely causing a failure.\n\nThis can be trivially fixed by swaping the order of the 2 calls.'}"
flink,bugs-dot-jar_FLINK-2713_63d9800e,"{'BugID': 'FLINK-2713', 'Summary': 'Custom StateCheckpointers should be included in the snapshots', 'Description': 'Currently the restoreInitialState call fails when the user uses a custom StateCheckpointer to create the snapshot, because the state is restored before the StateCheckpointer is set for the StreamOperatorState. (because the restoreInitialState() call precedes the open() call)\n\nTo avoid this issue, the custom StateCheckpointer instance should be stored within the snapshot and should be set in the StreamOperatorState before calling restoreState(..).\n\nTo reduce the overhead induced by this we can do 2 optimizations:\n - We only include custom StateCheckpointers (the default java serializer one is always available)\n - We only serialize the checkpointer once and store the byte array in the snapshot'}"
flink,bugs-dot-jar_FLINK-2734_8b40bb7a,"{'BugID': 'FLINK-2734', 'Summary': 'ArrayKeySelector returns wrong positions (or fails)', 'Description': 'The {{ArrayKeySelector}} is broken and returns wrong values in all cases except for [0] as a single only key position.'}"
flink,bugs-dot-jar_FLINK-2754_68912126,"{'BugID': 'FLINK-2754', 'Summary': 'FixedLengthRecordSorter can not write to output cross MemorySegments.', 'Description': ""FixedLengthRecordSorter can not write to output cross MemorySegments, it works well as it's only called to write a single record before. Should fix it and add more unit test.""}"
flink,bugs-dot-jar_FLINK-2763_af477563,"{'BugID': 'FLINK-2763', 'Summary': 'Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers.', 'Description': 'The following exception is thrown when running the example triangle listing with an unmodified master build (4cadc3d6).\n\n{noformat}\n./bin/flink run ~/flink-examples/flink-java-examples/target/flink-java-examples-0.10-SNAPSHOT-EnumTrianglesOpt.jar ~/rmat/undirected/s19_e8.ssv output\n{noformat}\n\nThe only changes to {{flink-conf.yaml}} are {{taskmanager.numberOfTaskSlots: 8}} and {{parallelism.default: 8}}.\n\nI have confirmed with input files [s19_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxR2lnMHR4amdyTnM/view?usp=sharing] (40 MB) and [s20_e8.ssv|https://drive.google.com/file/d/0B6TrSsnHj2HxNi1HbmptU29MTm8/view?usp=sharing] (83 MB). On a second machine only the larger file caused the exception.\n\n{noformat}\norg.apache.flink.client.program.ProgramInvocationException: The program execution failed: Job execution failed.\n\tat org.apache.flink.client.program.Client.runBlocking(Client.java:407)\n\tat org.apache.flink.client.program.Client.runBlocking(Client.java:386)\n\tat org.apache.flink.client.program.Client.runBlocking(Client.java:353)\n\tat org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:64)\n\tat org.apache.flink.examples.java.graph.EnumTrianglesOpt.main(EnumTrianglesOpt.java:125)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:434)\n\tat org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:350)\n\tat org.apache.flink.client.program.Client.runBlocking(Client.java:290)\n\tat org.apache.flink.client.CliFrontend.executeProgramBlocking(CliFrontend.java:675)\n\tat org.apache.flink.client.CliFrontend.run(CliFrontend.java:324)\n\tat org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:977)\n\tat org.apache.flink.client.CliFrontend.main(CliFrontend.java:1027)\nCaused by: org.apache.flink.runtime.client.JobExecutionException: Job execution failed.\n\tat org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:425)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:36)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:107)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:487)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:221)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: java.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers.\n\tat org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:288)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.insertBucketEntry(MutableHashTable.java:934)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:859)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.buildTableFromSpilledPartition(MutableHashTable.java:819)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.prepareNextPartition(MutableHashTable.java:517)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.nextRecord(MutableHashTable.java:556)\n\tat org.apache.flink.runtime.operators.hash.NonReusingBuildFirstHashMatchIterator.callWithNextKey(NonReusingBuildFirstHashMatchIterator.java:104)\n\tat org.apache.flink.runtime.operators.JoinDriver.run(JoinDriver.java:208)\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489)\n\tat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:579)\n\tat java.lang.Thread.run(Thread.java:745)\n{noformat}'}"
flink,bugs-dot-jar_FLINK-2800_b654e989,"{'BugID': 'FLINK-2800', 'Summary': 'kryo serialization problem', 'Description': 'Performing a cross of two dataset of POJOs I have got the exception below. The first time I run the process, there was no problem. When I run it the second time, I have got the exception. My guess is that it could be a race condition related to the reuse of the Kryo serializer object. However, it could also be ""a bug where type registrations are not properly forwarded to all Serializers"", as suggested by Stephan.\n\n------------------------------------------------------------------------\n2015-10-01 18:18:21 INFO  JobClient:161 - 10/01/2015 18:18:21\tCross(Cross at main(FlinkMongoHadoop2LinkPOI2CDA.java:160))(3/4) switched to FAILED \ncom.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 114\n\tat com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:119)\n\tat com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:641)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:752)\n\tat org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:210)\n\tat org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:127)\n\tat org.apache.flink.api.java.typeutils.runtime.TupleSerializer.deserialize(TupleSerializer.java:30)\n\tat org.apache.flink.runtime.operators.resettable.AbstractBlockResettableIterator.getNextRecord(AbstractBlockResettableIterator.java:180)\n\tat org.apache.flink.runtime.operators.resettable.BlockResettableMutableObjectIterator.next(BlockResettableMutableObjectIterator.java:111)\n\tat org.apache.flink.runtime.operators.CrossDriver.runBlockedOuterSecond(CrossDriver.java:309)\n\tat org.apache.flink.runtime.operators.CrossDriver.run(CrossDriver.java:162)\n\tat org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:489)\n\tat org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:354)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:581)\n\tat java.lang.Thread.run(Thread.java:745)'}"
flink,bugs-dot-jar_FLINK-2802_88a97768,"{'BugID': 'FLINK-2802', 'Summary': 'Watermark triggered operators cannot progress with cyclic flows', 'Description': 'The problem is that we can easily create a cyclic watermark (time) dependency in the stream graph which will result in a deadlock for watermark triggered operators such as  the `WindowOperator`.\n\nA solution to this could be to emit a Long.MAX_VALUE watermark from the iteration sources.'}"
flink,bugs-dot-jar_FLINK-2812_e494c279,"{'BugID': 'FLINK-2812', 'Summary': 'KeySelectorUtil.getSelectorForKeys and TypeExtractor.getKeySelectorTypes are incompatible', 'Description': 'The following code snippet fails, because {{KeySelectorUtil.getSelectorForKeys}} returns the base {{Tuple}} type.\n\n```java\nTypeInformation<Tuple2<Integer, Integer>> typeInfo = TypeExtractor\n.getForObject(Tuple2.of(0, 0));\n\nExecutionConfig config = new ExecutionConfig();\n\nKeySelector<Tuple2<Integer, Integer>, ?> keySelector = KeySelectorUtil.getSelectorForKeys(\nnew Keys.ExpressionKeys<>(new int[]{0}, typeInfo), typeInfo, config);\n\n// fails with InvalidTypesException\nTypeExtractor.getKeySelectorTypes(keySelector, typeInfo); \n```\n\nHowever if I manually define the key selector as follows the snippet works fine due to the key type being an integer.\n\n```java\nKeySelector<Tuple2<Integer, Integer>, Integer> keySelector =\n\nnew KeySelector<Tuple2<Integer, Integer>, Integer>() {\n\t@Override\n\tpublic Integer getKey(Tuple2<Integer, Integer> value) throws Exception {\n\t\treturn value.f0;\n\t}\n};\n```\n\nThe error message looks like this:\norg.apache.flink.api.common.functions.InvalidTypesException: Usage of class Tuple as a type is not allowed. Use a concrete subclass (e.g. Tuple1, Tuple2, etc.) instead.\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.createTypeInfoWithTypeHierarchy(TypeExtractor.java:401)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.privateCreateTypeInfo(TypeExtractor.java:379)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.getUnaryOperatorReturnType(TypeExtractor.java:279)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:229)\n\tat org.apache.flink.api.java.typeutils.TypeExtractor.getKeySelectorTypes(TypeExtractor.java:223)\n'}"
flink,bugs-dot-jar_FLINK-2817_5dfc897b,"{'BugID': 'FLINK-2817', 'Summary': 'FileMonitoring function throws NPE when location is empty', 'Description': '{{StreamExecutionEnvironment.readFileStream()}} does not handle a missing location properly. I would suggest to log that the location is empty and continue running the job.\n\nA test covering the correct behavior is also needed.'}"
flink,bugs-dot-jar_FLINK-2874_17e7b423,"{'BugID': 'FLINK-2874', 'Summary': 'Certain Avro generated getters/setters not recognized', 'Description': 'For Avro schemas where value null is not allowed, the field is unboxed e.g. int but the getter/setter methods provide the boxed Integer as interface:\n\n{code}\n{\n ""fields"": [\n  {\n   ""type"": ""double"", \n   ""name"": ""time""\n  }, \n}\n{code}\n\nThis results in Java\n\n{code}\n  private double time;\n\n  public java.lang.Double getTime() {\n    return time;\n  }\n\n  public void setTime(java.lang.Double value) {\n    this.time = value;\n  }\n{code}\n\nThere is also a problem when there is an underscore in the Avro schema, e.g.:\n\n{code}\n  {\n   ""default"": null, \n   ""type"": [\n    ""null"", \n    ""long""\n   ], \n   ""name"": ""conn_id""\n  }, \n{code}\n\nThis results in Java:\n\n{code}\nprivate java.lang.Long conn_id;\n\n  public java.lang.Long getConnId() {\n    return conn_id;\n  }\n\n  public void setConnId(java.lang.Long value) {\n    this.conn_id = value;\n  }\n{code}'}"
flink,bugs-dot-jar_FLINK-2964_76bebd42,"{'BugID': 'FLINK-2964', 'Summary': 'MutableHashTable fails when spilling partitions without overflow segments', 'Description': 'When one performs a join operation with many and large records then the join operation fails with the following exception when it tries to spill a {{HashPartition}}.\n\n{code}\njava.lang.RuntimeException: Bug in Hybrid Hash Join: Request to spill a partition with less than two buffers.\n\tat org.apache.flink.runtime.operators.hash.HashPartition.spillPartition(HashPartition.java:302)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.spillPartition(MutableHashTable.java:1108)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.nextSegment(MutableHashTable.java:1277)\n\tat org.apache.flink.runtime.operators.hash.HashPartition$BuildSideBuffer.nextSegment(HashPartition.java:524)\n\tat org.apache.flink.runtime.memory.AbstractPagedOutputView.advance(AbstractPagedOutputView.java:140)\n\tat org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:201)\n\tat org.apache.flink.runtime.memory.AbstractPagedOutputView.write(AbstractPagedOutputView.java:178)\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:74)\n\tat org.apache.flink.api.common.typeutils.base.array.BytePrimitiveArraySerializer.serialize(BytePrimitiveArraySerializer.java:30)\n\tat org.apache.flink.runtime.operators.hash.HashPartition.insertIntoBuildBuffer(HashPartition.java:257)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.insertIntoTable(MutableHashTable.java:856)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.buildInitialTable(MutableHashTable.java:685)\n\tat org.apache.flink.runtime.operators.hash.MutableHashTable.open(MutableHashTable.java:443)\n\tat org.apache.flink.runtime.operators.hash.HashTableTest.testSpillingWhenBuildingTableWithoutOverflow(HashTableTest.java:234)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:160)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:78)\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\n{code}\n\nThe reason is that the {{HashPartition}} does not include the number of used memory segments by the {{BuildSideBuffer}} when it counts the currently occupied memory segments.'}"
flink,bugs-dot-jar_FLINK-2968_59685903,"{'BugID': 'FLINK-2968', 'Summary': 'Windowed fold operation fails because the initial value was not serialized', 'Description': 'The windowed fold operation currently fails because the initial value was not serialized. The reason for this is that the fold operation is realized as a {{WindowFunction}} within an {{AbstractUdfStreamOperator}} and does not get the output type information forwarded (which is necessary for the serialization). \n\nThe solution is to let the {{AbstractUdfStreamOperator}} forward the output type information to the {{WindowFunction}} if it implements the {{OutputTypeConfigurable}} interface.'}"
flink,bugs-dot-jar_FLINK-3011_5a86a0a1,"{'BugID': 'FLINK-3011', 'Summary': 'Cannot cancel failing/restarting streaming job from the command line', 'Description': 'I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:\n\nThe exception I get:\n13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING.\n13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5.\n13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].'}"
flink,bugs-dot-jar_FLINK-3011_a402002d,"{'BugID': 'FLINK-3011', 'Summary': 'Cannot cancel failing/restarting streaming job from the command line', 'Description': 'I cannot seem to be able to cancel a failing/restarting job from the command line client. The job cannot be rescheduled so it keeps failing:\n\nThe exception I get:\n13:58:11,240 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job 0c895d22c632de5dfe16c42a9ba818d5 (player-id) changed to RESTARTING.\n13:58:25,234 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Trying to cancel job with ID 0c895d22c632de5dfe16c42a9ba818d5.\n13:58:25,561 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@127.0.0.1:42012] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].'}"
flink,bugs-dot-jar_FLINK-3052_8dc70f2e,"{'BugID': 'FLINK-3052', 'Summary': 'Optimizer does not push properties out of bulk iterations', 'Description': ""Flink's optimizer should be able to reuse interesting properties from outside the loop. In order to do that it is sometimes necessary to append a NoOp node to the step function which recomputes the required properties.\n\nThis is currently not working for {{BulkIterations}}, because the plans with the appended NoOp nodes are not added to the overall list of candidates.\n\nThis not only leads to sub-optimal plan selection but sometimes to the rejection of valid jobs. The following job, for example, will be falsely rejected by flink.\n\n{code}\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tDataSet<Tuple1<Long>> input1 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() {\n\t\t\t@Override\n\t\t\tpublic Tuple1<Long> map(Long value) throws Exception {\n\t\t\t\treturn new Tuple1<>(value);\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple1<Long>> input2 = env.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Long>>() {\n\t\t\t@Override\n\t\t\tpublic Tuple1<Long> map(Long value) throws Exception {\n\t\t\t\treturn new Tuple1<>(value);\n\t\t\t}\n\t\t});\n\n\t\tDataSet<Tuple1<Long>> distinctInput = input1.distinct();\n\n\t\tIterativeDataSet<Tuple1<Long>> iteration = distinctInput.iterate(10);\n\n\t\tDataSet<Tuple1<Long>> iterationStep = iteration\n\t\t\t\t.coGroup(input2)\n\t\t\t\t.where(0)\n\t\t\t\t.equalTo(0)\n\t\t\t\t.with(new CoGroupFunction<Tuple1<Long>, Tuple1<Long>, Tuple1<Long>>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void coGroup(\n\t\t\t\t\t\t\tIterable<Tuple1<Long>> first,\n\t\t\t\t\t\t\tIterable<Tuple1<Long>> second,\n\t\t\t\t\t\t\tCollector<Tuple1<Long>> out) throws Exception {\n\t\t\t\t\t\tIterator<Tuple1<Long>> it = first.iterator();\n\n\t\t\t\t\t\tif (it.hasNext()) {\n\t\t\t\t\t\t\tout.collect(it.next());\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\tDataSet<Tuple1<Long>> iterationResult = iteration.closeWith(iterationStep);\n\n\t\titerationResult.output(new DiscardingOutputFormat<Tuple1<Long>>());\n{code}""}"
flink,bugs-dot-jar_FLINK-3107_937963e3,"{'BugID': 'FLINK-3107', 'Summary': 'ZooKeeperCheckpointIDCounter.start() can block JobManager actor', 'Description': 'In HA mode, the job manager enables checkpoints during submission of streaming programs.\n\nThis leads to call to ZooKeeperCheckpointIDCounter.start(), which communicates with ZooKeeper. This can block the job manager actor.\n\nA solution is to start the counter later instead of the CheckpointCoordinator constructor.'}"
flink,bugs-dot-jar_FLINK-3189_a5b05566,"{'BugID': 'FLINK-3189', 'Summary': 'Error while parsing job arguments passed by CLI', 'Description': 'Flink CLI treats job arguments provided in format ""-<char>"" as its own parameters, which results in errors in execution.\n\nExample 1:\ncall: >bin/flink info myJarFile.jar -f flink -i <filepath> -m 1\nerror: Unrecognized option: -f\n\nExample 2:\nJob myJarFile.jar is uploaded to web submission client, flink parameter box is empty\nprogram arguments box: -f flink -i <filepath> -m 1\nerror: \nAn unexpected error occurred:\nUnrecognized option: -f\norg.apache.flink.client.cli.CliArgsException: Unrecognized option: -f\n\tat org.apache.flink.client.cli.CliFrontendParser.parseInfoCommand(CliFrontendParser.java:296)\n\tat org.apache.flink.client.CliFrontend.info(CliFrontend.java:376)\n\tat org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:983)\n\tat org.apache.flink.client.web.JobSubmissionServlet.doGet(JobSubmissionServlet.java:171)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:734)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:847)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:532)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:227)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:965)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:388)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:187)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:901)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n\tat org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:113)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:348)\n\tat org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)\n\tat org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:549)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:211)\n\tat org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:425)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:489)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:436)\n\tat java.lang.Thread.run(Thread.java:745)\n\nExecution of \n>bin/flink run myJarFile.jar -f flink -i <filepath> -m 1  \nworks perfectly fine'}"
flink,bugs-dot-jar_FLINK-3251_117ba95f,"{'BugID': 'FLINK-3251', 'Summary': 'Checkpoint stats show ghost numbers', 'Description': '[~StephanEwen] reported an issue with the display of checkpoint stats. A pipeline with a stateful source and stateless intermediate operator shows stats for the stateless intermediate operator. The numbers are most likely the same as for the source operator.'}"
flink,bugs-dot-jar_FLINK-3256_44061882,"{'BugID': 'FLINK-3256', 'Summary': 'Invalid execution graph cleanup for jobs with colocation groups', 'Description': 'Currently, upon restarting an execution graph, we clean-up the colocation constraints for each group present in an ExecutionJobVertex respectively.\n\nThis can lead to invalid reconfiguration upon a restart or any other activity that relies on state cleanup of the execution graph. For example, upon restarting a DataStream job with iterations the following steps are executed:\n\n1) IterationSource colgroup constraints are reset\n2) IterationSource execution vertices reset and create new colocation constraints\n3) IterationSink colgroup constraints are reset\n4) IterationSink execution vertices reset and create different colocation constraints.\n\nThis can be trivially fixed by reseting colocation groups independently from ExecutionJobVertices, thus, updating them once per reconfiguration.'}"
flink,bugs-dot-jar_FLINK-3260_6968a57a,"{'BugID': 'FLINK-3260', 'Summary': 'ExecutionGraph gets stuck in state FAILING', 'Description': 'It is a bit of a rare case, but the following can currently happen:\n\n  1. Jobs runs for a while, some tasks are already finished.\n  2. Job fails, goes to state failing and restarting. Non-finished tasks fail or are canceled.\n  3. For the finished tasks, ask-futures from certain messages (for example for releasing intermediate result partitions) can fail (timeout) and cause the execution to go from FINISHED to FAILED\n  4. This triggers the execution graph to go to FAILING without ever going further into RESTARTING again\n  5. The job is stuck\n\nIt initially looks like this is mainly an issue for batch jobs (jobs where tasks do finish, rather than run infinitely).\n\nThe log that shows how this manifests:\n{code}\n--------------------------------------------------------------------------------\n17:19:19,782 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started\n17:19:19,844 INFO  Remoting                                                      - Starting remoting\n17:19:20,065 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:56722]\n17:19:20,090 INFO  org.apache.flink.runtime.blob.BlobServer                      - Created BLOB server storage directory /tmp/blobStore-6766f51a-1c51-4a03-acfb-08c2c29c11f0\n17:19:20,096 INFO  org.apache.flink.runtime.blob.BlobServer                      - Started BLOB server at 0.0.0.0:43327 - max concurrent requests: 50 - max backlog: 1000\n17:19:20,113 INFO  org.apache.flink.runtime.jobmanager.MemoryArchivist           - Started memory archivist akka://flink/user/archive\n17:19:20,115 INFO  org.apache.flink.runtime.checkpoint.SavepointStoreFactory     - No savepoint state backend configured. Using job manager savepoint state backend.\n17:19:20,118 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Starting JobManager at akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:19:20,123 INFO  org.apache.flink.runtime.jobmanager.JobManager                - JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager was granted leadership with leader session ID None.\n17:19:25,605 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43702/user/taskmanager) as f213232054587f296a12140d56f63ed1. Current number of registered hosts is 1. Current number of alive task slots is 2.\n17:19:26,758 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:43956/user/taskmanager) as f9e78baa14fb38c69517fb1bcf4f419c. Current number of registered hosts is 2. Current number of alive task slots is 4.\n17:19:27,064 INFO  org.apache.flink.api.java.ExecutionEnvironment                - The job has 0 registered types and 0 default Kryo serializers\n17:19:27,071 INFO  org.apache.flink.client.program.Client                        - Starting client actor system\n17:19:27,072 INFO  org.apache.flink.runtime.client.JobClient                     - Starting JobClient actor system\n17:19:27,110 INFO  akka.event.slf4j.Slf4jLogger                                  - Slf4jLogger started\n17:19:27,121 INFO  Remoting                                                      - Starting remoting\n17:19:27,143 INFO  org.apache.flink.runtime.client.JobClient                     - Started JobClient actor system at 127.0.0.1:51198\n17:19:27,145 INFO  Remoting                                                      - Remoting started; listening on addresses :[akka.tcp://flink@127.0.0.1:51198]\n17:19:27,325 INFO  org.apache.flink.runtime.client.JobClientActor                - Disconnect from JobManager null.\n17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Received job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d).\n17:19:27,362 INFO  org.apache.flink.runtime.client.JobClientActor                - Could not submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d), because there is no connection to a JobManager.\n17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connect to JobManager Actor[akka.tcp://flink@127.0.0.1:56722/user/jobmanager#-1489998809].\n17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Connected to new JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:19:27,379 INFO  org.apache.flink.runtime.client.JobClientActor                - Sending message to JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager to submit job Flink Java Job at Mon Jan 18 17:19:27 UTC 2016 (fa05fd25993a8742da09cc5023c1e38d) and wait for progress\n17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Upload jar files to job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:19:27,380 INFO  org.apache.flink.runtime.client.JobClientActor                - Submit job to the job manager akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:19:27,453 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Submitting job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016).\n17:19:27,591 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Scheduling job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016).\n17:19:27,592 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from CREATED to SCHEDULED\n17:19:27,596 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from SCHEDULED to DEPLOYING\n17:19:27,597 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:27,606 INFO  org.apache.flink.runtime.client.JobClientActor                - Job was successfully submitted to the JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:19:27,630 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RUNNING.\n17:19:27,637 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from CREATED to SCHEDULED\n17:19:27,654 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tJob execution switched to status RUNNING.\n17:19:27,655 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to SCHEDULED \n17:19:27,656 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to DEPLOYING \n17:19:27,666 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from SCHEDULED to DEPLOYING\n17:19:27,667 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:27,667 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to SCHEDULED \n17:19:27,669 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to DEPLOYING \n17:19:27,681 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from CREATED to SCHEDULED\n17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from SCHEDULED to DEPLOYING\n17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from CREATED to SCHEDULED\n17:19:27,682 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from SCHEDULED to DEPLOYING\n17:19:27,685 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:27,686 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to SCHEDULED \n17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to DEPLOYING \n17:19:27,687 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to SCHEDULED \n17:19:27,692 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to DEPLOYING \n17:19:27,833 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from DEPLOYING to RUNNING\n17:19:27,839 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to RUNNING \n17:19:27,840 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from DEPLOYING to RUNNING\n17:19:27,852 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to RUNNING \n17:19:27,896 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from DEPLOYING to RUNNING\n17:19:27,898 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from DEPLOYING to RUNNING\n17:19:27,901 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to RUNNING \n17:19:27,905 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:27\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to RUNNING \n17:19:28,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from CREATED to SCHEDULED\n17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from CREATED to SCHEDULED\n17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from SCHEDULED to DEPLOYING\n17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:28,126 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from CREATED to SCHEDULED\n17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from SCHEDULED to DEPLOYING\n17:19:28,139 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:28,117 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from CREATED to SCHEDULED\n17:19:28,134 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from SCHEDULED to DEPLOYING\n17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:28,140 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from SCHEDULED to DEPLOYING\n17:19:28,141 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:28,147 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to SCHEDULED \n17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to SCHEDULED \n17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to DEPLOYING \n17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to SCHEDULED \n17:19:28,153 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to DEPLOYING \n17:19:28,156 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to DEPLOYING \n17:19:28,158 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to SCHEDULED \n17:19:28,165 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to DEPLOYING \n17:19:28,238 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (2/4) (e73af91028cb76f7d3cd887cb6d66755) switched from RUNNING to FINISHED\n17:19:28,242 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(2/4) switched to FINISHED \n17:19:28,308 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (3/4) (807daf978da9dc347dca930822c78f8f) switched from RUNNING to FINISHED\n17:19:28,315 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (1/4) (c79bf4381462c690f5999f2d1949ab50) switched from RUNNING to FINISHED\n17:19:28,317 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(3/4) switched to FINISHED \n17:19:28,318 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(1/4) switched to FINISHED \n17:19:28,328 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from DEPLOYING to RUNNING\n17:19:28,336 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to RUNNING \n17:19:28,338 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from DEPLOYING to RUNNING\n17:19:28,341 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to RUNNING \n17:19:28,459 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat)) (4/4) (ba45c37065b67fc8f5005a50d0e88fff) switched from RUNNING to FINISHED\n17:19:28,463 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tDataSource (at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73) (org.apache.flink.api.java.io.ParallelIteratorInputFormat))(4/4) switched to FINISHED \n17:19:28,520 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from DEPLOYING to RUNNING\n17:19:28,529 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to RUNNING \n17:19:28,540 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from DEPLOYING to RUNNING\n17:19:28,545 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:28\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to RUNNING \n17:19:32,384 INFO  org.apache.flink.runtime.instance.InstanceManager             - Registered TaskManager at testing-worker-linux-docker-e6d6931f-3200-linux-4 (akka.tcp://flink@172.17.0.253:60852/user/taskmanager) as 5848d44035a164a0302da6c8701ff748. Current number of registered hosts is 3. Current number of alive task slots is 6.\n17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CREATED to SCHEDULED\n17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from SCHEDULED to DEPLOYING\n17:19:32,598 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Deploying Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (attempt #0) to testing-worker-linux-docker-e6d6931f-3200-linux-4\n17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tReduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to SCHEDULED \n17:19:32,605 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tReduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to DEPLOYING \n17:19:32,611 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (4/4) (c928d19f73d700e80cdfad650689febb) switched from RUNNING to FINISHED\n17:19:32,614 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(4/4) switched to FINISHED \n17:19:32,717 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/4) (6421c8f88b191ea844619a40a523773b) switched from RUNNING to FINISHED\n17:19:32,719 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/4) switched to FINISHED \n17:19:32,724 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from DEPLOYING to RUNNING\n17:19:32,726 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tReduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to RUNNING \n17:19:32,843 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from RUNNING to FINISHED\n17:19:32,845 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:32\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FINISHED \n17:19:33,092 WARN  akka.remote.ReliableDeliverySupervisor                        - Association with remote system [akka.tcp://flink@172.17.0.253:43702] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].\n17:19:39,111 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702\n17:19:39,113 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager terminated.\n17:19:39,114 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (3/4) (7997918330ecf2610b3298a8c8ef2852) switched from RUNNING to FAILED\n17:19:39,120 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(3/4) switched to FAILED \njava.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager\n\tat org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151)\n\tat org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547)\n\tat org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119)\n\tat org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156)\n\tat org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215)\n\tat org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:792)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:44)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46)\n\tat akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369)\n\tat akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:486)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:221)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n17:19:39,129 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from RUNNING to CANCELING\n17:19:39,132 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - DataSink (collect()) (1/1) (895e1ea552281a665ae390c966cdb3b7) switched from CREATED to CANCELED\n17:19:39,149 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tJob execution switched to status FAILING.\njava.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager\n\tat org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151)\n\tat org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547)\n\tat org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119)\n\tat org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156)\n\tat org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215)\n\tat org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:792)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:44)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46)\n\tat akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369)\n\tat akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:486)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:221)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tReduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to CANCELING \n17:19:39,173 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tDataSink (collect())(1/1) switched to CANCELED \n17:19:39,174 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Reduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (1/1) (d0f8f69f9047c3154b860850955de20f) switched from CANCELING to FAILED\n17:19:39,177 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tReduce (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(1/1) switched to FAILED \njava.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager\n\tat org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151)\n\tat org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547)\n\tat org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119)\n\tat org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156)\n\tat org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215)\n\tat org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:792)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:44)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46)\n\tat akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369)\n\tat akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:486)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:221)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n17:19:39,179 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:39\tJob execution switched to status RESTARTING.\n17:19:39,179 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Delaying retry of job execution for 10000 ms ...\n17:19:39,179 INFO  org.apache.flink.runtime.instance.InstanceManager             - Unregistered task manager akka.tcp://flink@172.17.0.253:43702/user/taskmanager. Number of registered task managers 2. Number of available slots 4.\n17:19:39,179 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING.\njava.lang.Exception: The slot in which the task was executed has been released. Probably loss of TaskManager f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager\n\tat org.apache.flink.runtime.instance.SimpleSlot.releaseSlot(SimpleSlot.java:151)\n\tat org.apache.flink.runtime.instance.SlotSharingGroupAssignment.releaseSharedSlot(SlotSharingGroupAssignment.java:547)\n\tat org.apache.flink.runtime.instance.SharedSlot.releaseSlot(SharedSlot.java:119)\n\tat org.apache.flink.runtime.instance.Instance.markDead(Instance.java:156)\n\tat org.apache.flink.runtime.instance.InstanceManager.unregisterTaskManager(InstanceManager.java:215)\n\tat org.apache.flink.runtime.jobmanager.JobManager$$anonfun$handleMessage$1.applyOrElse(JobManager.scala:792)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LeaderSessionMessageFilter$$anonfun$receive$1.applyOrElse(LeaderSessionMessageFilter.scala:44)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)\n\tat scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:33)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.apply(LogMessages.scala:28)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat org.apache.flink.runtime.LogMessages$$anon$1.applyOrElse(LogMessages.scala:28)\n\tat akka.actor.Actor$class.aroundReceive(Actor.scala:465)\n\tat org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:100)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)\n\tat akka.actor.dungeon.DeathWatch$class.receivedTerminated(DeathWatch.scala:46)\n\tat akka.actor.ActorCell.receivedTerminated(ActorCell.scala:369)\n\tat akka.actor.ActorCell.autoReceiveMessage(ActorCell.scala:501)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:486)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:221)\n\tat akka.dispatch.Mailbox.exec(Mailbox.scala:231)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n17:19:39,180 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to RESTARTING.\n17:19:42,766 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - CHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) (2/4) (d0d011dc0a0823bcec5a57a369b334ed) switched from FINISHED to FAILED\n17:19:42,773 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42\tCHAIN Partition -> Map (Map at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73)) -> Combine (Reduce at testTaskManagerFailure(TaskManagerProcessFailureBatchRecoveryITCase.java:73))(2/4) switched to FAILED \njava.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to:\n\tat org.apache.flink.runtime.executiongraph.Execution$5.onFailure(Execution.java:915)\n\tat akka.dispatch.OnFailure.internal(Future.scala:228)\n\tat akka.dispatch.OnFailure.internal(Future.scala:227)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:174)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:171)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:136)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:134)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms]\n\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)\n\tat akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)\n\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n\tat java.lang.Thread.run(Thread.java:745)\n\n17:19:42,774 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Status of job fa05fd25993a8742da09cc5023c1e38d (Flink Java Job at Mon Jan 18 17:19:27 UTC 2016) changed to FAILING.\njava.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to:\n\tat org.apache.flink.runtime.executiongraph.Execution$5.onFailure(Execution.java:915)\n\tat akka.dispatch.OnFailure.internal(Future.scala:228)\n\tat akka.dispatch.OnFailure.internal(Future.scala:227)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:174)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:171)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:136)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:134)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms]\n\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)\n\tat akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)\n\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n\tat java.lang.Thread.run(Thread.java:745)\n17:19:42,780 INFO  org.apache.flink.runtime.client.JobClientActor                - 01/18/2016 17:19:42\tJob execution switched to status FAILING.\njava.lang.IllegalStateException: Update task on instance f213232054587f296a12140d56f63ed1 @ testing-worker-linux-docker-e6d6931f-3200-linux-4 - 2 slots - URL: akka.tcp://flink@172.17.0.253:43702/user/taskmanager failed due to:\n\tat org.apache.flink.runtime.executiongraph.Execution$5.onFailure(Execution.java:915)\n\tat akka.dispatch.OnFailure.internal(Future.scala:228)\n\tat akka.dispatch.OnFailure.internal(Future.scala:227)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:174)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:171)\n\tat scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)\n\tat scala.runtime.AbstractPartialFunction.applyOrElse(AbstractPartialFunction.scala:25)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:136)\n\tat scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:134)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@172.17.0.253:43702/user/taskmanager#-1712955384]] after [10000 ms]\n\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)\n\tat akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)\n\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n\tat java.lang.Thread.run(Thread.java:745)\n17:19:49,152 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702\n17:19:59,172 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702\n17:20:09,191 WARN  Remoting                                                      - Tried to associate with unreachable remote address [akka.tcp://flink@172.17.0.253:43702]. Address is now gated for 5000 ms, all messages to this address will be delivered to dead letters. Reason: Connection refused: /172.17.0.253:43702\n17:24:32,423 INFO  org.apache.flink.runtime.jobmanager.JobManager                - Stopping JobManager akka.tcp://flink@127.0.0.1:56722/user/jobmanager.\n17:24:32,440 ERROR org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase  - \n--------------------------------------------------------------------------------\nTest testTaskManagerProcessFailure[0](org.apache.flink.test.recovery.TaskManagerProcessFailureBatchRecoveryITCase) failed with:\njava.lang.AssertionError: The program did not finish in time\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.junit.Assert.assertFalse(Assert.java:64)\n\tat org.apache.flink.test.recovery.AbstractTaskManagerProcessFailureRecoveryTest.testTaskManagerProcessFailure(AbstractTaskManagerProcessFailureRecoveryTest.java:212)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.junit.runners.Suite.runChild(Suite.java:127)\n\tat org.junit.runners.Suite.runChild(Suite.java:26)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:283)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:173)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:128)\n{code}'}"
flink,bugs-dot-jar_FLINK-3267_ed3810b1,"{'BugID': 'FLINK-3267', 'Summary': 'Disable reference tracking in Kryo fallback serializer', 'Description': ""Kryo runs extra logic to track and resolve repeated references to the same object (similar as JavaSerialization)\n\nWe should disable reference tracking\n  - reference tracking is costly\n  - it is virtually always unnecessary in the datatypes used in Flink\n  - most importantly, it is inconsistent with Flink's own serialization (which does not do reference tracking)\n  - It may have problems if elements are read in a different order than they are written.""}"
flink,bugs-dot-jar_FLINK-3314_8fc7e7af,"{'BugID': 'FLINK-3314', 'Summary': 'Early cancel calls can cause Tasks to not cancel properly', 'Description': 'When a task receives the ""cancel()"" call before the operators are properly instantiated, it can be that the operator never receives a cancel call.\n\nIn certain cases, this causes the operator to hang.\n'}"
flink,bugs-dot-jar_FLINK-3342_8e3e2f8f,"{'BugID': 'FLINK-3342', 'Summary': 'Operator checkpoint statistics state size overflow ', 'Description': 'State sizes ({{long}}) of checkpoint stats overflow when summing them up per operator, because the sum is stored in an {{int}}.'}"
flink,bugs-dot-jar_FLINK-3513_d90672fd,"{'BugID': 'FLINK-3513', 'Summary': 'Fix interplay of automatic Operator UID and Changing name of WindowOperator', 'Description': ""WindowOperator can have a changing name because it has the TypeSerializer .toString() output in it's name. For some type serializers that don't implement toString() this means that the name changes.\n\nThis means that savepoint restore does not work for the automatically generated UID.""}"
flink,bugs-dot-jar_FLINK-3534_734ba01d,"{'BugID': 'FLINK-3534', 'Summary': 'Cancelling a running job can lead to restart instead of stopping', 'Description': 'I just tried cancelling a regularly running job. Instead of the job stopping, it restarted.\n\n\n{code}\n2016-02-29 10:39:28,415 INFO  org.apache.flink.yarn.YarnJobManager                          - Trying to cancel job with ID 5c0604694c8469cfbb89daaa990068df.\n2016-02-29 10:39:28,416 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from RUNNING to CANCELING\n....\n2016-02-29 10:39:28,488 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (19/24) (c1be31b0be596d2521073b2d78ffa60a) switched from CANCELING to CANCELED\n2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (e3b05555ab0e373defb925898de9f200) switched from CANCELING to FAILED\n2016-02-29 10:40:08,468 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (1/24) (5ad172ec9932b24d5a98377a2c82b0b3) switched from CANCELING to FAILED\n2016-02-29 10:40:08,472 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (2/24) (5404ca28ac7cf23b67dff30ef2309078) switched from CANCELING to FAILED\n2016-02-29 10:40:08,473 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to FAILING.\njava.lang.Exception: Task could not be canceled.\n\tat org.apache.flink.runtime.executiongraph.Execution$5.onComplete(Execution.java:902)\n\tat akka.dispatch.OnComplete.internal(Future.scala:246)\n\tat akka.dispatch.OnComplete.internal(Future.scala:244)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:174)\n\tat akka.dispatch.japi$CallbackBridge.apply(Future.scala:171)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)\n\tat scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\nCaused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka.tcp://flink@10.240.242.143:50119/user/taskmanager#640539146]] after [10000 ms]\n\tat akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)\n\tat akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)\n\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)\n\tat akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423)\n\tat akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-02-29 10:40:08,477 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (3/24) (fc527d65ec8df3ccf68f882d968e776e) switched from CANCELING to FAILED\n2016-02-29 10:40:08,487 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - TriggerWindow(TumblingTimeWindows(60000), FoldingStateDescriptor{name=window-contents, defaultValue=(0,9223372036854775807,0), serializer=null}, EventTimeTrigger(), WindowedStream.apply(WindowedStream.java:397)) (4/24) (afb1aa3c2d8acdee0f138cf344238e4e) switched from CANCELING to FAILED\n2016-02-29 10:40:08,488 INFO  org.apache.flink.runtime.executiongraph.restart.FixedDelayRestartStrategy  - Delaying retry of job execution for 3000 ms ...\n2016-02-29 10:40:08,488 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RESTARTING.\n2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to CREATED.\n2016-02-29 10:40:11,490 INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph        - Source: Out of order data generator -> (Flat Map, Timestamps/Watermarks) (1/1) (1319b2f44d78d99948ffde4350c052d9) switched from CREATED to SCHEDULED\n2016-02-29 10:40:11,490 INFO  org.apache.flink.yarn.YarnJobManager                          - Status of job 5c0604694c8469cfbb89daaa990068df (Event counter: {auto.offset.reset=earliest, rocksdb=hdfs:///user/robert/rocksdb, generateInPlace=soTrue, parallelism=24, bootstrap.servers=cdh544-worker-0:9092, topic=eventsGenerator, eventsPerKeyPerGenerator=2, numKeys=1000000000, zookeeper.connect=cdh544-worker-0:2181, timeSliceSize=60000, eventsKerPey=1, genPar=1}) changed to RUNNING.\n{code}\n'}"
flink,bugs-dot-jar_FLINK-3566_434e88fd,"{'BugID': 'FLINK-3566', 'Summary': 'Input type validation often fails on custom TypeInfo implementations', 'Description': 'Input type validation often fails when used with custom type infos. One example of this behaviour can be reproduced by creating a custom type info with our own field type:\n\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\nenv.generateSequence(1, 10).map(new MapFunction<Long, Tuple1<Optional<Long>>>() {\n\t\t\t@Override\n\t\t\tpublic Tuple1<Optional<Long>> map(Long value) throws Exception {\n\t\t\t\treturn Tuple1.of(Optional.of(value));\n\t\t\t}\n\t\t}).returns(new TupleTypeInfo<>(new OptionTypeInfo<Long>(BasicTypeInfo.LONG_TYPE_INFO)))\n\t\t\t\t.keyBy(new KeySelector<Tuple1<Optional<Long>>, Optional<Long>>() {\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Optional<Long> getKey(Tuple1<Optional<Long>> value) throws Exception {\n\t\t\t\t\t\treturn value.f0;\n\t\t\t\t\t}\n\t\t\t\t});\n\nThis will fail on Input type validation at the KeySelector (or any other function for example a mapper) with the following exception:\n\nInput mismatch: Basic type expected.'}"
flink,bugs-dot-jar_FLINK-3684_e3759a5e,"{'BugID': 'FLINK-3684', 'Summary': 'CEP operator does not forward watermarks properly', 'Description': ""The CEP stream operator don't emit a proper watermark when using event time.""}"
flink,bugs-dot-jar_FLINK-3740_f2f5bd5b,"{'BugID': 'FLINK-3740', 'Summary': 'Session Window State is Not Checkpointed', 'Description': 'The merging window state in the {{WindowOperator}} is not checkpointed. This means that programs containing session windows will fail upon restore after a failure.\n\nI propose adding a simulated snapshot/restore cycle to the tests in {{WindowOperatorTest}} to catch these problems in the future.'}"
flink,bugs-dot-jar_FLINK-3760_494212b3,"{'BugID': 'FLINK-3760', 'Summary': 'Fix StateDescriptor.readObject ', 'Description': 'The readObject method of StateDescriptor uses uses {{ObjectInputStream.read()}}. For very large serialized default values this will not necessarily read all data in one go. We need a loop that reads it in several steps.'}"
flink,bugs-dot-jar_FLINK-3762_dc78a747,"{'BugID': 'FLINK-3762', 'Summary': ' Kryo StackOverflowError due to disabled Kryo Reference tracking', 'Description': 'As discussed on the dev list,\n\nIn {{KryoSerializer.java}}\n\nKryo Reference tracking is disabled by default:\n\n{code}\n    kryo.setReferences(false);\n{code}\n\nThis can causes  {{StackOverflowError}} Exceptions when serializing many objects that may contain recursive objects:\n\n{code}\njava.lang.StackOverflowError\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:48)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495)\n\tat com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:523)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:61)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:495)\n{code}\n\nBy enabling reference tracking, we can fix this problem.\n\n[1]https://gist.github.com/andrewpalumbo/40c7422a5187a24cd03d7d81feb2a419\n '}"
flink,bugs-dot-jar_FLINK-996_32a003d5,"{'BugID': 'FLINK-996', 'Summary': 'NullPointerException while translating union node', 'Description': 'The NepheleJobGraphGenerator throws a NullPointerException when translating a binary union operator. The BinaryUnionPlanNode is not replaced by a NAryUnionPlanNode and thus is still treated as a DualInputVertex. Accessing the driver code of the BinaryUnionPlanNode causes then the NullPointerException.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1020_83427028,"{'BugID': 'OAK-1020', 'Summary': 'Property value converion ignores reisdual property definition', 'Description': 'Assume following node type which a property defined with type and a residual unnamed property also defined\n\n{noformat}\n[oak:foo]\n - stringProp (String)\n - * (undefined)\n{noformat}\n\nFor such node type if a property {{stringProp}} is being set with a binary value then Oak converts it into a String property thereby causing binary stream to change. In JR2 conversion would not happen as conversion logic treats setting (stringProp,BINARY) as a residual property'}"
jackrabbit-oak,bugs-dot-jar_OAK-1024_2b5d3afb,"{'BugID': 'OAK-1024', 'Summary': 'Full-text search on the traversing index fails if the condition contains a slash', 'Description': ""A full-text search on the traversing index falls back to a sort of manual evaluation of results. \nThis is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_\n\nI'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.\n\nExample:\n{code}\n//*[jcr:contains(., 'text/plain')]\n{code}\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-1024_e39b4d96,"{'BugID': 'OAK-1024', 'Summary': 'Full-text search on the traversing index fails if the condition contains a slash', 'Description': ""A full-text search on the traversing index falls back to a sort of manual evaluation of results. \nThis is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_\n\nI'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.\n\nExample:\n{code}\n//*[jcr:contains(., 'text/plain')]\n{code}\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-1024_ecc5bdfd,"{'BugID': 'OAK-1024', 'Summary': 'Full-text search on the traversing index fails if the condition contains a slash', 'Description': ""A full-text search on the traversing index falls back to a sort of manual evaluation of results. \nThis is handled by the _FullTextTerm_ class, and it appears that it passes the constraint text through a cleanup process where it strips most of the characters that are neither _Character.isLetterOrDigit(c)_ not in the list _+-:&_\n\nI'm not exactly sure where this list comes from, but I see the '/' character is missing which causes a certain type of query to fail.\n\nExample:\n{code}\n//*[jcr:contains(., 'text/plain')]\n{code}\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-1035_b2ca8baa,"{'BugID': 'OAK-1035', 'Summary': 'Property Index: cost calculation is wrong (zero) when searching for many values', 'Description': 'Currently, for queries of the form \n\n{code}\nselect * from [nt:unstructured] where type = \'xyz\'\n{code}\n\nthe node type index is used in some cases, even if there is an index on the property ""type"". The reason is that the cost for the node type index is 0 due to a bug. The node type index internally uses the property index on the property ""jcr:primaryType"", and asks for the cost using all possible children node types of ""nt:unstructured"". The returned cost is 0 because of this bug. The cost estimation is an extrapolation of the number of entries for the first 3 values. It is currently coded as:\n\n{code}\ncount = count / size / i;\n{code}\n\nwhen in fact it should be written as:\n\n{code}\ncount = count * size / i;\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1054_0adf3a6e,"{'BugID': 'OAK-1054', 'Summary': 'Folder containing an admin user should not be removed', 'Description': 'The action of removing a folder that contains the admin user should fail.\n\nThis is already the case if it is tried to remove the admin node .\n\nAttaching unit test'}"
jackrabbit-oak,bugs-dot-jar_OAK-1075_79467350,"{'BugID': 'OAK-1075', 'Summary': 'XPath query failures for mvps', 'Description': 'Adding some cases related to mvps that are not currently covered by the existing (jackrabbit) tests.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1076_9238264d,"{'BugID': 'OAK-1076', 'Summary': 'XPath failures for typed properties', 'Description': 'It looks like there are some failures in xpath queries that expect a match only on properties of a certain type (which is to be inferred from the query)'}"
jackrabbit-oak,bugs-dot-jar_OAK-1081_4ce4e3c9,"{'BugID': 'OAK-1081', 'Summary': 'Node.getNodes throwing exception if user does not have access to any child node', 'Description': 'When trying to obtain child iterator via Node.getNodes {{InvalidItemStateException}} is thrown if user does not have access to its content\n\n{code:java}\n    @Test\n    public void testGetChildren() throws Exception {\n        deny(path, privilegesFromName(PrivilegeConstants.JCR_ADD_CHILD_NODES));\n        NodeIterator it1 = testSession.getNode(path).getNodes();\n        while(it1.hasNext()){\n            Node n = it1.nextNode();\n            NodeIterator it2 = n.getNodes();\n        }\n    }\n{code}\n\nExecuting above code leads to following exception\n\n{noformat}\njavax.jcr.InvalidItemStateException: Item is stale\n\tat org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getTree(NodeDelegate.java:827)\n\tat org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.getChildren(NodeDelegate.java:336)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl$8.perform(NodeImpl.java:546)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl$8.perform(NodeImpl.java:543)\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125)\n\tat org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:113)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl.getNodes(NodeImpl.java:543)\n\tat org.apache.jackrabbit.oak.jcr.security.authorization.ReadPropertyTest.testGetChildren(ReadPropertyTest.java:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:464)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:157)\n\tat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)\n\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)\n{noformat}\n\nThe exception is thrown for path {{/testroot/node1/rep:policy}}. \n\nThe issue occurs because the {{NodeIterator}} {{it1}} includes {{rep:policy}} and later when its child are accessed security check leads to exception. Probably the {{it1}} should not include {{rep:policy}} as part of child list and filter it out'}"
jackrabbit-oak,bugs-dot-jar_OAK-1093_531aca78,"{'BugID': 'OAK-1093', 'Summary': 'IllegalArgumentException on Row.getValues()', 'Description': 'Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \\[nt:base\\] WHERE \\[sling:resourceType\\]=""cq/personalization/components/contextstores/surferinfo""}}\n\n{quote}\njava.lang.IllegalArgumentException\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95)\n\tat org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266)\n\tat org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99)\n\tat com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128)\n\tat com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91)\n{quote}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1093_d7f0f180,"{'BugID': 'OAK-1093', 'Summary': 'IllegalArgumentException on Row.getValues()', 'Description': 'Calling {{row.getValues()}} is throwing an {{IllegalArgumentException}} when called on the {{QueryResult}} of the query {{SELECT properties FROM \\[nt:base\\] WHERE \\[sling:resourceType\\]=""cq/personalization/components/contextstores/surferinfo""}}\n\n{quote}\njava.lang.IllegalArgumentException\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueImpl.checkSingleValued(ValueImpl.java:85)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueImpl.<init>(ValueImpl.java:72)\n\tat org.apache.jackrabbit.oak.plugins.value.ValueFactoryImpl.createValue(ValueFactoryImpl.java:95)\n\tat org.apache.jackrabbit.oak.jcr.query.QueryResultImpl.createValue(QueryResultImpl.java:266)\n\tat org.apache.jackrabbit.oak.jcr.query.RowImpl.getValues(RowImpl.java:99)\n\tat com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.getListProperty(FrameworkComponentImpl.java:128)\n\tat com.day.cq.analytics.sitecatalyst.impl.FrameworkComponentImpl.<init>(FrameworkComponentImpl.java:91)\n{quote}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1094_2e20589f,"{'BugID': 'OAK-1094', 'Summary': 'CacheLIRS implementation incomplete', 'Description': 'The current CacheLIRS implementation is not complete and e.g. does not provide a write through ConcurrentMap view on {{asMap()}}. For OAK-1088 it would be good to have this implementation as it allows conditional and atomic updates of cache entries.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1096_be44b816,"{'BugID': 'OAK-1096', 'Summary': 'QueryManager does not have autorefresh', 'Description': 'Having two sessions A and B.\nA writes something for example /content/page/text = ""text""\nAccessing B\'s QueryManager and exexcute a query for ""text"" nothing will be found.\nTriggering an explicit refresh on B before the query and the hit is found.\n\nI assume that the autorefresh is missed for that case'}"
jackrabbit-oak,bugs-dot-jar_OAK-1104_7ae92779,"{'BugID': 'OAK-1104', 'Summary': 'SegmentNodeStore rebase operation assumes wrong child node order', 'Description': ""This popped up during the async merge process. The merge first does a rebase which can fail, making some index files look like they disappeared [0], wrapping the actual root cause.\n\nThe problem is that the rebase failed and removed the missing file. This can be seen by analyzing the ':conflict' marker info:\nbq. addExistingNode {_b_Lucene41_0.doc, _b.fdx, _b.fdt, _b_4.del, }\nso it points to something trying to add some index related files twice, almost like a concurrent commit exception.\n\nDigging even deeper I found that the rebase operation during the state comparison phase assumes a certain order of child nodes [1], and based on that tries to read the mentioned nodes again, thinking that they are new ones, when if fact they are already present in the list [2].\nThis causes a conflict which fails the entire async update process, but also any lucene search, as the index files are now gone and the index is in a corrupted state.\n\n\n[0] \n{noformat}\n*WARN* [pool-5-thread-2] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Index update async failed org.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:122)\n\tat org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64)\n\tat org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:64)\n\tat org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:129)\n\tat org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56)\n\tat org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:100)\n\tat org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)\n\tat org.quartz.core.JobRunShell.run(JobRunShell.java:207)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\nCaused by: java.io.FileNotFoundException: _b_Lucene41_0.doc at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.openInput(OakDirectory.java:145)\n{noformat}\n\n[1] http://svn.apache.org/viewvc/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/segment/MapRecord.java?view=markup#l329\n\n[2]\nbefore child list\n{noformat}\n[_b_Lucene41_0.doc, _b.fdx, _b.fdt, segments_34, _b_4.del, _b_Lucene41_0.pos, _b.nvm, _b.nvd, _b.fnm, _3n.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, _3n.cfe, segments.gen, _3n.cfs, _b.si]\n{noformat}\n\nafter list\n{noformat}\n_b_Lucene41_0.pos, _3k.cfs, _3j_1.del, _b.nvm, _b.nvd, _3d.cfe, _3d.cfs, _b.fnm, _3j.si, _3h.si, _3i.cfe, _3i.cfs, _3e_2.del, _3f.si, _b_Lucene41_0.tip, _b_Lucene41_0.tim, segments.gen, _3e.cfe, _3e.cfs, _b.si,_3g.si, _3l.si, _3i_1.del, _3d_3.del, _3e.si, _3d.si, _b_Lucene41_0.doc, _3h_2.del, _3i.si, _3k_1.del, _3j.cfe, _3j.cfs, _b.fdx, _b.fdt, _3g_1.del, _3k.si, _3l.cfe, _3l.cfs, segments_33, _3f_1.del, _3h.cfe, _3h.cfs, _b_4.del, _3f.cfe, _3f.cfs, _3g.cfe, _3g.cfs\n{noformat}\n\n\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-1108_a8c925e0,"{'BugID': 'OAK-1108', 'Summary': 'Query constraints marked as invalid in the case of an mvp', 'Description': ""It seems that in the case of a query that has more constraints on the same property, like\nbq. //*[(@prop = 'aaa' and @prop = 'bbb' and @prop = 'ccc')]\n\nthe filter is marked as invalid (_#isAlwaysFalse_) and the query returns no results.\n\nThis is incorrect and affects queries that search for multi-valued properties on nodes.\n\nThis comes from/affects OAK-1075.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1111_459bd065,"{'BugID': 'OAK-1111', 'Summary': ""Node#setProperty(String, Calendar) doesn't take time zone in account"", 'Description': ""Node#setProperty(String, Calendar) doesn't take time zone in account.\n\nIt looks the Calendar value is straightly stored as a long without take in consideration the time zone,\n\nUnit test to follow""}"
jackrabbit-oak,bugs-dot-jar_OAK-1122_5286861d,"{'BugID': 'OAK-1122', 'Summary': 'Empty branch commit returns head revision on trunk', 'Description': 'MicroKernelImpl returns the head revision on trunk when an empty commit happens on a branch revision.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1129_2f95b81f,"{'BugID': 'OAK-1129', 'Summary': 'Repeated MongoMK.rebase() always adds new revision', 'Description': 'MongoMK always adds a new revision to the branch on rebase, even when the branch is already up-to-date.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1155_f64e8adc,"{'BugID': 'OAK-1155', 'Summary': 'PropertyIndex cost calculation is faulty', 'Description': 'The cost calculation can easily go out of bounds when it needs to estimate (whenever there are more than 100 nodes).\nThe high value it returns can be higher than the traversal index which has a max of 10M, but can be less smaller.\n\n\nFor example:\n  100 nodes in the index:\n  with a single level /content cost is 6250000\n  adding a second level /content/data cost jumps to 1.544804416E9\n\n  101 nodes in the index:\n  with a single level /content cost is 100\n  adding a second level /content/data stays at 100\n\n  100 nodes, 12 levels deep, cost is 2.147483647E9\n  101 nodes, 12 levels deep, cost is 6.7108864E7'}"
jackrabbit-oak,bugs-dot-jar_OAK-1168_c05cec12,"{'BugID': 'OAK-1168', 'Summary': 'Invalid JCR paths not caught', 'Description': ""{{NamePathMapper.getOakPath}} should return {{null}} when called with an invalid JCR path like {{foo:bar]baz}}, but it doesn't. \n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-1173_61c877d8,"{'BugID': 'OAK-1173', 'Summary': 'NPE if checking for a non-existing node in version storage', 'Description': 'NPE If a tree given to CompiledPermissionImpl.getTreePermission() does not have a primary type, e.g. for a ""hidden"" oak node:\n\n{noformat}\n\t  at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)\n\t  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl.getTreePermission(CompiledPermissionImpl.java:160)\n\t  at org.apache.jackrabbit.oak.security.authorization.permission.CompiledPermissionImpl$TreePermissionImpl.getChildPermission(CompiledPermissionImpl.java:443)\n\t  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.getTreePermission(SecureNodeBuilder.java:352)\n\t  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.exists(SecureNodeBuilder.java:129)\n\t  at org.apache.jackrabbit.oak.core.SecureNodeBuilder.hasChildNode(SecureNodeBuilder.java:271)\n\t  at org.apache.jackrabbit.oak.core.AbstractTree.getChildrenCount(AbstractTree.java:248)\n{noformat}\n\nThe tree passed here to get the children count is: {{/jcr:system/jcr:versionStorage}} and the child node not having a primary type is {{:index}}\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1174_342809f7,"{'BugID': 'OAK-1174', 'Summary': 'Inconsistent handling of invalid names/paths', 'Description': 'Passing an invalid name to a JCR method might or might not throw a {{RepositoryException}} depending on whether name re-mappings exist or not:\n\n{code}\nsession.itemExists(""/jcr:cont]ent"");\n{code}\n\nreturns {{false}} if no name re-mappings exist but throws a {{RepositoryException}} otherwise. '}"
jackrabbit-oak,bugs-dot-jar_OAK-1178_84fb6b29,"{'BugID': 'OAK-1178', 'Summary': 'MutableTree#isNew: replace implementation by NodeBuilder#isNew ', 'Description': 'Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.\n\nSee also OAK-947.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1178_f2bb1a17,"{'BugID': 'OAK-1178', 'Summary': 'MutableTree#isNew: replace implementation by NodeBuilder#isNew ', 'Description': 'Similar to the issue described in OAK-1177 we may consider replacing the implementation of MutableTree#isNew by the corresponding call on the NodeBuilder.\n\nSee also OAK-947.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1184_f72dd8d1,"{'BugID': 'OAK-1184', 'Summary': 'Uploading large number of files to single folder fails. ', 'Description': 'Repository: OAK with TarPM \nUpload is successful till 254 files and it started failing afterwards with exception in server logs. \n[1] \n{code}\n14.11.2013 12:36:34.608 *ERROR* [10.40.146.206 [1384412794576] POST /content/dam/cq9032/./Coconut-5mb-110.jpg HTTP/1.1] org.apache.sling.servlets.post.impl.operations.ModifyOperation Exception during response processing.\njava.lang.IllegalStateException: null\n\tat com.google.common.base.Preconditions.checkState(Preconditions.java:133) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeRecordId(SegmentWriter.java:259) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeListBucket(SegmentWriter.java:346) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeList(SegmentWriter.java:508) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeProperty(SegmentWriter.java:669) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:847) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$3.childNodeChanged(SegmentWriter.java:806) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$3.childNodeChanged(SegmentWriter.java:806) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter$3.childNodeChanged(SegmentWriter.java:806) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:387) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentWriter.writeNode(SegmentWriter.java:797) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:53) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentRootBuilder.getNodeState(SegmentRootBuilder.java:21) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.rebase(SegmentNodeStore.java:135) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.merge(SegmentNodeStore.java:113) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.merge(SegmentNodeStoreService.java:174) ~[na:na]\n\tat org.apache.jackrabbit.oak.core.AbstractRoot.commit(AbstractRoot.java:260) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:224) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:219) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:207) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:332) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:399) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:396) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:128) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:117) ~[na:na]\n\tat org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:396) ~[na:na]\n\tat sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source) ~[na:na]\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:1.6.0_26]\n\tat java.lang.reflect.Method.invoke(Unknown Source) ~[na:1.6.0_26]\n\tat org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109) ~[na:na]\n\tat $Proxy9.save(Unknown Source) ~[na:na]\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1186_52372042,"{'BugID': 'OAK-1186', 'Summary': 'Parallel execution of ConcurrentReadAccessControlledTreeTest fails with MongoMK', 'Description': ""The is caused by concurrent creation of test content and the conflict it creates in the index. Every Oak test instance tries to create {{/oak:index/nodetype/:index/nt%3Afile}}, but only one will succeed. AFAICS there are two options how to handle this:\n\n- Implement conflict annotation (OAK-1185), though I'm not sure this will really work. On commit, the rebase happens first, when changes from the other Oak instance may not be visible yet. Then, the commit hook runs and perform another branch commit with the changes, which works fine. Only the last step fails, when MongoMK tries to merge the branch. This is the point when the conflict may be detected.\n\n- Implement a retry logic in MongoMK/NS""}"
jackrabbit-oak,bugs-dot-jar_OAK-1208_cb3ac20d,"{'BugID': 'OAK-1208', 'Summary': 'Lucene Index should ignore property existence checks', 'Description': ""Some optimizations on the query engine transform certain clauses in property existence checks. ie (p = 'somevalue' turns into 'p is not null').\n\nThis doesn't play well with lucene as it can not  effectively build a 'not null' query, even worse the query doesn't return any results.\n\nAs a fix I'll just skip the existence constraints from the generated lucene query.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1215_a9efe3c4,"{'BugID': 'OAK-1215', 'Summary': ""Wildcards in relative property paths don't work in search expressions"", 'Description': ""A search XPath of the form:\n{code}\n/jcr:root/etc/commerce/products//*[@size='M' or */@size='M']\n{code}\nreturns:\n{code}\nInvalid path: *\n{code}\n(This works fine in Jackrabbit.)""}"
jackrabbit-oak,bugs-dot-jar_OAK-1216_e403e003,"{'BugID': 'OAK-1216', 'Summary': 'Path parsing must support SNS indexes, irrespective of SNS support', 'Description': '{code}\nSession.getNode(""/foo/bar[2]"");\n{code}\n\nthrows {{javax.jcr.RepositoryException: Invalid name or path: /foo/bar\\[2]}}\n\nThis should be an ItemNotFoundException (if the item does not exist), irrespective if the repository supports SNS or not.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1225_3535afe2,"{'BugID': 'OAK-1225', 'Summary': 'Session.nodeExists(""/foo/bar[2]"") must not throw PathNotFoundException', 'Description': 'similar to OAK-1216, Session.nodeExists() of an SNS path with indexes > 1 should return false.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1227_117b0a3d,"{'BugID': 'OAK-1227', 'Summary': 'Node.hasNode(""foo[2]"") must not throw PathNotFoundException', 'Description': 'similar to OAK-1225, Node.hasNode(""foo[2]"") should return false'}"
jackrabbit-oak,bugs-dot-jar_OAK-1235_1beb2a50,"{'BugID': 'OAK-1235', 'Summary': 'Upgrade should not overwrite new oak specific builtin nodetypes', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-1244_b4a93c81,"{'BugID': 'OAK-1244', 'Summary': 'Always create new UUID on ImportBehavior.IMPORT_UUID_CREATE_NEW', 'Description': 'The implementation should create a new UUID for each referenceable node even if there is no existing node with that UUID. This spec says:\n\nbq.  Incoming nodes are assigned newly created identifiers upon addition to the workspace. As a result, identifier collisions never occur.\n\nThis will break backward compatibility, but is IMO the correct behavior and the only way to guarantee import of referenceable nodes does not fail in a concurrent import scenario. See OAK-1186 for more details.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1250_0c3b3306,"{'BugID': 'OAK-1250', 'Summary': 'Guard against invalid/missing checkpoints', 'Description': ""Playing with the backup revealed a case where a checkpoint can become invalid after a manual restore of the repository. [0]\nThe NodeStore#retrieve apis already specify that this can return null in the case the checkpoint doesn't exist anymore, but it looks like the storage bits aren't yet prepared for that scenario.\n\n\n\n[0]\n{noformat}\norg.apache.sling.commons.scheduler.impl.QuartzScheduler Exception during job execution of org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate@3a6d47 : Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8\njava.lang.IllegalStateException: Failed to load segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8\n\tat org.apache.jackrabbit.oak.plugins.segment.AbstractStore.readSegment(AbstractStore.java:109) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.Segment.getSegment(Segment.java:189) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.Record.getSegment(Record.java:97) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:56) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getChildNode(SegmentNodeState.java:209) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStore.retrieve(SegmentNodeStore.java:175) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService.retrieve(SegmentNodeStoreService.java:198) ~[na:na]\n\tat org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:97) ~[na:na]\n\tat org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105) ~[org.apache.sling.commons.scheduler-2.4.2.jar:na]\n\tat org.quartz.core.JobRunShell.run(JobRunShell.java:207) [org.apache.sling.commons.scheduler-2.4.2.jar:na]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]\n\tat java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]\nCaused by: java.lang.IllegalStateException: Segment 8a8b281c-1a02-4950-aad5-aad8e436a0d8 not found\n\tat org.apache.jackrabbit.oak.plugins.segment.file.FileStore.loadSegment(FileStore.java:184) ~[na:na]\n{noformat}""}"
jackrabbit-oak,bugs-dot-jar_OAK-1254_25a70439,"{'BugID': 'OAK-1254', 'Summary': 'Parallel execution of SimpleSearchTest fails with MongoMK', 'Description': 'At some point in the benchmark run one MongoMK instance will fail to read a node created by another instance. The exception is very similar to *E1* reported in OAK-1204.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1269_b8fe2ded,"{'BugID': 'OAK-1269', 'Summary': ""NodeType index doesn't respect the declaringNodeTypes setting"", 'Description': ""Following the OAK-1150 discussion, I've noticed that the node type index doesn't respect the declaringNodeTypes setting.\nSetting a restriction on the node type index definition breaks the index - there are 0 query hits.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1270_70564c7c,"{'BugID': 'OAK-1270', 'Summary': 'Revisit full-text queries in case of multiple tokens ', 'Description': ""There's still an issue with tokenizing the search terms when trying for example to search for a fulltext term that will split into 2 actual terms because of the analyzer.\n\nTaking 'hello-world*' this will break into 2 tokens 'hello' and 'world*' which when treated as a PhraseQuery will not work, so I want to change this into a MutiPhraseQuery based on the simple tokens provided and all the existing tokens that match the wildchar character.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1287_14849e22,"{'BugID': 'OAK-1287', 'Summary': 'java.lang.IllegalArgumentException when running FlatTreeWithAceForSamePrincipalTest', 'Description': 'Running \n{code}\njava -jar oak-run*.jar benchmark FlatTreeWithAceForSamePrincipalTest Oak-Tar\n{code}\nwill end with\n{code}\njava.lang.IllegalArgumentException\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:77)\n\tat org.apache.jackrabbit.oak.plugins.segment.ListRecord.<init>(ListRecord.java:37)\n\tat org.apache.jackrabbit.oak.plugins.segment.ListRecord.getEntries(ListRecord.java:80)\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:130)\n\tat org.apache.jackrabbit.oak.util.PropertyBuilder.assignFrom(PropertyBuilder.java:225)\n\tat org.apache.jackrabbit.oak.util.PropertyBuilder.copy(PropertyBuilder.java:136)\n\tat org.apache.jackrabbit.oak.core.MutableTree.addChild(MutableTree.java:216)\n\tat org.apache.jackrabbit.oak.util.TreeUtil.addChild(TreeUtil.java:190)\n\tat org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.internalAddChild(NodeDelegate.java:841)\n\tat org.apache.jackrabbit.oak.jcr.delegate.NodeDelegate.addChild(NodeDelegate.java:684)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:288)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl$5.perform(NodeImpl.java:253)\n\tat org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:125)\n\tat org.apache.jackrabbit.oak.jcr.session.ItemImpl.perform(ItemImpl.java:111)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:253)\n\tat org.apache.jackrabbit.oak.jcr.session.NodeImpl.addNode(NodeImpl.java:238)\n\tat org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.beforeSuite(FlatTreeWithAceForSamePrincipalTest.java:56)\n\tat org.apache.jackrabbit.oak.benchmark.AbstractTest.setUp(AbstractTest.java:113)\n\tat org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.setUp(FlatTreeWithAceForSamePrincipalTest.java:31)\n\tat org.apache.jackrabbit.oak.benchmark.AbstractTest.runTest(AbstractTest.java:151)\n\tat org.apache.jackrabbit.oak.benchmark.AbstractTest.run(AbstractTest.java:138)\n\tat org.apache.jackrabbit.oak.benchmark.FlatTreeWithAceForSamePrincipalTest.run(FlatTreeWithAceForSamePrincipalTest.java:31)\n\tat org.apache.jackrabbit.oak.benchmark.BenchmarkRunner.main(BenchmarkRunner.java:195)\n\tat org.apache.jackrabbit.oak.run.Main.main(Main.java:81)\n\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1289_0c3e3d70,"{'BugID': 'OAK-1289', 'Summary': 'Range check fails with IllegalArgumentException', 'Description': '{{Range.includes()}} fails with IllegalArgumentException when provided revision is from another cluster node:\n\n{noformat}\njava.lang.IllegalArgumentException: Trying to compare revisions of different cluster ids: r142f43d2f0f-0-2 and r142f43d46fb-0-1\n\tat org.apache.jackrabbit.oak.plugins.mongomk.Revision.compareRevisionTime(Revision.java:84)\n\tat org.apache.jackrabbit.oak.plugins.mongomk.Range.includes(Range.java:55)\n{noformat}\n\nThe IllegalArgumentException was introduced with OAK-1274.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1297_73cc2442,"{'BugID': 'OAK-1297', 'Summary': 'MoveDetector does not detect moved nodes that have been moved in an earlier commit already', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-1308_69ba2a54,"{'BugID': 'OAK-1308', 'Summary': ""XPath queries with ISO9075 escaped properties don't work"", 'Description': 'XPath queries with ISO9075 escaped properties or relative path don\'t work as expected. Example: \n\n{code}\n/jcr:root//*/element(*,rep:User)[_x002e_tokens/@jcr:primaryType]\n{code}\n\nThe relative property should be converted to "".tokens/@jcr:primaryType"", but is not.\n\nThis issue is similar to OAK-1000, but for property names or relative properties.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1320_64045631,"{'BugID': 'OAK-1320', 'Summary': 'Inconsistent state in Mongo/KernelRootBuilder', 'Description': 'The state of Kernel- and MongoRootBuilder may turn inconsistent when a NodeStoreBranch.merge() performs a rebase followed by a failed merge on the underlying storage. The head and base are not properly updated to reflect the successful rebase.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1348_bc7b7e8c,"{'BugID': 'OAK-1348', 'Summary': 'ACE merging not behaving correctly if not using managed principals', 'Description': '{{org.apache.jackrabbit.api.security.JackrabbitAccessControlList#addEntry()}} does not work correctly, if the given principal is not retrieved from the PrincipalManager.\n\nException:\n{noformat}\nCaused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakAccessControl0013: Duplicate ACE found in policy\n\tat org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.accessViolation(AccessControlValidator.java:278)\n\tat org.apache.jackrabbit.oak.security.authorization.accesscontrol.AccessControlValidator.checkValidPolicy(AccessControlValidator.java:188)\n{noformat}\n\nthis used to work in jackrabbit 2.x.\n\nthe problem is probably in {{org.apache.jackrabbit.oak.security.authorization.accesscontrol.ACL#internalAddEntry}} where the principals are ""equalled"" instead of comparing their names.\n\nnote, that adding an ACE with such a principal works, just the merging/overwriting detection doesn\'t.\n\ntest:\n{code}\n  Principal p1 = new Principal() { getName(){return ""foo""}};\n  Principal p2 = new Principal() { getName(){return ""foo""}};\n  acl.addEntry(p1, privileges, true);\n  acl.addEntry(p2, privileges, false);\n  ...\n  save(); // throws\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-135_438e31a7,"{'BugID': 'OAK-135', 'Summary': 'Better support for RangeIterators', 'Description': ""Currently all RangeIterators returned from the JCR API don't implement the {{getSize()}} method but rather return {{-1}}. We should return the size of the iterator if and where feasible. ""}"
jackrabbit-oak,bugs-dot-jar_OAK-1363_69b68890,"{'BugID': 'OAK-1363', 'Summary': 'TokenLoginModule does not set userId on auth info', 'Description': 'the token login module does not set the userid in the authinfo (because it does not know it). and the LoginModuleImpl does not overwrite the AuthInfo if it already exists.\n\nthe consequence: {{Session.getUserID()}} returns {{NULL}} for logins that create a token.\n\nI think that the authinfos should be added even if they already exist. and all users of the public credentials need to be aware that authinfos can exist that are not complete.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1364_05c89637,"{'BugID': 'OAK-1364', 'Summary': 'CacheLIRS concurrency issue', 'Description': 'Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:\n\n{code}\njava.lang.NullPointerException: null\norg.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470) \norg.apache.jackrabbit.oak.cache.CacheLIRS$1.values(CacheLIRS.java:1432)\norg.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205)\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1364_b481a14c,"{'BugID': 'OAK-1364', 'Summary': 'CacheLIRS concurrency issue', 'Description': 'Some of the methods of the cache can throw a NullPointerException when the cache is used concurrently. Example stack trace:\n\n{code}\njava.lang.NullPointerException: null\norg.apache.jackrabbit.oak.cache.CacheLIRS.values(CacheLIRS.java:470) \norg.apache.jackrabbit.oak.cache.CacheLIRS$1.values(CacheLIRS.java:1432)\norg.apache.jackrabbit.oak.plugins.segment.file.FileStore.flush(FileStore.java:205)\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1369_ce0b0955,"{'BugID': 'OAK-1369', 'Summary': 'XPath queries: compatibility for missing @ in front of property names', 'Description': ""XPath queries with conditions of the form {noformat}[id='test']{noformat} are not problematic. Jackrabbit 2.x interpreted such conditions as {noformat}[@id='test']{noformat}, and Oak currently interprets them as {noformat}[@id/* = 'test']{noformat}, as this is the expected behavior for conditions of the form {noformat}[jcr:contains(id, 'test')]{noformat}.\n\nI believe the condition {noformat}[id='test']{noformat} is illegal, and it would be better to throw an exception instead, saying a @ is missing.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1429_279bb3ce,"{'BugID': 'OAK-1429', 'Summary': 'Slow event listeners do not scale as expected', 'Description': '{{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store. '}"
jackrabbit-oak,bugs-dot-jar_OAK-1429_c2f5ca6c,"{'BugID': 'OAK-1429', 'Summary': 'Slow event listeners do not scale as expected', 'Description': '{{org.apache.jackrabbit.oak.jcr.LargeOperationIT#slowListener}} does not scale to {{O n log n}} on the document node store. '}"
jackrabbit-oak,bugs-dot-jar_OAK-1432_808ac9c0,"{'BugID': 'OAK-1432', 'Summary': 'Query: use ""union"" for complex XPath queries that use multiple ""or""', 'Description': 'The following XPath query is converted to a union, however there is still an ""or"" in the converted query, which means the query still can\'t use all indexes and has to traverse the whole repository:\n\n{noformat}\n/jcr:root/a/b//element(*, nt:unstructured)[(\n(@sling:resourceType = \'x\' \nor @sling:resourceType = \'dam/collection\') \nor @sling:resourceSuperType = \'dam/collection\')] \n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1460_f1ba7a42,"{'BugID': 'OAK-1460', 'Summary': ':childOrder out of sync when node is made orderable concurrently', 'Description': 'The ChildOrderConflictHandler does not merge the :childOrder when an addExistingProperty conflict occurs.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1467_dde7de85,"{'BugID': 'OAK-1467', 'Summary': 'Commit.rollback() may remove changes from other commit', 'Description': 'Commit.rollback() removes documents it previously created. With concurrent commits it may happen that this method removes documents some other commit modified in the meantime.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1516_7c62bd81,"{'BugID': 'OAK-1516', 'Summary': 'PhraseQuery fails due to missing posiion info in indexed fields', 'Description': 'Following OAK-1487 I\'ve introduced a regression in the indexing of fields on the Lucene index.\nThere are some types of queries (the ones that use property restrictions) that cannot run anymore.\n\nbq. /jcr:root/content/dam//*[jcr:contains(jcr:content/metadata/@dc:format, \'application/pdf\')] \n\nbq. Caused by: java.lang.IllegalStateException: field ""dc:format"" was indexed without position data; cannot run PhraseQuery (term=text)\n\nI could not reproduce this in an unit test so far.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1614_86edbffb,"{'BugID': 'OAK-1614', 'Summary': ""Oak Analyzer can't tokenize chinese phrases"", 'Description': ""It looks like the _WhitespaceTokenizer_ cannot properly split Chinese phrases, for example '美女衬衫'.\nI could not find a reference to this issue other than LUCENE-5096.\n\nThe fix is to switch to the _ClassicTokenizer_ which seems better equipped for this kind of task.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1624_6d8146f8,"{'BugID': 'OAK-1624', 'Summary': 'Item names with trailing spaces should not be allowed', 'Description': 'the following should fail:\n\n{code}\n        Node hello = session.getRootNode().addNode(""hello"");\n        session.save();\n\n        Node illegal = hello.addNode(""test ""); <-- here\n        session.save();\n\n        assertEquals(""/hello/test "", illegal.getPath()); <-- and here\n\n        Node other = session.getNode(""/hello/test ""); <-- and here\n        assertTrue(other.isSame(illegal));\n        assertTrue(session.nodeExists(""/hello/test "")); <-- and here\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1648_fdc54465,"{'BugID': 'OAK-1648', 'Summary': 'Creating multiple checkpoint on same head revision overwrites previous entries', 'Description': 'Currently when a checkpoint is created in DocumentNodeStore then it is saved in form of currentHeadRev=>expiryTime. Now if multiple checkpoints are created where head revision has not changed then only the last one would be saved and previous entries would be overridden as revision is used as key\n\nOne fix would be to change the expiry time only if the new expiry time is greater than previous entry. However doing that safely in a cluster (check then save) is currently not possible with DocumentStore API as the modCount check if only supported for Nodes.\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1655_01a8b283,"{'BugID': 'OAK-1655', 'Summary': 'DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks', 'Description': 'Currently the {{DataStoreBlobStore}} has a pending TODO\n\n{code}\n @Override\n    public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {\n        //TODO Ignores the maxLastModifiedTime currently.\n        return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {\n            @Nullable\n            @Override\n            public String apply(@Nullable DataIdentifier input) {\n                return input.toString();\n            }\n        });\n    }\n{code}\n\nDue to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted'}"
jackrabbit-oak,bugs-dot-jar_OAK-1655_c91bfa54,"{'BugID': 'OAK-1655', 'Summary': 'DataStoreBlobStore does not take into maxLastModifiedTime when fetching all chunks', 'Description': 'Currently the {{DataStoreBlobStore}} has a pending TODO\n\n{code}\n @Override\n    public Iterator<String> getAllChunkIds(long maxLastModifiedTime) throws Exception {\n        //TODO Ignores the maxLastModifiedTime currently.\n        return Iterators.transform(delegate.getAllIdentifiers(), new Function<DataIdentifier, String>() {\n            @Nullable\n            @Override\n            public String apply(@Nullable DataIdentifier input) {\n                return input.toString();\n            }\n        });\n    }\n{code}\n\nDue to this it currently returns all blobId. This would issue when new binary gets created while a blob gc is running as such binaries might be considered orphan and deleted'}"
jackrabbit-oak,bugs-dot-jar_OAK-1662_3efb5cbf,"{'BugID': 'OAK-1662', 'Summary': 'Node not accessible after document split', 'Description': 'In a cluster setup it may happen that a node becomes inaccessible when all remaining local revision entries after a split are not yet visible to a cluster node.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1668_63070cf9,"{'BugID': 'OAK-1668', 'Summary': ""Lucene should not serve queries for what it doesn't index"", 'Description': 'If a query is asked and Lucene is chosen as index for serving it, it\nwill try to serve all the restrictions of the query, even the one that\nare not indexed.\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1674_073b814c,"{'BugID': 'OAK-1674', 'Summary': 'Node isNew() is false in case the node is removed and added in same commit', 'Description': 'When you remove a Node /path/a transiently and add one add /path/a again.\nThe transiently added Node isNew() check will be false.\n{code}\nroot.getNode(name).remove();\nNode newNode = root.addNode(name);\nnowNode.isNew() => false\n{code}\n\nThe API says\n{quote}\nReturns true if this is a new item, meaning that it exists only in transient storage on the Session and has not yet been saved. Within a transaction, isNew on an Item may return false (because the item has been saved) even if that Item is not in persistent storage (because the transaction has not yet been committed)....\n{quote}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1697_1552be04,"{'BugID': 'OAK-1697', 'Summary': 'Unresolved conflicts in TokenProviderImpl#createToken()', 'Description': ""In certain situations (e.g. heavy load) {{TokenProviderImpl#createToken()}} might create some unresolved conflicts.\n\ne.g. \n\n{code}\norg.apache.jackrabbit.oak.api.CommitFailedException: OakState0001: Unresolved conflicts in /home/users/..../..../.tokens/2014-04-07T11.55.58.167+02.00\n{code}\n\nand\n\n{code}\n01.04.2014 17:52:41.216 *WARN* [qtp218544742-286] org.apache.jackrabbit.oak.security.authentication.token.TokenProviderImpl Failed to create login token.\n01.04.2014 17:52:41.218 *WARN* [qtp218544742-300] org.eclipse.jetty.servlet.ServletHandler /projects.html\njava.lang.IllegalArgumentException: Invalid token ''\n    at org.apache.jackrabbit.api.security.authentication.token.TokenCredentials.<init>(TokenCredentials.java:42)\n{code}""}"
jackrabbit-oak,bugs-dot-jar_OAK-1719_c3773d53,"{'BugID': 'OAK-1719', 'Summary': 'Missing commit hooks in upgrade', 'Description': ""There's a TODO in the RepositoryUpgrade class about missing commit hooks. For example the PermissionHook isn't currently run as a part of the upgrade, which breaks permission evaluation even though the actual ACL nodes are present after the upgrade.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1727_26041fe7,"{'BugID': 'OAK-1727', 'Summary': 'Cross foreign cluster revision comparison may be wrong', 'Description': 'Running one of the access control related benchmarks concurrently on a MongoDB may result in strange conflicts even when DocumentNodeStore retries the commit. The root cause may be a wrong revision comparison when both revision to compare are from a foreign cluster node and one of them is not withing the known seen-at revision ranges.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1729_7ba9dd66,"{'BugID': 'OAK-1729', 'Summary': 'DocumentNodeStore revision GC removes intermediate docs', 'Description': 'The revision garbage collection in DocumentNodeStore removes intermediate documents of the revision history of a node even if it is still in use.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1731_024e5d37,"{'BugID': 'OAK-1731', 'Summary': 'Repository upgrade does not copy default values of property definitions', 'Description': 'The {{RepositoryUpgrade}} class needs to copy also the default values of property definitions in the node types being upgraded. See the TODO in https://github.com/apache/jackrabbit-oak/blob/jackrabbit-oak-0.20.0/oak-upgrade/src/main/java/org/apache/jackrabbit/oak/upgrade/RepositoryUpgrade.java#L485.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1739_8188ef54,"{'BugID': 'OAK-1739', 'Summary': 'Incorrect handling of multivalued comparisons in queries', 'Description': '[Section 6.7.14|http://www.day.com/specs/jcr/2.0/6_Query.html#6.7.16 Comparison] of the JCR 2.0 spec says:\n\nbq. ... operand1 may evaluate to an array of values (for example, the values of a multi-valued property), in which case the comparison is separately performed for each element of the array, and the Comparison constraint is satisfied as a whole if the comparison against any element of the array is satisfied.\n\nThis is currently not the case in Oak. Instead only the first value of the array is used in the comparison.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1749_591e4d4a,"{'BugID': 'OAK-1749', 'Summary': 'AsyncIndexUpdate may resurrect nodes', 'Description': 'There is a race condition in the AsyncIndexUpdate.run() method. The implementation creates a checkpoint used as the after node state for the comparison with the previous checkpoint. In a next step a builder is created from the current root state of the node store. Node removed between the checkpoint call and retrieving the root state may get resurrected by the AsyncIndexUpdate.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1761_f37ce716,"{'BugID': 'OAK-1761', 'Summary': 'DocumentNodeStore does not make use of References while serializing Blob', 'Description': 'The BlobSerializer in DocumentNodeStore does not make use of Blob references which results in copying the blobs by value hence significantly slowing down any migration '}"
jackrabbit-oak,bugs-dot-jar_OAK-1770_192ee9e4,"{'BugID': 'OAK-1770', 'Summary': 'Document split suppressed with steady load on many cluster nodes', 'Description': 'Document split is suppressed when there is a steady write load on many cluster nodes. The document grows bigger over time and leads to poor performance.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1779_9d36bede,"{'BugID': 'OAK-1779', 'Summary': 'Stale cache after MongoMK GC', 'Description': ""After a MongoMK revision GC the docChildrenCache may be stale and lead to a NPE when reading children with deleted and GC'ed siblings.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1784_2426deae,"{'BugID': 'OAK-1784', 'Summary': 'Async index update persists conflict markers', 'Description': ""A long running test I performed yesterday failed with a FileNotFoundException in the lucene index. After analyzing the issue it turned  out the async index update persisted a conflict markers introduced by a rebase call. So far I'm not able to reproduce it with a more simple test setup and after a shorter time (the initial test failed after 10 hours). Given the way the async index update work, there shouldn't be any conflicts, because it's the only component writing into this location of the repository. \n\nAs an immediate workaround, I'd like to add the AnnotatingConflictHandler & ConflictValidator combo to the merge call to make sure a commit with conflict markers does not get persisted.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1788_dd3437d4,"{'BugID': 'OAK-1788', 'Summary': 'ConcurrentConflictTest fails occasionally', 'Description': 'Occurs every now and then on buildbot. E.g.:\nhttp://ci.apache.org/builders/oak-trunk-win7/builds/16'}"
jackrabbit-oak,bugs-dot-jar_OAK-1789_07646fba,"{'BugID': 'OAK-1789', 'Summary': 'Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes', 'Description': 'In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.\n\nTo avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1789_08ba79d4,"{'BugID': 'OAK-1789', 'Summary': 'Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes', 'Description': 'In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.\n\nTo avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1789_9f7c1df0,"{'BugID': 'OAK-1789', 'Summary': 'Upgraded version history has UUIDs as jcr:frozenUuid of non-referenceable nodes', 'Description': 'In Jackrabbit Classic each node, even non-referenceable ones, has a UUID as its identifier, and thus the {{jcr:frozenUuid}} properties of frozen nodes are always UUIDs. In contrast Oak uses path identifiers for non-referenceable frozen nodes (see OAK-1009), which presents a problem when dealing with version histories migrated from Jackrabbit Classic.\n\nTo avoid this mismatch, the upgrade code should check each frozen node for referenceability and replace the frozen UUID with a path identifier if needed.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1793_16225d51,"{'BugID': 'OAK-1793', 'Summary': 'MongoMK GC removes documents with data still in use', 'Description': 'The version garbage collector may delete previous documents that contain commit root information still in use by the main document.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1807_077efee5,"{'BugID': 'OAK-1807', 'Summary': 'ConstraintViolationException seen with multiple Oak/Mongo with ConcurrentCreateNodesTest', 'Description': 'While running ConcurrentCreateNodesTest with 5 instances writing to same Mongo instance following exception is seen\n\n{noformat}\nException in thread ""Background job org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer@3f56e5ed"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist\n    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:111)\n    at org.apache.jackrabbit.oak.benchmark.AbstractTest$1.run(AbstractTest.java:481)\nCaused by: javax.jcr.nodetype.ConstraintViolationException: OakConstraint0001: /: The primary type rep:root does not exist\n    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:225)\n    at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)\n    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:679)\n    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:553)\n    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:417)\n    at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.perform(SessionImpl.java:414)\n    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.perform(SessionDelegate.java:308)\n    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.perform(SessionImpl.java:127)\n    at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:414)\n    at org.apache.jackrabbit.oak.benchmark.ConcurrentCreateNodesTest$Writer.run(ConcurrentCreateNodesTest.java:100)\n    ... 1 more\nCaused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakConstraint0001: /: The primary type rep:root does not exist\n    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.constraintViolation(TypeEditor.java:150)\n    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.getEffectiveType(TypeEditor.java:286)\n    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditor.<init>(TypeEditor.java:101)\n    at org.apache.jackrabbit.oak.plugins.nodetype.TypeEditorProvider.getRootEditor(TypeEditorProvider.java:85)\n    at org.apache.jackrabbit.oak.spi.commit.CompositeEditorProvider.getRootEditor(CompositeEditorProvider.java:80)\n    at org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:53)\n    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)\n    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:60)\n    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch$InMemory.merge(AbstractNodeStoreBranch.java:498)\n    at org.apache.jackrabbit.oak.spi.state.AbstractNodeStoreBranch.merge(AbstractNodeStoreBranch.java:300)\n    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:129)\n    at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:159)\n    at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1275)\n    at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)\n    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:405)\n    at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:551)\n    ... 7 more\n{noformat}\n\nThis has been reported by [~rogoz]'}"
jackrabbit-oak,bugs-dot-jar_OAK-1817_78c37386,"{'BugID': 'OAK-1817', 'Summary': 'NPE in MarkSweepGarbageCollector.saveBatchToFile during Datastore GC with FileDataStore', 'Description': 'During running a datastore garbage collection on a Jackrabbit 2 FileDataStore (org.apache.jackrabbit.oak.plugins.blob.datastore.FileDataStore, see http://jackrabbit.apache.org/oak/docs/osgi_config.html) an NPE is thrown\n{code}\n13.05.2014 17:50:16.944 *ERROR* [qtp1416657193-147] org.apache.jackrabbit.oak.management.ManagementOperation Blob garbage collection failed\njava.lang.RuntimeException: Error in retrieving references\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector$1.addReference(MarkSweepGarbageCollector.java:395)\n\tat org.apache.jackrabbit.oak.plugins.segment.Segment.collectBlobReferences(Segment.java:248)\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentTracker.collectBlobReferences(SegmentTracker.java:178)\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentBlobReferenceRetriever.collectReferences(SegmentBlobReferenceRetriever.java:38)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.iterateNodeTree(MarkSweepGarbageCollector.java:361)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.mark(MarkSweepGarbageCollector.java:201)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.markAndSweep(MarkSweepGarbageCollector.java:173)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.collectGarbage(MarkSweepGarbageCollector.java:149)\n\tat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService$2.collectGarbage(SegmentNodeStoreService.java:185)\n\tat org.apache.jackrabbit.oak.plugins.blob.BlobGC$1.call(BlobGC.java:68)\n\tat org.apache.jackrabbit.oak.plugins.blob.BlobGC$1.call(BlobGC.java:64)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException: null\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192)\n\tat com.google.common.base.Joiner.toString(Joiner.java:436)\n\tat com.google.common.base.Joiner.appendTo(Joiner.java:108)\n\tat com.google.common.base.Joiner.appendTo(Joiner.java:152)\n\tat com.google.common.base.Joiner.join(Joiner.java:193)\n\tat com.google.common.base.Joiner.join(Joiner.java:183)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector.saveBatchToFile(MarkSweepGarbageCollector.java:317)\n\tat org.apache.jackrabbit.oak.plugins.blob.MarkSweepGarbageCollector$1.addReference(MarkSweepGarbageCollector.java:391)\n\t... 14 common frames omitted\n{code}\n\nAttached you find the OSGi config for both the nodestore and the datastore.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1822_016df669,"{'BugID': 'OAK-1822', 'Summary': 'NodeDocument _modified may go back in time', 'Description': 'In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1822_3e83a4c1,"{'BugID': 'OAK-1822', 'Summary': 'NodeDocument _modified may go back in time', 'Description': 'In a cluster with multiple DocumentMK instances the _modified field of a NodeDocument may go back in time. This will result in incorrect diff calculations when the DocumentNodeStore uses the _modified field to find changed nodes for a given revision range.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1829_ca36450e,"{'BugID': 'OAK-1829', 'Summary': 'IllegalStateException when using ""lowerCase""/""lower"" on a array property', 'Description': ""if query contain lowerCase on array property then QueryResult.getRows() throwing  IllegalStateException.\n\nQuery which causing issue\n\n select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND (LOWER([selector_1].[dataDictionaryRefs]) = 'employeedd')\n\nIf we remove LOWER function then it is working \n\n select [selector_1].* from [nt:unstructured] AS [selector_1] where (([selector_1].[lcc:className] = 'com.adobe.icc.dbforms.obj.ConditionalDataModule')) AND ([selector_1].[dataDictionaryRefs] = 'EmployeeDD')""}"
jackrabbit-oak,bugs-dot-jar_OAK-1848_093b9128,"{'BugID': 'OAK-1848', 'Summary': 'Default sync handler property mapping does not allow constant properties', 'Description': 'it would be useful, if the default sync handler user (and group) mapping could also handle constant properties and use given primary type and mixin type information. eg:\n\n{noformat}\nprofile/nt:primaryType=""sling:Folder""\nprofile/sling:resourceType=""sling/security/profile""\n{noformat}\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-185_7fe28a0e,"{'BugID': 'OAK-185', 'Summary': 'Trying to remove a missing property throws PathNotFoundException', 'Description': 'The following code snippet throws a {{PathNotFoundException}} if the ""missing"" property is not present.\n\n{code:java}\nnode.setProperty(""missing"", (String) null);\n{code}\n\nA better way to handle such a case would be for the above statement to simply do nothing.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1874_3ae276c1,"{'BugID': 'OAK-1874', 'Summary': 'Indexes: re-index automatically when adding an index', 'Description': 'When adding an index via import of content, the index is not automatically re-built. This is problematic, because subsequent queries will return no data because of that. Currently, the only way to re-index is to set the ""reindex"" property to ""true"".\n\nI suggest that indexes are automatically re-indexes if the hidden child node ("":data"" I believe) is missing. This is in addition to the ""reindex"" property.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1877_716e1237,"{'BugID': 'OAK-1877', 'Summary': 'Hourly async reindexing on an idle instance', 'Description': 'OAK-1292 introduced the following interesting but not very nice behavior:\n\nOn an idle system with no changes for an extended amount of time, the OAK-1292 change blocks the async indexer from updating the reference to the last indexed checkpoint. After one hour (the default checkpoint lifetime), the referenced checkpoint will expire, and the indexer will fall back to full reindexing.\n\nThe result of this behavior is that once every hour, the size of an idle instance will grow with dozens or hundreds of megabytes of new index data generated by reindexing. Older index data becomes garbage, but the compaction code from OAK-1804 is needed to make it collectable. A better solution would be to prevent the reindexing from happening in the first place.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1883_9c2421ed,"{'BugID': 'OAK-1883', 'Summary': 'Unnecessary invocations of LastRevRecovery when recovery already done.', 'Description': 'Even after _lastRev recovery executed on a cluster node, there are unnecessary  invocations of recovery happening on that cluster node, till that cluster node comes online again.\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1894_35562cce,"{'BugID': 'OAK-1894', 'Summary': 'PropertyIndex only considers the cost of a single indexed property', 'Description': ""The existing PropertyIndex loops through the PropertyRestriction objects in the Filter and essentially only calculates the cost of the first indexed property. This isn't actually the first property in the query and Filter.propertyRestrictions is a HashMap.\n\nMore confusingly, the plan for a query with multiple indexed properties outputs *all* indexed properties, even though only the first one is used.\n\nFor queries with multiple indexed properties, the cheapest property index should be used in all three relevant places: when calculating the cost, when executing the query, and when producing the plan.""}"
jackrabbit-oak,bugs-dot-jar_OAK-1899_b6f89048,"{'BugID': 'OAK-1899', 'Summary': 'Ordered index fails with old index content', 'Description': 'With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.\n\nAs a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.\n\n{noformat}\njava.lang.IndexOutOfBoundsException: index (3) must be less than size (1)\n    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)\n    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)\n    at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)\n    at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1899_df59fb45,"{'BugID': 'OAK-1899', 'Summary': 'Ordered index fails with old index content', 'Description': 'With the latest changes, the ordered index no longer works with old index data. When running the latest Oak 1.0.2 snapshot run against an Oak 1.0.0 repository with an existing ordered index, the index fails with the exception below.\n\nAs a workaround, the ordered index can be manually re-built. Either the index re-build needs to be automatic, or the ordered index needs to work with the old index content.\n\n{noformat}\njava.lang.IndexOutOfBoundsException: index (3) must be less than size (1)\n    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:306)\n    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:285)\n    at org.apache.jackrabbit.oak.plugins.segment.SegmentPropertyState.getValue(SegmentPropertyState.java:157)\n    at org.apache.jackrabbit.oak.plugins.index.property.strategy.OrderedContentMirrorStoreStrategy.getPropertyNext(OrderedContentMirrorStoreStrategy.java:1024)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1916_705ce1d1,"{'BugID': 'OAK-1916', 'Summary': ""NodeStoreKernel doesn't handle array properties correctly"", 'Description': '{{NodeStoreKernel}} currently only supports array properties of type long. For other types it will fail with an {{IllegalStateException}}. See also the FIXME in the code.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1926_9225a3e2,"{'BugID': 'OAK-1926', 'Summary': 'UnmergedBranch state growing with empty BranchCommit leading to performance degradation', 'Description': 'In some cluster deployment cases it has been seen that in memory state of UnmergedBranches contains large number of empty commits. For e.g. in  one of of the runs there were 750 entries in the UnmergedBranches and each Branch had empty branch commits.\n\nIf there are large number of UnmergedBranches then read performance would degrade as for determining revision validity currently logic scans all branches\n\nBelow is some part of UnmergedBranch state\n\n{noformat}\nBranch 1\n1 -> br146d2edb7a7-0-1 (true) (revision: ""br146d2edb7a7-0-1"", clusterId: 1, time: ""2014-06-25 05:08:52.903"", branch: true)\n2 -> br146d2f0450b-0-1 (true) (revision: ""br146d2f0450b-0-1"", clusterId: 1, time: ""2014-06-25 05:11:40.171"", branch: true)\nBranch 2\n1 -> br146d2ef1d08-0-1 (true) (revision: ""br146d2ef1d08-0-1"", clusterId: 1, time: ""2014-06-25 05:10:24.392"", branch: true)\nBranch 3\n1 -> br146d2ed26ca-0-1 (true) (revision: ""br146d2ed26ca-0-1"", clusterId: 1, time: ""2014-06-25 05:08:15.818"", branch: true)\n2 -> br146d2edfd0e-0-1 (true) (revision: ""br146d2edfd0e-0-1"", clusterId: 1, time: ""2014-06-25 05:09:10.670"", branch: true)\nBranch 4\n1 -> br146d2ecd85b-0-1 (true) (revision: ""br146d2ecd85b-0-1"", clusterId: 1, time: ""2014-06-25 05:07:55.739"", branch: true)\nBranch 5\n1 -> br146d2ec21a0-0-1 (true) (revision: ""br146d2ec21a0-0-1"", clusterId: 1, time: ""2014-06-25 05:07:08.960"", branch: true)\n2 -> br146d2ec8eca-0-1 (true) (revision: ""br146d2ec8eca-0-1"", clusterId: 1, time: ""2014-06-25 05:07:36.906"", branch: true)\nBranch 6\n1 -> br146d2eaf159-1-1 (true) (revision: ""br146d2eaf159-1-1"", clusterId: 1, time: ""2014-06-25 05:05:51.065"", counter: 1, branch: true)\nBranch 7\n1 -> br146d2e9a513-0-1 (true) (revision: ""br146d2e9a513-0-1"", clusterId: 1, time: ""2014-06-25 05:04:26.003"", branch: true)\n{noformat}\n\n[~mreutegg] Suggested that these branch might be for those revision which have resulted in a collision and upon checking it indeed appears to be the case  (value true in brackets above indicate that). Further given the age of such revision it looks like they get populated upon startup itself\n\n*Fix*\n* Need to check why we need to populate the UnermgedBranch\n* Possibly implement some purge job which would remove such stale entries \n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-1932_913c2f53,"{'BugID': 'OAK-1932', 'Summary': 'TarMK compaction can create mixed segments', 'Description': 'As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1932_c215b267,"{'BugID': 'OAK-1932', 'Summary': 'TarMK compaction can create mixed segments', 'Description': 'As described in http://markmail.org/message/ujkqdlthudaortxf, commits that occur while the compaction operation is running can make the compacted segments contain references to older data segments, which prevents old data from being reclaimed during cleanup.'}"
jackrabbit-oak,bugs-dot-jar_OAK-1933_2e16a983,"{'BugID': 'OAK-1933', 'Summary': 'Query: UnsupportedOperationException for some combinations of ""or"" and ""and"" conditions', 'Description': 'The following query throws an UnsupportedOperationException:\n\n{noformat}\nselect * from [nt:base] \n  where [a] = 1 and [b] = 2 and [b] = 3 or [c] = 4\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1959_93c1aa40,"{'BugID': 'OAK-1959', 'Summary': 'AsyncIndexUpdate unable to cope with missing checkpoint ref', 'Description': 'The async index uses a checkpoint reference stored under the _:async_ hidden node as a base for running the index diff.\nIt might happen that this reference is stale (pointing to checkpoints that no longer exist) so the async indexer logs a warning that it will reindex everything and will start its work.\nThe trouble is with the #mergeWithConcurrencyCheck which does not cope well with this scenario. Even if the ref checkpoint is null, it will throw a concurrent update exception which will be logged as a misleading debug log _Concurrent update detected in the async index update_.\n\nOverall the code looks stuck in an endless reindexing loop.\n\n{code}\n*WARN* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Failed to retrieve previously indexed checkpoint 569d8847-ebb6-4832-a55f-2b0b1a32ae71; re-running the initial async index update\n*DEBUG* [pool-9-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate Concurrent update detected in the async index update\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-1977_4bfbfcdd,"{'BugID': 'OAK-1977', 'Summary': 'ContentMirrorStoreStrategy should utilize path restriction when available', 'Description': ""Currently {{ContentStoreMirrorStrategy}} has a mirror of content path under {{:index}}. Yet, while {{query}} (and {{count}}) methods doesn't jump directly into restricted path.\n\nThis would be very useful for {{PropertyIndex}} where the queries can be optimized by supplying a path restriction along with an indexed property restriction (I don't know if queries with references would use paths so much though)""}"
jackrabbit-oak,bugs-dot-jar_OAK-1985_f620b79b,"{'BugID': 'OAK-1985', 'Summary': ""TokenLoginModule can't handle case insensitive userids"", 'Description': 'Login against TokenLoginModule with an userid different in case throws:\n  javax.security.auth.login.LoginException: Invalid token credentials.\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-2021_004db804,"{'BugID': 'OAK-2021', 'Summary': 'XPath queries with certain combinations of ""or"" conditions don\'t use an index', 'Description': 'XPath queries with the following conditions are not converted to ""union"" SQL-2 queries and therefore don\'t use an index:\n\n{noformat}\n/jcr:root/content//*[((@i = \'1\' or @i = \'2\') or (@s = \'x\')) and (@t = \'a\' or @t = \'b\')]\n{noformat}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-2029_e30023ba,"{'BugID': 'OAK-2029', 'Summary': ""Oak Lucene index doesn't get notified about updates when index is stored on the file system"", 'Description': ""It looks like the the lucene IndexTracked class responsible for refreshing the in-memory cache of the lucene index doesn't get the update notification when the index is stored on the file system.\nThis results in searches not working until the next restart""}"
jackrabbit-oak,bugs-dot-jar_OAK-2047_a0a495f0,"{'BugID': 'OAK-2047', 'Summary': 'Missing privileges after repository upgrade', 'Description': 'After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties,\nrep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).\n\nThe reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade. '}"
jackrabbit-oak,bugs-dot-jar_OAK-2047_ca63fdf3,"{'BugID': 'OAK-2047', 'Summary': 'Missing privileges after repository upgrade', 'Description': 'After upgrading from Jackrabbit classic all Oak specific privileges are missing (rep:userManagement, rep:readNodes, rep:readProperties, rep:addProperties,\nrep:alterProperties, rep:removeProperties, rep:indexDefinitionManagement).\n\nThe reason seems to be that the {{PrivilegeInitializer}} is not run during upgrade. '}"
jackrabbit-oak,bugs-dot-jar_OAK-2049_4af0d4ee,"{'BugID': 'OAK-2049', 'Summary': 'ArrayIndexOutOfBoundsException in Segment.getRefId()', 'Description': 'It looks like there is some SegmentMK bug that causes the {{Segment.getRefId()}} to throw an {{ArrayIndexOutOfBoundsException}} in some fairly rare corner cases.\nThe data was originally migrated into oak via the crx2oak tool mentioned here: http://docs.adobe.com/docs/en/aem/6-0/deploy/upgrade.html\nThat tool uses *oak-core-1.0.0* creating an oak instance.\n\nSimilar to OAK-1566 this system was using FileDataStore with SegmentNodeStore.\n\nIn this case the error is seen when running offline compaction using oak-run-1.1-SNAPSHOT.jar (latest).\n\n{code:none}\n> java -Xmx4096m -jar oak-run-1.1-SNAPSHOT.jar compact /oak/crx-quickstart/repository/segmentstore\nApache Jackrabbit Oak 1.1-SNAPSHOT\nCompacting /wcm/cq-author/crx-quickstart/repository/segmentstore\nbefore [data00055a.tar, data00064a.tar, data00045b.tar, data00005a.tar, data00018a.tar, data00022a.tar, data00047a.tar, data00037a.tar, data00049a.tar, data00014a.tar, data00066a.tar, data00020a.tar, data00058a.tar, data00065a.tar, data00069a.tar, data00012a.tar, data00009a.tar, data00060a.tar, data00041a.tar, data00016a.tar, data00072a.tar, data00048a.tar, data00061a.tar, data00053a.tar, data00038a.tar, data00001a.tar, data00034a.tar, data00003a.tar, data00052a.tar, data00006a.tar, data00027a.tar, data00031a.tar, data00056a.tar, data00035a.tar, data00063a.tar, data00068a.tar, data00008v.tar, data00010a.tar, data00043b.tar, data00021a.tar, data00017a.tar, data00024a.tar, data00054a.tar, data00051a.tar, data00057a.tar, data00059a.tar, data00036a.tar, data00033a.tar, data00019a.tar, data00046a.tar, data00067a.tar, data00004a.tar, data00044a.tar, data00013a.tar, data00070a.tar, data00026a.tar, data00002a.tar, data00011a.tar, journal.log, data00030a.tar, data00042a.tar, data00025a.tar, data00062a.tar, data00023a.tar, data00071a.tar, data00032b.tar, data00040a.tar, data00015a.tar, data00029a.tar, data00050a.tar, data00000a.tar, data00007a.tar, data00028a.tar, data00039a.tar]\n-> compacting\nException in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 206\nat org.apache.jackrabbit.oak.plugins.segment.Segment.getRefId(Segment.java:191)\nat org.apache.jackrabbit.oak.plugins.segment.Segment.internalReadRecordId(Segment.java:299)\nat org.apache.jackrabbit.oak.plugins.segment.Segment.readRecordId(Segment.java:295)\nat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplateId(SegmentNodeState.java:69)\nat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getTemplate(SegmentNodeState.java:78)\nat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.getProperties(SegmentNodeState.java:150)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:154)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor$CompactDiff.childNodeAdded(Compactor.java:124)\nat org.apache.jackrabbit.oak.plugins.memory.EmptyNodeState.compareAgainstEmptyState(EmptyNodeState.java:160)\nat org.apache.jackrabbit.oak.plugins.segment.SegmentNodeState.compareAgainstBaseState(SegmentNodeState.java:395)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor.process(Compactor.java:80)\nat org.apache.jackrabbit.oak.plugins.segment.Compactor.compact(Compactor.java:85)\nat org.apache.jackrabbit.oak.plugins.segment.file.FileStore.compact(FileStore.java:438)\nat org.apache.jackrabbit.oak.run.Main.compact(Main.java:311)\nat org.apache.jackrabbit.oak.run.Main.main(Main.java:133)\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-2062_5c4589bd,"{'BugID': 'OAK-2062', 'Summary': 'Range queries and relative properties resultset should be consistent with JR2', 'Description': 'When running a range query like {{/jcr:root/content/nodes//*[(*/*/*/@prop >= 9)]}} the resultset is not consistent for the same use-case when running in jacrabbit 2.'}"
jackrabbit-oak,bugs-dot-jar_OAK-208_daf9a4ef,"{'BugID': 'OAK-208', 'Summary': 'RootImplFuzzIT test failures', 'Description': ""As seen in the CI build, {{RootImplFuzzIT}} fails every now and then. This might be because of OAK-174, but there's been quite a bit of other work on the same area, so this could be caused also by something else.\n\nThe troublesome seeds as seen in failing CI builds are 1437930918, 206057576, 1638075186, 1705736349, -1856261793 and 569172885.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2117_c7669f31,"{'BugID': 'OAK-2117', 'Summary': 'Reindex removes all nodes under index definition node', 'Description': 'Reindex logic in {{IndexUpdate}} removes all child node from index definition node thus removing valid nodes which might be part of index defintion. It should only remove hidden nodes'}"
jackrabbit-oak,bugs-dot-jar_OAK-2147_a1556c30,"{'BugID': 'OAK-2147', 'Summary': '[Ordered Index] Indexing on large content is slow', 'Description': 'Indexing large number of ordered properties is quite slow.\n\nExplore ways of making it faster. The current skip list implementation uses 4 lanes with a probability of 10%. It should be made configurable and the defaults changed.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2174_5931a4a7,"{'BugID': 'OAK-2174', 'Summary': ""Non-blocking reindexing doesn't finish properly"", 'Description': ""The non blocking reindexer needs to run at least 2 cycles before setting the index definition back to synchronous mode. Currently it is too eager to mark the status as 'done' which confuses the _PropertyIndexAsyncReindex_ mbean into thinking the indexing is over and so skipping the final round that is supposed to do the switch back to sync mode.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2219_f2740ce1,"{'BugID': 'OAK-2219', 'Summary': 'Ordered index does not return relative properties for un-restricted indexes', 'Description': 'Even if we specify an index without any restriction to node type; the ordered index does not return any result for relative properties'}"
jackrabbit-oak,bugs-dot-jar_OAK-2235_29d3d8f1,"{'BugID': 'OAK-2235', 'Summary': 'Lucene index not created if no node is indexed', 'Description': ""If a Lucene property index is defined for a property which is not present in any of the nodes then {{LuceneIndexWriter}} would create any lucene index for that.\n\nFor eg if we have an index of {{foo}} and none of the node has property {{foo}} set in that case {{LuceneIndexWriter}} would not create an {{IndexWriter}} and hence no directory would be created. Later when system performs a query like {{select jcr:path from nt:base where foo = 'bar'}} then {{LucenePropertyIndex}} would not participate in the query as no Lucene index would be found and system would revert to traversal.\n\nAs a fix Lucene index should still be created even if it does not contain any document""}"
jackrabbit-oak,bugs-dot-jar_OAK-2238_a28098fd,"{'BugID': 'OAK-2238', 'Summary': 'Session.getItem violates JCR Spec', 'Description': 'Session.getItem(path) is supposed to first return a node for the given path, and if no node is found return a property.  The oak implementation returns this in the opposite order.\n\nsee attached patch for a possible fix.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2246_dcadb0e1,"{'BugID': 'OAK-2246', 'Summary': 'UUID collision check is not does not work in transient space', 'Description': 'I think OAK-1037 broke the system view import.\n\ntest case:\n1. create a new node with a uuid (referenceable, or new user)\n2. import systemview with IMPORT_UUID_COLLISION_REPLACE_EXISTING\n3. save()\n\nresult:\n{noformat}\njavax.jcr.nodetype.ConstraintViolationException: OakConstraint0030: Uniqueness constraint violated at path [/] for one of the property in [jcr:uuid] having value e358efa4-89f5-3062-b10d-d7316b65649e\n{noformat}\n\nexpected:\n* imported content should replace the existing node - even in transient space.\n\nnote:\n* if you perform a save() after step 1, everything works.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2249_6dde8e9d,"{'BugID': 'OAK-2249', 'Summary': 'XPath: Query with mixed full-text, ""and"", ""or"" conditions fails', 'Description': ""When performing a query like \n\n{noformat}\n        //element(*, test:Asset)[\n            (\n                jcr:contains(., 'summer')\n                or\n                jcr:content/metadata/@tags = 'namespace:season/summer'\n            ) and\n                jcr:contains(jcr:content/metadata/@format, 'image')\n            ]\n\n{noformat}\n\nThe Lucene/Aggregate returns as well nodes that does not match all the\ncriterias.\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-2250_08b25cb0,"{'BugID': 'OAK-2250', 'Summary': 'Lucene Index property definition is ignored if its not in includePropertyNames config', 'Description': 'Lucene index property definition will not be used unless that property is in includePropertyNames config. This enforces including that property in includePropertyNames.\nincludePropertyNames restricts all properties from getting indexed, so user is now enforced to include all properties in includePropertyNames to be indexed.\n '}"
jackrabbit-oak,bugs-dot-jar_OAK-225_e33328e0,"{'BugID': 'OAK-225', 'Summary': 'Sling I18N queries not supported by Oak', 'Description': ""The Sling I18N component issues XPath queries like the following:\n\n{code:none}\n//element(*,mix:language)[fn:lower-case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message)\n{code}\n\nSuch queries currently fail with the following exception:\n\n{code:none}\njavax.jcr.query.InvalidQueryException: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (\n        at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:115)\n        at org.apache.jackrabbit.oak.jcr.query.QueryImpl.execute(QueryImpl.java:85)\n        at org.apache.sling.jcr.resource.JcrResourceUtil.query(JcrResourceUtil.java:52)\n        at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.queryResources(JcrResourceProvider.java:262)\n        ... 54 more\nCaused by: java.text.ParseException: Query: //element(*,mix:language)[fn:lower-(*)case(@jcr:language)='en']//element(*,sling:Message)[@sling:message]/(@sling:key|@sling:message); expected: (\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.getSyntaxError(XPathToSQL2Converter.java:704)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.read(XPathToSQL2Converter.java:410)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseExpression(XPathToSQL2Converter.java:336)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseCondition(XPathToSQL2Converter.java:279)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseAnd(XPathToSQL2Converter.java:252)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.parseConstraint(XPathToSQL2Converter.java:244)\n        at org.apache.jackrabbit.oak.query.XPathToSQL2Converter.convert(XPathToSQL2Converter.java:153)\n        at org.apache.jackrabbit.oak.query.QueryEngineImpl.parseQuery(QueryEngineImpl.java:86)\n        at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:99)\n        at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:39)\n        at org.apache.jackrabbit.oak.jcr.query.QueryManagerImpl.executeQuery(QueryManagerImpl.java:110)\n{code}""}"
jackrabbit-oak,bugs-dot-jar_OAK-2260_0ac7ff20,"{'BugID': 'OAK-2260', 'Summary': 'TarMK Cold Standby can corrupt bulk segments', 'Description': ""There's a race condition on the segment transfer code that may introduce corrupted binary segments on the secondary instance. What can happen during the head sync phase is that the master may send the head segment twice which will make the client receive&store the second segment thinking it's a different one.\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-2288_57bd2dc5,"{'BugID': 'OAK-2288', 'Summary': 'DocumentNS may expose branch commit on earlier revision', 'Description': ""The DocumentNodeStore may expose the changes of a branch on a revision earlier than it's commit revision. This only happens when the read revision equals the revision of the not yet merged changes on the branch.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2308_f4d5bbe1,"{'BugID': 'OAK-2308', 'Summary': 'Incorrect recovery of _lastRev for branch commit', 'Description': 'The recovery process for _lastRevs is incorrect for branch commits. It propagates the revision of the commit to the branch up to the root node instead of the revision of the merge for the changes.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2311_ca85ecce,"{'BugID': 'OAK-2311', 'Summary': 'Released checkpoint can still be retrieved ', 'Description': 'The following fails on the 2nd assertion on the MongoMK\n\n{code}\nassertTrue(store.release(cp));\nassertNull(store.retrieve(cp));\n{code}\n\nThe JavaDoc on the {{release}} method is a bit vague, but I assume it is safe to assume that when it returns {{true}} the checkpoint should be gone. If not, we should update the JavaDoc. '}"
jackrabbit-oak,bugs-dot-jar_OAK-2318_1d08cbd3,"{'BugID': 'OAK-2318', 'Summary': 'DocumentNodeStore.diffManyChildren() reads too many nodes', 'Description': ""DocumentNodeStore.diffManyChildren() compares too many nodes when running in a non-clustered setup and there are many changes below a location with 'many' children.\n\nThis is a regression introduced by OAK-2232. The fix changed the way how the minimum revision is calculated based on the two revisions to compare. The seen-at revision of the RevisionComparator is taken into account. However, in a single cluster node setup, the revision range for the current clusterId is never updated. This means the minimum revision is calculated too far back and causes queries with too many nodes than necessary.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2330_408a566e,"{'BugID': 'OAK-2330', 'Summary': 'Field boost not working if the property for indexing is picked using aggregate index rules', 'Description': 'For below index definition - \n{code}\n{  \n   jcr:primaryType:""oak:QueryIndexDefinition"",\n   compatVersion:2,\n   type:""lucene"",\n   async:""async"",\n   reindex:false,\n   reindexCount:12,\n   aggregates:{  \n      jcr:primaryType:""oak:Unstructured"",\n      app:Asset:{  \n         jcr:primaryType:""oak:Unstructured"",\n         include0:{  \n            jcr:primaryType:""oak:Unstructured"",\n            path:""jcr:content/metadata/*""\n         }\n      }\n   },\n   indexRules:{  \n      jcr:primaryType:""nt:unstructured"",\n      app:Asset:{  \n         jcr:primaryType:""nt:unstructured"",\n         properties:{  \n            jcr:primaryType:""nt:unstructured"",\n            foo:{  \n               jcr:primaryType:""nt:unstructured"",\n               nodeScopeIndex:true,\n               ordered:true,\n               propertyIndex:true,\n               name:""jcr:content/metadata/foo"",\n               type:""Long"",\n               boost:3,\n               nodeName:""foo""\n            }\n         }\n      }\n   }\n}\n{code}\n\nOn executing query of form - \n\n{code}\n//element(*, app:Asset) \n[\n    jcr:contains(., \'bar\' )\n]\n{code}\n\nshould boost the results containing property - \'jcr:content/metadata/foo\', but its ignoring index time boosting for it.\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-2336_d0f6715d,"{'BugID': 'OAK-2336', 'Summary': 'NodeDocument.getNodeAtRevision() may read too many revisions', 'Description': 'This is a regression introduced by OAK-1972.\n\nThe revision returned with the value may be different from the revision of the change when the change was first committed to a branch and later merged. In this case the value will return the merge revision. The check in getNodeAtRevision() introduced with OAK-1972 then assumes there may be more recent changes in a previous document and starts to scan the revision history. This scan depends on the number of changes that have been applied on the document since the most recent change on the property in question.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2345_a0dc4c89,"{'BugID': 'OAK-2345', 'Summary': 'Diff reads too many nodes', 'Description': 'DocumentNodeStore.diffManyChildren() may read too many nodes when there is an inactive cluster node with an old _lastRev on the root document. This is a regression introduced with the fix for OAK-2232.\n\nThe fix assumes an inactive cluster node does not have a revision range with an old revision seen at a current timestamp. The DocumentNodeStore will in fact purge revisions from the range in the RevisionComparator after an hour. But on startup the first background read may populate the RevisionComparator with a revision, which is potentially very old (e.g. if the clusterId is not used anymore). '}"
jackrabbit-oak,bugs-dot-jar_OAK-2355_74f22886,"{'BugID': 'OAK-2355', 'Summary': 'TarMK Cold Standby expose standby read timeout value', 'Description': ""Running into a read timeout on the standby instance logs some uncaught error:\n{code}\norg.apache.jackrabbit.oak.plugins.segment.standby.client.SegmentLoaderHandler Exception caught, closing channel.\nio.netty.handler.timeout.ReadTimeoutException: null\n{code}\n\nI'm not sure how/if I need to fix this, the sync process will pickup again, but we can expose the timeout value, so if the network connection is known to be poor, a client can increase the timeout to work around this issue.\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-2359_b3071839,"{'BugID': 'OAK-2359', 'Summary': 'read is inefficient when there are many split documents', 'Description': ""As reported in OAK-2358 there is a potential problem with revisionGC not cleaning up split documents properly (in 1.0.8.r1644758 at least). \n\nAs a side-effect, having many garbage-revisions renders the diffImpl algorithm to become very slow - normally it would take only a few millis, but with nodes that have many split-documents I can see diffImpl take hundres of millis, sometimes up to a few seconds. Which causes the observation dequeuing to be slower than the rate in which observation events are enqueued, which results in observation queue never being cleaned up and event handling being delayed more and more.\n\nAdding some logging showed that diffImpl would often read many split-documents, which supports the assumption that the revisionGC not cleaning up revisions has the diffImpl-slowness as a side-effect. Having said that - diffImpl should probably still be able to run fast, since all the revisions it should look at should be in the main document, not in split documents.\n\nI dont have a test case handy for this at the moment unfortunately - if more is coming up, I'll add more details here.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2363_90ea7aa5,"{'BugID': 'OAK-2363', 'Summary': 'NPE in DocumentNodeStore#retrieve for non existing checkpoint', 'Description': 'Said method throws a NPE when passing it a valid revision identifier from a non existing checkpoint. '}"
jackrabbit-oak,bugs-dot-jar_OAK-2388_487de751,"{'BugID': 'OAK-2388', 'Summary': 'Possibility of overflow in file length calculation', 'Description': 'In OakDirectory the length of a file is calculated in following way\n\n{code:title=OakDirectory|linenumbers=true}\n        public OakIndexFile(String name, NodeBuilder file) {\n            ...\n            this.blobSize = determineBlobSize(file);\n            this.blob = new byte[blobSize];\n\n            PropertyState property = file.getProperty(JCR_DATA);\n            if (property != null && property.getType() == BINARIES) {\n                this.data = newArrayList(property.getValue(BINARIES));\n            } else {\n                this.data = newArrayList();\n            }\n\n            this.length = data.size() * blobSize;\n            if (!data.isEmpty()) {\n                Blob last = data.get(data.size() - 1);\n                this.length -= blobSize - last.length();\n            }\n{code}\n\nIn above calculation its possible to have an overflow in\n\nbq. this.length = data.size() * blobSize;\n\nAs multiplication of two integers result in an integer [1]\n\n[1] http://stackoverflow.com/questions/12861893/casting-result-of-multiplication-two-positive-integers-to-long-is-negative-value'}"
jackrabbit-oak,bugs-dot-jar_OAK-2389_0fa892b3,"{'BugID': 'OAK-2389', 'Summary': 'issues with JsopBuilder.encode and .escape', 'Description': '1) escape() escapes many characters that do not need to be escaped (>127)\n\n2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2389_7c320b1e,"{'BugID': 'OAK-2389', 'Summary': 'issues with JsopBuilder.encode and .escape', 'Description': '1) escape() escapes many characters that do not need to be escaped (>127)\n\n2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2389_8079f7b5,"{'BugID': 'OAK-2389', 'Summary': 'issues with JsopBuilder.encode and .escape', 'Description': '1) escape() escapes many characters that do not need to be escaped (>127)\n\n2) encode() does not encode many control characters that would need to be escaped when read through a JSON parser.'}"
jackrabbit-oak,bugs-dot-jar_OAK-238_24ce6788,"{'BugID': 'OAK-238', 'Summary': 'ValueFactory: Missing identifier validation when creating (weak)reference value from String', 'Description': ""the JCR specification mandates validation of the identifier during\nvalue conversion from STRING to REFERENCE or WEAK_REFERENCE:\n\n<quote from 3.6.4.1 From STRING To)>\nREFERENCE or WEAKREFERENCE: If the string is a syntactically valid \nidentifier, according to the implementation, it is converted directly, otherwise a \nValueFormatException is thrown. The identifier is not required to be that of an \nexisting node in the current workspace. \n<end_quote>\n\nthe current ValueFactory implementation in oak-jcr lacks that validation:\ncreating a REFERENCE or WEAKREFERENCE value using\nValueFactory#createValue(String, int) succeeds even if the specified string\nisn't a valid referenceable node identifier.\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-2418_039f892d,"{'BugID': 'OAK-2418', 'Summary': 'int overflow with orderby causing huge slowdown', 'Description': ""Consider the following query:\n{code}\n//element(*,slingevent:Job) order by @slingevent:created ascending\n{code}\nthis query - when running with a large number of slingevent:Job around - will take a very long time due to the fact, that FilterIterators.SortIterator.init() in the following loop:\n{code}\nif (list.size() > max * 2) {\n  // remove tail entries right now, to save memory\n  Collections.sort(list, orderBy);\n  keepFirst(list, max);\n}\n{code}\ndoes a multiplication with 'max', which is by default set to Integer.MAX_VALUE (see FilterIterators.newCombinedFilter). This results in max *2 to overflow (result is -2) - thus that init-loop will sort the list for every additional entry. Which is definitely not the intention.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2420_24cb1908,"{'BugID': 'OAK-2420', 'Summary': 'DocumentNodeStore revision GC may lead to NPE', 'Description': 'The DocumentNodeStore revision GC may cause a NPE in a reader thread when the GC deletes documents currently accessed by the reader. The {{docChildrenCache}} is invalidated in {{VersionGarbageCollector.collectDeletedDocuments()}} after documents are removed in the DocumentStore. The NPE may occur if removed documents are access in between.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2426_920f32d0,"{'BugID': 'OAK-2426', 'Summary': '[LucenePropertyIndex] full-text search on first level relative node returns no result', 'Description': 'Following query does not return any result even with a proper index defined [1].\n{noformat}//element(*, test:Page)[ "" +\n            ""jcr:contains(jcr:content, \'summer\') ]\n{noformat}\n\n[1]\n{code}\n{\n  ""jcr:primaryType"": ""oak:QueryIndexDefinition"",\n  ""compatVersion"": 2,\n  ""name"": ""pageIndex"",\n  ""type"": ""lucene"",\n  ""async"": ""async"",\n  ""reindex"": true,\n  ""aggregates"": {\n    ""jcr:primaryType"": ""nt:unstructured"",\n    ""test:Page"": {\n      ""jcr:primaryType"": ""nt:unstructured"",\n      ""include0"": {\n        ""jcr:primaryType"": ""nt:unstructured"",\n        ""relativeNode"": true,\n        ""path"": ""jcr:content""\n      }\n    }\n  },\n  ""indexRules"": {\n    ""jcr:primaryType"": ""nt:unstructured"",\n    ""test:Page"": {\n      ""jcr:primaryType"": ""nt:unstructured"",\n      ""properties"": {\n        ""jcr:primaryType"": ""nt:unstructured"",\n        ""jcr:lastModified"": {\n          ""jcr:primaryType"": ""nt:unstructured"",\n          ""ordered"": true,\n          ""propertyIndex"": true,\n          ""name"": ""jcr:content/jcr:lastModified"",\n          ""type"": ""Date""\n        }\n      }\n    }\n  }\n}\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-2427_e6d4f9a6,"{'BugID': 'OAK-2427', 'Summary': 'XPath to SQL-2 conversion fails due to escaping error', 'Description': 'The problem is that the comment is not properly escaped (a C-style comment), so that ""*/"" in the XPath query accidentally ends the comment in the SQL-2 query.\n\nThe following query can\'t be converted to SQL-2, because it contains ""*/"":\n\n{noformat}\n/jcr:root/etc//*[@type = \'product\' \nand ((@size = \'M\' or */@size= \'M\' or */*/@size = \'M\' \nor */*/*/@size = \'M\' or */*/*/*/@size = \'M\' or */*/*/*/*/@size = \'M\'))]\n{noformat}\n\nI think this was introduced by OAK-2354 \nhttp://svn.apache.org/viewvc?view=revision&amp;revision=1645616\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-2430_be3a9114,"{'BugID': 'OAK-2430', 'Summary': 'TARMK Cold Standby size increase due to checkpoints copy', 'Description': 'The current sync design gets confused by existing checkpoints and tries to copy them by value, bypassing the current storage optimization where there are a lot of references to existing content. this can result in a considerable size increase on the standby.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2433_7fca85bf,"{'BugID': 'OAK-2433', 'Summary': 'IllegalStateException for ValueMap on _revisions', 'Description': 'An IllegalStateException may be thrown by the MergeSortedIterator when _revisions on the root document are read with the ValueMap implementation. It only happens when the local _revisions map has entries that are lower than the most recent split document.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2434_8159fc21,"{'BugID': 'OAK-2434', 'Summary': 'Lucene AND query with a complex OR phrase returns incorrect result ', 'Description': 'Queries like this {noformat}/jcr:root/content//element(*, test:Asset)[(jcr:contains(., \'cube\')) and (jcr:contains(jcr:content/@foo, \'""a"" OR ""b""\'))]\n{noformat} returns wrong results.\n\nThis get converted to {noformat}+:fulltext:cube full:jcr:content/foo:""a"" full:jcr:content/foo:""b""\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-2435_7e250001,"{'BugID': 'OAK-2435', 'Summary': 'UpdateOp.Key.equals() incorrect', 'Description': 'As reported on the dev list [0], the equals implementation of UpdateOp.Key is incorrect.\n\n[0] http://markmail.org/message/acpg2mhbxjn4lglu'}"
jackrabbit-oak,bugs-dot-jar_OAK-2439_beaca1a4,"{'BugID': 'OAK-2439', 'Summary': 'IndexPlanner returning plan for queries involving jcr:score', 'Description': 'Consider a query like \n\n{noformat}\n/jcr:root//element(*, cq:Taggable)[ (@cq:tags = \'geometrixx-outdoors:activity/biking\' or @cq:tags = \'/etc/tags/geometrixx-outdoors/activity/biking\') ] order by @jcr:score descending\n\n{noformat}\n\nAnd a seemingly non related index like\n\n{noformat}\n/oak:index/assetType\n  ...\n  - type = ""lucene""\n  + indexRules\n    + nt:base\n      + properties\n        + assetType\n          - propertyIndex = true\n          - name = ""assetType""\n{noformat}\n\nThen currently {{IndexPlanner}} would return a plan because even when it cannot evaluate any of property restrictions because it thinks it can sort on {{jcr:score}}. This later results in an exception like\n\n{noformat}\n14.01.2015 16:16:35.866 *ERROR* [0:0:0:0:0:0:0:1 [1421248595863] POST /bin/tagcommand HTTP/1.1] org.apache.sling.engine.impl.SlingRequestProcessorImpl service: Uncaught Throwable\njava.lang.IllegalStateException: No query created for filter Filter(query=select [jcr:path], [jcr:score], * from [cq:Taggable] as a where [cq:tags] in(\'geometrixx-outdoors:activity/swimming\', \'/etc/tags/geometrixx-outdoors/activity/swimming\') and isdescendantnode(a, \'/\') order by [jcr:score] desc /* xpath: /jcr:root//element(*, cq:Taggable)[ (@cq:tags = \'geometrixx-outdoors:activity/swimming\' or @cq:tags = \'/etc/tags/geometrixx-outdoors/activity/swimming\') ] order by @jcr:score descending */, path=//*, property=[cq:tags=in(geometrixx-outdoors:activity/swimming, /etc/tags/geometrixx-outdoors/activity/swimming)])\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getQuery(LucenePropertyIndex.java:505)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.access$200(LucenePropertyIndex.java:158)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex$1.loadDocs(LucenePropertyIndex.java:303)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex$1.computeNext(LucenePropertyIndex.java:261)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex$1.computeNext(LucenePropertyIndex.java:253)\n\tat com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-2442_ea7a6199,"{'BugID': 'OAK-2442', 'Summary': 'NoSuchElementException thrown by NodeDocument', 'Description': 'Following error is seen with latest 1.0.9-SNAPSHOT builds on some system\n\n{noformat}\nCaused by: java.util.NoSuchElementException: null\n        at java.util.TreeMap.key(TreeMap.java:1221)\n        at java.util.TreeMap.firstKey(TreeMap.java:285)\n        at java.util.Collections$UnmodifiableSortedMap.firstKey(Collections.java:1549)\n        at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)\n        at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930)\n{noformat}\n\nMost likely the above occurs because a {{TreeMap}} associated with some key in NodeDocument is empty.\n\n{noformat}\n23.01.2015 01:57:23.308 *WARN* [pool-11-thread-5]org.apache.jackrabbit.oak.plugins.observation.NodeObserver Error whiledispatching observation eventscom.google.common.util.concurrent.UncheckedExecutionException:com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getChildren(DocumentNodeStore.java:731)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffImpl(DocumentNodeStore.java:1666)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.access$200(DocumentNodeStore.java:105)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$7.call(DocumentNodeStore.java:1260)\n        at org.apache.jackrabbit.oak.plugins.document.MongoDiffCache.getChanges(MongoDiffCache.java:88)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.diffChildren(DocumentNodeStore.java:1255)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeState.compareAgainstBaseState(DocumentNodeState.java:260)\n        at org.apache.jackrabbit.oak.plugins.observation.EventGenerator$Continuation.run(EventGenerator.java:172)\n        at org.apache.jackrabbit.oak.plugins.observation.EventGenerator.generate(EventGenerator.java:118)\n        at org.apache.jackrabbit.oak.plugins.observation.NodeObserver.contentChanged(NodeObserver.java:156)\n        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:117)\n        at org.apache.jackrabbit.oak.spi.commit.BackgroundObserver$1$1.call(BackgroundObserver.java:111)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: com.google.common.util.concurrent.UncheckedExecutionException:java.util.NoSuchElementException\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2199)\n        at com.google.common.cache.LocalCache.get(LocalCache.java:3932)\n        at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4721)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.getNode(DocumentNodeStore.java:704)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readChildren(DocumentNodeStore.java:786)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$4.call(DocumentNodeStore.java:734)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$4.call(DocumentNodeStore.java:731)\n        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)        ... 18 common frames omitted\nCaused by: java.util.NoSuchElementException: null\n        at java.util.TreeMap.key(TreeMap.java:1221)\n        at java.util.TreeMap.firstKey(TreeMap.java:285)\n        at java.util.Collections$UnmodifiableSortedMap.firstKey(Collections.java:1549)\n        at com.google.common.collect.ForwardingSortedMap.firstKey(ForwardingSortedMap.java:73)\n        at org.apache.jackrabbit.oak.plugins.document.NodeDocument.getNodeAtRevision(NodeDocument.java:819)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.readNode(DocumentNodeStore.java:930)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$3.call(DocumentNodeStore.java:707)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore$3.call(DocumentNodeStore.java:704)\n        at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4724)\n        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3522)\n        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2315)\n        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2278)\n        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2193)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-2465_60186813,"{'BugID': 'OAK-2465', 'Summary': 'Long overflow in PermissionEntryProviderImpl', 'Description': 'PermissionEntryProviderImpl#init can end up in a Long overflow if the underlying implementation does not know the exact value of the number children, and the child node count is higher than maxSize.\n\nI will attach a patch with a test case'}"
jackrabbit-oak,bugs-dot-jar_OAK-2524_977a31d8,"{'BugID': 'OAK-2524', 'Summary': 'Error while configuring analyzer by composition', 'Description': 'Error while creating analyzer by composition from osgi due to an illegal argument {{jcr:primaryType}} passed to {{TokenizerFactory.forName(clazz, args)}} in {{NodeStateAnalyzerFactory.loadTokenizer()}}\n\n{noformat}\nCaused by: java.lang.IllegalArgumentException: Unknown parameters: {jcr:primaryType=nt:unstructured}\n\tat org.apache.lucene.analysis.core.LowerCaseFilterFactory.<init>(LowerCaseFilterFactory.java:45)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-2528_239de7b8,"{'BugID': 'OAK-2528', 'Summary': 'Entries in _commitRoot not purged', 'Description': 'Entries in _commitRoot are not purged or moved to previous documents if there are no changes with those revisions. Usually there is always a change associated with a _commitRoot, but in some cases it may happen that the only update on the document is for non-revisioned data like the _children flag.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2559_dfa87520,"{'BugID': 'OAK-2559', 'Summary': 'Lucene index rules should be case insensitive', 'Description': 'Following the lucene index definitions update, the ignored properties are upgraded as a lower case version, but the rest of the lucene bits (indexing) still take the case into account, resulting in the exclude rules being ignored, and properties being indexed.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2603_77d2d3b0,"{'BugID': 'OAK-2603', 'Summary': 'Failure in one of the batch in VersionGC might lead to orphaned nodes', 'Description': 'VersionGC logic currently performs deletion of nodes in batches. For GC to work properly NodeDocument should always be removed in bottom-up mode i.e. parent node should be removed *after* child has been removed\n\nCurrently the GC logic deletes the NodeDocument in undefined order. In such mode if one of the batch fails then its possible that parent might have got deleted but the child was not deleted. \n\nNow in next run the child node would not be recognized as a deleted node because the commit root would not be found.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2642_36fe017c,"{'BugID': 'OAK-2642', 'Summary': 'DocumentNodeStore.dispose() may leave repository in an inconsistent state', 'Description': 'The repository may become inconsistent when a commit happens while the DocumentNodeStore is disposed. \n\nThe node store writes back pending _lastRevs and then unset the active flag in the clusterNodes collection. It is possible a commit gets through even after the _lastRevs had been updated and the active flag is cleared. This means the missing _lastRev updates will not be recovered on a restart or by another cluster node.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2649_72d24f4b,"{'BugID': 'OAK-2649', 'Summary': 'IndexCopier might create empty files in case of error occuring while copying', 'Description': 'On some of the setups following logs are seen\n{noformat}\nerror.log:12.03.2015 03:53:59.785 *WARN* [pool-5-thread-90] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2uv.cfs in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/e5a943cdec3000bd8ce54924fd2070ab5d1d35b9ecf530963a3583d43bf28293/1 but size of local 0 differs from remote 1070972. Content would be read from remote file only\nerror.log:12.03.2015 03:54:02.883 *WARN* [pool-5-thread-125] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rr.si in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 240. Content would be read from remote file only\nerror.log:12.03.2015 03:54:03.467 *WARN* [pool-5-thread-132] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2ro_3.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 42. Content would be read from remote file only\nerror.log:12.03.2015 03:54:03.737 *WARN* [pool-5-thread-135] org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier Found local copy for _2rm_2.del in MMapDirectory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 lockFactory=NativeFSLockFactory@/mnt/installation/crx-quickstart/repository/index/43b36b107f8ce7e162c15b22508aa457ff6ae0083ed3e12d14a7dab67f886def/1 but size of local 0 differs from remote 35. Content would be read from remote file only\n{noformat}\n\nThey indicate that copier has created files of size 0. Looking at the code flow this can happen in case while starting copying some error occurs in between. {{org.apache.lucene.store.Directory#copy}} do take care of removing the file in case of error but that is done only for IOException and not for other cases.\n\nAs a fix the logic should ensure that local file gets deleted if the copy was not successful'}"
jackrabbit-oak,bugs-dot-jar_OAK-2691_d2da7499,"{'BugID': 'OAK-2691', 'Summary': 'Blob GC throws NPE', 'Description': 'Blob GC when registered without a shared data store throws NPE.\nThe {{ClusterRepositoryInfo#getId}} method should check if clusterId is registered or not.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2695_0598498e,"{'BugID': 'OAK-2695', 'Summary': 'DocumentNodeStore.dispatch() may pass null to NodeStateDiff', 'Description': 'This is a regression introduced by OAK-2562. The dispatch method passes a null state if the node does not exist at a given revision.'}"
jackrabbit-oak,bugs-dot-jar_OAK-2740_429baf4d,"{'BugID': 'OAK-2740', 'Summary': 'TreeTypeProvider treats optimized node type definition info as Ac-Content', 'Description': ""while investigating a bug reported by [~teofili] and [~mpetria] that cause group-import with policy node to fail when run with non-administrative session, i found that the {{TreeTypeProvider}} wrongly identifies the optimized item definition information stored with the node types (e.g. {{/jcr:system/jcr:nodeTypes/rep:AccessControllable/rep:namedChildNodeDefinitions/rep:policy}} ) as access control content and thus doesn't read it properly when using a session that doesn't have jcr:readAccessControl privilege at /jcr:system/jcr:nodeTypes.\n\nthe effect of this bug is as follows:\nthe internal calculation of the effective node type and thus item definitions will not work properly for {{rep:policy}} nodes (and similar) as the editing session cannot read the full (oak internal) node type definition as stored below {{/jcr:system/jcr:nodeTypes}}.""}"
jackrabbit-oak,bugs-dot-jar_OAK-276_1bf5c550,"{'BugID': 'OAK-276', 'Summary': ""potential clash of commit id's after restart"", 'Description': ""the commit id's in the current implementation are counter-based, i.e. every commit (on HEAD or on a branch) gets its id by incrementing counter.\n\nonly the current HEAD id is recorded/persisted. on startup the counter is initialized with the current HEAD id. \n\nassume the following sequence:\n\n- ...startup...\n- counter == HEAD == 99\n- commit on HEAD -> new HEAD rev: ++counter == 100\n- create branch -> new branch rev: ++counter == 101\n- ...restart...\n- counter == HEAD == 100\n- commit on HEAD -> new HEAD rev: ++counter == 101 => clashes with older branch rev! \n\nsince a commit is never overwritten the above scenario results in a private branch revision marked as HEAD, i.e. the revision history is corrupted.""}"
jackrabbit-oak,bugs-dot-jar_OAK-278_db19e70f,"{'BugID': 'OAK-278', 'Summary': 'Tree.getStatus() and Tree.getPropertyStatus() fail for items whose parent has been removed', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-2799_3979fa8d,"{'BugID': 'OAK-2799', 'Summary': 'OakIndexInput cloned instances are not closed', 'Description': ""Related to the inspections I was doing for OAK-2798 I also noticed that we don't fully comply with the {{IndexInput}}\xa0javadoc [1]\xa0as the cloned instances should throw the given exception if original is closed, but I also think that the original instance should close the cloned instances, see also [ByteBufferIndexInput#close|https://github.com/apache/lucene-solr/blob/lucene_solr_4_7_1/lucene/core/src/java/org/apache/lucene/store/ByteBufferIndexInput.java#L271].\n\n[1]\xa0: {code}\n/** Abstract base class for input from a file in a {@link Directory}.  A\n * random-access input stream.  Used for all Lucene index input operations.\n *\n * <p>{@code IndexInput} may only be used from one thread, because it is not\n * thread safe (it keeps internal state like file position). To allow\n * multithreaded use, every {@code IndexInput} instance must be cloned before\n * used in another thread. Subclasses must therefore implement {@link #clone()},\n * returning a new {@code IndexInput} which operates on the same underlying\n * resource, but positioned independently. Lucene never closes cloned\n * {@code IndexInput}s, it will only do this on the original one.\n * The original instance must take care that cloned instances throw\n * {@link AlreadyClosedException} when the original one is closed.\n{code}""}"
jackrabbit-oak,bugs-dot-jar_OAK-2864_f51ea2a2,"{'BugID': 'OAK-2864', 'Summary': 'XPath backwards compatibility issue with false() and true()', 'Description': 'In JR2 (actually CRX 2) both of the following queries for nodes with a\nboolean property can be parsed, however only query (a) returns search\nresults.\n{noformat}\n    (a) /jcr:root/test//*[@foo = true()]\n    (b) /jcr:root/test//*[@foo = true]\n{noformat}\n\nOn Oak 1.2, query (a) results in an exception\\[0\\] and query (b) returns\nsearch results.\n\nSee discussion at http://markmail.org/thread/kpews55jpdwm62ds'}"
jackrabbit-oak,bugs-dot-jar_OAK-2929_a2950285,"{'BugID': 'OAK-2929', 'Summary': 'Parent of unseen children must not be removable', 'Description': ""With OAK-2673, it's now possible to have hidden intermediate nodes created concurrently.\nSo, a scenario like:\n{noformat}\nstart -> /:hidden\nN1 creates /:hiddent/parent/node1\nN2 creates /:hidden/parent/node2\n{noformat}\nis allowed.\n\nBut, if N2's creation of {{parent}} got persisted later than that on N1, then N2 is currently able to delete {{parent}} even though there's {{node1}}.""}"
jackrabbit-oak,bugs-dot-jar_OAK-2933_44585b0c,"{'BugID': 'OAK-2933', 'Summary': 'AccessDenied when modifying transiently moved item with too many ACEs', 'Description': ""If at least the following preconditions are fulfilled, saving a moved item fails with access denied:\n\n1. there are more PermissionEntries in the PermissionEntryCache than the configured EagerCacheSize\n2. an node is moved to a location where the user has write access through a group membership\n3. a property is added to the transiently moved item\n\nFor example:\n1. set the *eagerCacheSize* to '0'\n2. create new group *testgroup* and user *testuser*\n3. make *testuser* member of *testgroup*\n4. create nodes {{/testroot/a}} and {{/testroot/a/b}} and {{/testroot/a/c}}\n5. allow *testgroup* {{rep:write}} on {{/testroot/a}}\n6. as *testuser* create {{/testroot/a/b/item}} (to verify that the user has write access)\n7. as *testuser* move {{/testroot/a/b/item}} to {{/testroot/a/c/item}}\n8. {{save()}} -> works\n9. as *testuser* move {{/testroot/a/c/item}} back to {{/testroot/a/b/item}} AND add new property to the transient {{/testroot/a/b/item}}\n10. {{save()}} -> access denied\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-296_5449bf39,"{'BugID': 'OAK-296', 'Summary': 'PathUtils.isAncestor(""/"", ""/"") should return false but returns true', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-2999_3bf07779,"{'BugID': 'OAK-2999', 'Summary': 'Index updation fails on updating multivalued property', 'Description': 'On emptying a multivalued property, fulltext index updation fails and one can search on old values. Following test demonstrates the issue.\nAdded below test in [LuceneIndexQueryTest.java|https://github.com/apache/jackrabbit-oak/blob/trunk/oak-lucene/src/test/java/org/apache/jackrabbit/oak/plugins/index/lucene/LuceneIndexQueryTest.java] which should pass - \n{code}\n    @Test\n    public void testMultiValuedPropUpdate() throws Exception {\n        Tree test = root.getTree(""/"").addChild(""test"");\n        String child = ""child"";\n        String mulValuedProp = ""prop"";\n        test.addChild(child).setProperty(mulValuedProp, of(""foo"",""bar""), Type.STRINGS);\n        root.commit();\n        assertQuery(\n                ""/jcr:root//*[jcr:contains(@"" + mulValuedProp + "", \'foo\')]"",\n                ""xpath"", ImmutableList.of(""/test/"" + child));\n        test.getChild(child).setProperty(mulValuedProp, new ArrayList<String>(), Type.STRINGS);\n        root.commit();\n        assertQuery(\n                ""/jcr:root//*[jcr:contains(@"" + mulValuedProp + "", \'foo\')]"",\n                ""xpath"", new ArrayList<String>());\n\n        test.getChild(child).setProperty(mulValuedProp, of(""bar""), Type.STRINGS);\n        root.commit();\n        assertQuery(\n                ""/jcr:root//*[jcr:contains(@"" + mulValuedProp + "", \'foo\')]"",\n                ""xpath"", new ArrayList<String>());\n\n    }\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3013_eabb4066,"{'BugID': 'OAK-3013', 'Summary': 'SQL2 query with union, limit and offset can return invalid results', 'Description': 'when using order, limit and offset and a SQL2 query that contains an union of two subqueries that have common results can return invalid results\n\nExample: assuming content tree /test/a/b/c/d/e exists\n{code:sql}\nSELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, \'/test\') UNION SELECT [jcr:path] FROM [nt:base] AS a WHERE ISDESCENDANTNODE(a, \'/test\')"" ORDER BY [jcr:path]\n{code}\nwith limit=3 and offset 2 returns only one row ( instead of 3 )\n\nthe correct result set is\n{noformat}\n/test/a/b/c\n/test/a/b/c/d\n/test/a/b/c/d/e\n{noformat}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-3019_5135cf4b,"{'BugID': 'OAK-3019', 'Summary': 'VersionablePathHook must not process hidden nodes', 'Description': 'The VersionablePathHook also processes hidden nodes, e.g. index data, which adds considerable overhead to the merge phase.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3020_147515ae,"{'BugID': 'OAK-3020', 'Summary': 'Async Update fails after IllegalArgumentException', 'Description': 'The async index update can fail due to a mismatch between an index definition and the actual content. If that is the case, it seems that it can no longer make any progress. Instead it re-indexes the latest changes over and over again until it hits the problematic property.\n\nDiscussion at http://markmail.org/thread/42bixzkrkwv4s6tq\n\nStacktrace attached.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3021_494da6de,"{'BugID': 'OAK-3021', 'Summary': 'UserValidator and AccessControlValidator must not process hidden nodes', 'Description': 'This is similar to OAK-3019 but for {{UserValidator}} and {{AccessControlValidator}}.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3028_89317b28,"{'BugID': 'OAK-3028', 'Summary': 'Hierarchy conflict detection broken', 'Description': 'Hierarchy conflict detection is broken in 1.0.14. It may happen that a child document is created even though the parent is considered deleted.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3053_7552a10b,"{'BugID': 'OAK-3053', 'Summary': 'Locking issues seen with CopyOnWrite mode enabled', 'Description': 'When CopyOnWrite mode is enabled and incremental mode is enabled i.e. {{indexPath}} property set then failure in any indexing cycle would prevent further indexing from progressing. For e.g. if any indexing cycle fails then subsequent indexing cycle would fail with Lucene locking related exception\n\n{noformat}\nCaused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/tmp/junit8067118705344013640/2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae/1/write.lock\n\tat org.apache.lucene.store.Lock.obtain(Lock.java:89)\n\tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:707)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.getWriter(LuceneIndexEditorContext.java:169)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.addOrUpdate(LuceneIndexEditor.java:293)\n\t... 37 more\n{noformat}\n\nAny further indexing would continue to fail with this exception'}"
jackrabbit-oak,bugs-dot-jar_OAK-3079_33c18762,"{'BugID': 'OAK-3079', 'Summary': 'LastRevRecoveryAgent can update _lastRev of children but not the root', 'Description': ""As mentioned in [OAK-2131|https://issues.apache.org/jira/browse/OAK-2131?focusedCommentId=14616391&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14616391] there can be a situation wherein the LastRevRecoveryAgent updates some nodes in the tree but not the root. This seems to happen due to OAK-2131's change in the Commit.applyToCache (where paths to update are collected via tracker.track): in that code, paths which are non-root and for which no content has changed (and mind you, a content change includes adding _deleted, which happens by default for nodes with children) are not 'tracked', ie for those the _lastRev is not update by subsequent backgroundUpdate operations - leaving them 'old/out-of-date'. This seems correct as per description/intention of OAK-2131 where the last revision can be determined via the commitRoot of the parent. But it has the effect that the LastRevRecoveryAgent then finds those intermittent nodes to be updated while as the root has already been updated (which is at first glance non-intuitive).\n\nI'll attach a test case to reproduce this.\n\nPerhaps this is a bug, perhaps it's ok. [~mreutegg] wdyt?""}"
jackrabbit-oak,bugs-dot-jar_OAK-3082_29e5b734,"{'BugID': 'OAK-3082', 'Summary': 'Redundent entries in effective policies per principal-set', 'Description': 'when retrieving the effective policies for a given set of principals the resulting array of policies contains redundant entries if a given policy contains multiple ACEs for the given set of principals. '}"
jackrabbit-oak,bugs-dot-jar_OAK-3089_ba38c380,"{'BugID': 'OAK-3089', 'Summary': 'LIRS cache: zero size cache causes IllegalArgumentException', 'Description': 'The LIRS cache does not support a zero size cache currently. Such a configuration causes an IllegalArgumentException.\n\nInstead, no exception should be thrown, and no or a minimum size cache should be used.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3099_25850476,"{'BugID': 'OAK-3099', 'Summary': 'Revision GC fails when split documents with very long paths are present', 'Description': 'My company is using the MongoDB microkernel with Oak, and we\'ve noticed that the daily revision GC is failing with errors like this:\n{code}\n13.07.2015 13:06:16.261 *ERROR* [pool-7-thread-1-Maintenance Queue(com/adobe/granite/maintenance/job/RevisionCleanupTask)] org.apache.jackrabbit.oak.management.ManagementOperation Revision garbage collection failed\njava.lang.IllegalArgumentException: 13:h113f9d0fe7ac0f87fa06397c37b9ffd4b372eeb1ec93e0818bb4024a32587820\nat org.apache.jackrabbit.oak.plugins.document.Revision.fromString(Revision.java:236)\nat org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:84)\nat org.apache.jackrabbit.oak.plugins.document.SplitDocumentCleanUp.disconnect(SplitDocumentCleanUp.java:56)\nat org.apache.jackrabbit.oak.plugins.document.VersionGCSupport.deleteSplitDocuments(VersionGCSupport.java:53)\nat org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.collectSplitDocuments(VersionGarbageCollector.java:117)\nat org.apache.jackrabbit.oak.plugins.document.VersionGarbageCollector.gc(VersionGarbageCollector.java:105)\nat org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreService$2.run(DocumentNodeStoreService.java:511)\nat org.apache.jackrabbit.oak.spi.state.RevisionGC$1.call(RevisionGC.java:68)\nat org.apache.jackrabbit.oak.spi.state.RevisionGC$1.call(RevisionGC.java:64)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\n{code}\n\nI\'ve narrowed the issue down to the disconnect(NodeDocument) method of the [SplitDocumentCleanUp class|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/SplitDocumentCleanUp.java]. The method always tries to extract the path of the node from its ID, but this won\'t work for documents whose path is very long because those documents will have the hash of their path in the ID.\n\nI believe this code should fix the issue, but I haven\'t had a chance to actually try it:\n{code}\n    private void disconnect(NodeDocument splitDoc) {\n        String mainId = Utils.getIdFromPath(splitDoc.getMainPath());\n        NodeDocument doc = store.find(NODES, mainId);\n        if (doc == null) {\n            LOG.warn(""Main document {} already removed. Split document is {}"",\n                    mainId, splitId);\n            return;\n        }\n        String path = splitDoc.getPath();\n        int slashIdx = path.lastIndexOf(\'/\');\n        int height = Integer.parseInt(path.substring(slashIdx + 1));\n        Revision rev = Revision.fromString(\n                path.substring(path.lastIndexOf(\'/\', slashIdx - 1) + 1, slashIdx));\n        doc = doc.findPrevReferencingDoc(rev, height);\n        if (doc == null) {\n            LOG.warn(""Split document {} not referenced anymore. Main document is {}"",\n                    splitId, mainId);\n            return;\n        }\n        // remove reference\n        if (doc.getSplitDocType() == INTERMEDIATE) {\n            disconnectFromIntermediate(doc, rev);\n        } else {\n            markStaleOnMain(doc, rev, height);\n        }\n    }\n{code}\nBy using getPath(), the code should automatically use either the ID or the _path property, whichever is right for the document.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3104_38f5ef13,"{'BugID': 'OAK-3104', 'Summary': ""Version garbage collector doesn't collect a rolled back document if it was never deleted"", 'Description': 'If a commit gets rolled back it can leave (in case the document was never deleted explicitly) a document in a state like:\n{noformat}\n{\n       ""_id"" : ""7:/etc/workflow/packages/2014/10/12/rep:ours"",\n       ""_deleted"" : {\n\n       },\n       ""_commitRoot"" : {\n\n       },\n       ""jcr:primaryType"" : {\n\n       },\n       ""_modified"" : NumberLong(1413126245),\n       ""_children"" : true,\n       ""_modCount"" : NumberLong(2)\n}\n{noformat}\n\nIf the path is fairly busy, the document can get created naturally later and then follow the usual cycle. But, at times, such documents are ephemeral in nature and never re-used. In those cases, such documents can remain silently without getting collected.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3105_311e8b33,"{'BugID': 'OAK-3105', 'Summary': ""SegmentWriter doesn't properly check the length of external blob IDs"", 'Description': 'To store the length field of an external binary ID, the following encoding is used:\n\n{noformat}\n1110 + 4bit + 8bit\n{noformat}\n\nwhich allows to store numbers between 0 and 2{^}12^ - 1. \n\nThe current implementation of {{SegmentWriter}} allows the length of binary IDs to range between 0 and 2{^}13^ - 1, writing incorrect data when the length of the binary ID ranges from 2{^}12^ to 2{^}13^ - 1.\n\nWhen reading this incorrect data back, an {{IllegalStateException}} is thrown complaining that the first byte of the length fields has an unexpected value record type. See OAK-1842 for an example.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3110_d10362c0,"{'BugID': 'OAK-3110', 'Summary': 'AsyncIndexer fails due to FileNotFoundException thrown by CopyOnWrite logic', 'Description': 'At times the CopyOnWrite reports following exception\n\n{noformat}\n15.07.2015 14:20:35.930 *WARN* [pool-58-thread-1] org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate The async index update failed\norg.apache.jackrabbit.oak.api.CommitFailedException: OakLucene0004: Failed to close the Lucene index\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:204)\n\tat org.apache.jackrabbit.oak.plugins.index.IndexUpdate.leave(IndexUpdate.java:219)\n\tat org.apache.jackrabbit.oak.spi.commit.VisibleEditor.leave(VisibleEditor.java:63)\n\tat org.apache.jackrabbit.oak.spi.commit.EditorDiff.process(EditorDiff.java:56)\n\tat org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.updateIndex(AsyncIndexUpdate.java:366)\n\tat org.apache.jackrabbit.oak.plugins.index.AsyncIndexUpdate.run(AsyncIndexUpdate.java:311)\n\tat org.apache.sling.commons.scheduler.impl.QuartzJobExecutor.execute(QuartzJobExecutor.java:105)\n\tat org.quartz.core.JobRunShell.run(JobRunShell.java:207)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: _2s7.fdt\n\tat org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:261)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory$COWLocalFileReference.fileLength(IndexCopier.java:837)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier$CopyOnWriteDirectory.fileLength(IndexCopier.java:607)\n\tat org.apache.lucene.index.SegmentCommitInfo.sizeInBytes(SegmentCommitInfo.java:141)\n\tat org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:529)\n\tat org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)\n\tat org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:508)\n\tat org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:618)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3147)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3123)\n\tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:988)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:932)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:894)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditorContext.closeWriter(LuceneIndexEditorContext.java:192)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexEditor.leave(LuceneIndexEditor.java:202)\n\t... 10 common frames omitted\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3123_f3c9c818,"{'BugID': 'OAK-3123', 'Summary': 'NPE in RecordIdMap', 'Description': '{{RecordIdMap}} is not properly guarded against NPEs when calling accessors on an empty map (which is represented by {{keys == null}}. \n\n{noformat}\ntestRecordIdMap(org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest)  Time elapsed: 0.019 sec  <<< ERROR!\njava.lang.NullPointerException\nat org.apache.jackrabbit.oak.plugins.segment.RecordIdMap.size(RecordIdMap.java:100)\nat org.apache.jackrabbit.oak.plugins.segment.RecordIdMapTest.testRecordIdMap(RecordIdMapTest.java:64)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3137_c65b07c3,"{'BugID': 'OAK-3137', 'Summary': 'Global fulltext index returning plan for pure NodeType queries', 'Description': 'On a system having\n\n# Global fulltext index enabled with version V2 and {{evaluatePathRestriction}} enabled\n# NodeType index having indexing enabled for specific nodetype like cq:ClientLibraryFolder\n\nA query like\n\n{noformat}\n/jcr:root//element(*, cq:ClientLibraryFolder)\n{noformat}\n\nEnds up getting evaluated by fulltext index as it return plan with include all query\n\n*Expected*\nFor such query global fulltext index should not return any plan if the path restriction is on root path with include all children'}"
jackrabbit-oak,bugs-dot-jar_OAK-313_e115fd90,"{'BugID': 'OAK-313', 'Summary': 'Trailing slash not removed for simple path in JCR to Oak path conversion', 'Description': 'While converting from JCR path to Oak path the trailing slashes are not removed for simple paths. They are removed for complex path\n\n{code}\nassertEquals(""/oak-foo:bar/oak-quu:qux"",npMapper.getOakPath(""/foo:bar/quu:qux/""));\nassertEquals(""/a/b/c"",npMapper.getOakPath(""/a/b/c/""));\n    }\n{code}\n\nOf the two cases above the first one passes while the second one fails\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-3156_786b3d76,"{'BugID': 'OAK-3156', 'Summary': ""Lucene suggestions index definition can't be restricted to a specific type of node"", 'Description': 'While performing a suggestor query like \n\n{code}\nSELECT [rep:suggest()] as suggestion  FROM [nt:unstructured] WHERE suggest(\'foo\')\n{code}\n\nSuggestor does not provide any result. In current implementation, [suggestions|http://jackrabbit.apache.org/oak/docs/query/lucene.html#Suggestions] in Oak work only for index definitions for {{nt:base}} nodetype.\nSo, an index definition like:\n{code:xml}\n    <lucene-suggest\n        jcr:primaryType=""oak:QueryIndexDefinition""\n        async=""async""\n        compatVersion=""{Long}2""\n        type=""lucene"">\n        <indexRules jcr:primaryType=""nt:unstructured"">\n            <nt:base jcr:primaryType=""nt:unstructured"">\n                <properties jcr:primaryType=""nt:unstructured"">\n                    <description\n                        jcr:primaryType=""nt:unstructured""\n                        analyzed=""{Boolean}true""\n                        name=""description""\n                        propertyIndex=""{Boolean}true""\n                        useInSuggest=""{Boolean}true""/>\n                </properties>\n            </nt:base>\n        </indexRules>\n    </lucene-suggest>\n{code}\nworks, but if we change nodetype to {{nt:unstructured}} like:\n{code:xml}\n    <lucene-suggest\n        jcr:primaryType=""oak:QueryIndexDefinition""\n        async=""async""\n        compatVersion=""{Long}2""\n        type=""lucene"">\n        <indexRules jcr:primaryType=""nt:unstructured"">\n            <nt:unstructured jcr:primaryType=""nt:unstructured"">\n                <properties jcr:primaryType=""nt:unstructured"">\n                    <description\n                        jcr:primaryType=""nt:unstructured""\n                        analyzed=""{Boolean}true""\n                        name=""description""\n                        propertyIndex=""{Boolean}true""\n                        useInSuggest=""{Boolean}true""/>\n                </properties>\n            </nt:base>\n        </indexRules>\n    </lucene-suggest>\n{code}\n, it won\'t work.\n\nThe issue is that suggestor implementation essentially is passing a pseudo row with path=/.:\n{code:title=LucenePropertyIndex.java}\n    private boolean loadDocs() {\n...\n                        queue.add(new LuceneResultRow(suggestedWords));\n...\n{code}\nand\n{code:title=LucenePropertyIndex.java}\n        LuceneResultRow(Iterable<String> suggestWords) {\n            this.path = ""/"";\n            this.score = 1.0d;\n            this.suggestWords = suggestWords;\n        }\n{code}\nDue to path being set to ""/"", {{SelectorImpl}} later filters out the result as {{rep:root}} (primary type of ""/"") isn\'t a {{nt:unstructured}}.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3249_64712735,"{'BugID': 'OAK-3249', 'Summary': 'Some version copy settings conflicts with the earlyShutdown', 'Description': ""The {{RepositoryUpgrade#earlyShutdown}} property causes the source CRX2 repository to shutdown right after copying the content, before the first commit hook is launched. However, the {{VersionableEditor}} hook sometimes needs access to the source repository, to read the version histories that hasn't been copied yet (as the version histories are copied in two stages). As a result, the {{earlyShutdown}} may cause the upgrade process to fail.\n\n{{earlyShutdown}} should be overriden for all cases in which the source repository is still needed in the commit hook phase. In particular, it should be set to {{false}} if:\n\n* orphaned version histories are not copied,\n* orphaned version histories are copied, but the copyOrphanedVersion date is set after the copyVersion date.""}"
jackrabbit-oak,bugs-dot-jar_OAK-3310_4416a9f8,"{'BugID': 'OAK-3310', 'Summary': 'Write operations on Property do not check checked-out state of Node', 'Description': 'Write operations on Property do not check the checked-out state. The same is true for Node.setProperty(..., null).'}"
jackrabbit-oak,bugs-dot-jar_OAK-3318_e12e2052,"{'BugID': 'OAK-3318', 'Summary': 'IndexRule not respecting inheritence based on mixins', 'Description': 'IndexRule are meant to be applied based on both primaryType and minin type based inheritance. Currently it appears that only primaryType based inheritance is working '}"
jackrabbit-oak,bugs-dot-jar_OAK-3324_5f863af6,"{'BugID': 'OAK-3324', 'Summary': 'Evaluation with restriction is not consistent with parent ACLs', 'Description': ""consider the following ACL setup:\n\n{noformat}\ntestuser allow rep:read,rep:write      /testroot\ntestuser deny  jcr:removeNode /testroot/a  glob=*/c\ntestuser allow jcr:removeNode /testroot/a  glob=*/b\n{noformat}\n\nnow: {{hasPermission(/tesroot/a/b/c, jcr:removeNode) == false}} but the user is still able to delete the node.\n\n* if we change the order of the ACEs with the restriction, it works (i.e. the user can't delete)\n* if we use direct ACLs on the respective nodes, it works\n\nI think this is a bug...but I'm not sure if {{hasPermission}} is wrong, or the check during node deletion.\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-3333_194999ed,"{'BugID': 'OAK-3333', 'Summary': 'SplitOperations purges _commitRoot entries too eagerly', 'Description': 'OAK-2528 introduced purging of _commitRoot entries without associated local changes on the document. Those _commitRoot entries are created when a child nodes is added and the _children flag is touched on the parent.\n\nThe purge operation is too eager and removes all such entries, which may result in an undetected hierarchy conflict.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3367_06812d25,"{'BugID': 'OAK-3367', 'Summary': 'Boosting fields not working as expected', 'Description': ""When the boost support was added the intention was to support a usecase like \n\n{quote}\nFor the fulltext search on a node where the fulltext content is derived from multiple field it should be possible to boost specific text contributed by individual field. Meaning that if a title field is boosted more than description, the title (part) in the fulltext field will mean more than the description (part) in the fulltext field.\n{quote}\n\nThis would enable a user to perform a search like _/jcr:root/content/geometrixx-outdoors/en//element(*, cq:Page)\\[jcr:contains(., 'Keyword')\\]_ and get a result where pages having 'Keyword' in title come above in search result compared to those where Keyword is found in description.\n\nCurrent implementation just sets the boost while add the field value to fulltext field with the intention that Lucene would use the boost as explained above. However it does not work like that and boost value gets multiplies with other field and hence boosting does not work as expected""}"
jackrabbit-oak,bugs-dot-jar_OAK-3377_00b9bc52,"{'BugID': 'OAK-3377', 'Summary': 'Two spaces in SQL2 fulltext search -> error', 'Description': 'Execute the following SQL2 query (eg, in crx/de\'s query tool)\nSELECT * FROM [nt:unstructured] AS c WHERE (CONTAINS(c.[jcr:title], \'a  b\') AND ISDESCENDANTNODE(c, \'/content\'))\n(note there are 2 spaces between ""a"" and ""b"")\nResult: java.lang.IllegalArgumentException: Invalid expression: \'a b\'\n\nIf there is only 1 space between a and b, there is no error. \n\nPer jsr-283, fulltext expressions should be able to have strings of whitespace.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3396_c83755c3,"{'BugID': 'OAK-3396', 'Summary': 'NPE during syncAllExternalUsers in LdapIdentityProvider.createUser', 'Description': ""When executing the JMX method syncAllExternalUsers the following NPE has been encountered. This likely indicates that - for a particular user - there is no attribute '{{uid}}':\n\n{code}\njava.lang.NullPointerException\nat org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.createUser(LdapIdentityProvider.java:667)\nat org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider.access$000(LdapIdentityProvider.java:88)\nat org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider$1.getNext(LdapIdentityProvider.java:281)\nat org.apache.jackrabbit.oak.security.authentication.ldap.impl.LdapIdentityProvider$1.getNext(LdapIdentityProvider.java:273)\nat org.apache.jackrabbit.commons.iterator.AbstractLazyIterator.hasNext(AbstractLazyIterator.java:39)\nat org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl$Delegatee.syncAllExternalUsers(SyncMBeanImpl.java:245)\nat org.apache.jackrabbit.oak.spi.security.authentication.external.impl.jmx.SyncMBeanImpl.syncAllExternalUsers(SyncMBeanImpl.java:426)\n{code}""}"
jackrabbit-oak,bugs-dot-jar_OAK-3411_978c77ff,"{'BugID': 'OAK-3411', 'Summary': 'Inconsistent read on DocumentNodeStore startup', 'Description': ""This is a regression introduced with OAK-2929. On DocumentNodeStore startup the RevisionComparator of the local instance is initialized with the current _lastRev entries from the other cluster nodes. The external _lastRev entries are 'seenAt' the same revision, which means for those revisions the RevisionComparator will use the clusterId to compare them. This is also described in OAK-3388.\n\nOAK-2929 changed the sequence of revisions to check for conflicts from StableRevisionComparator to RevisionComparator. This makes the conflict check susceptible to the RevisionComparison behaviour described in OAK-3388. Commits may be rejected with a conflict, when there isn't really a conflict.""}"
jackrabbit-oak,bugs-dot-jar_OAK-3412_2f85bd78,"{'BugID': 'OAK-3412', 'Summary': 'Node name having non space whitspace chars should not be allowed', 'Description': ""Due to the changes done in OAK-1174 node with non space whitespace chars like '\\n', '\\r' etc can be created. This is not desirable and also JR2 does not allow such node to be created so check must be added to prevent such a name from getting created.\n\nAs discussed in [1] this is regression due to usage of incorrect utility method as part of [2] the fix can be simply using a {{Character#isWhitespace}} instead of {{Character#isSpaceChar}}\n\n[1] http://mail-archives.apache.org/mod_mbox/jackrabbit-oak-dev/201509.mbox/%3CCAHCW-mkkGtxkn%2B9xfXuvMTfgykewjMPsLwrVH%2B00%2BXaBQjA0sg%40mail.gmail.com%3E\n[2] https://github.com/apache/jackrabbit-oak/commit/342809f7f04221782ca6bbfbde9392ec4ff441c2""}"
jackrabbit-oak,bugs-dot-jar_OAK-3424_f4349a96,"{'BugID': 'OAK-3424', 'Summary': 'ClusterNodeInfo does not pick an existing entry on startup', 'Description': ""When the {{DocumentNodeStore}} starts up, it attempts to find an entry that matches the current instance (which is defined by something based on network interface address and the current working directory).\n\nHowever, an additional check is done when the cluster lease end time hasn't been reached, in which case the entry is skipped (assuming it belongs to a different instance), and the scan continues. When no other entry is found, a new one is created.\n\nSo why would we *ever* consider instances with matching instance information to be different? As far as I can tell the answer is: for unit testing.\n\nBut...\n\nWith the current assignment very weird things can happen, and I believe I see exactly this happening in a customer problem I'm investigating. The sequence is:\n\n1) First system startup, cluster node id 1 is assigned\n\n2) System crashes or was crashed\n\n3) System restarts within the lease time (120s?), a new cluster node id is assigned\n\n4) System shuts down, and gets restarted after a longer interval: cluster id 1 is used again, and system starts {{MissingLastRevRecovery}}, despite the previous shutdown having been clean\n\nSo what we see is that the system starts up with varying cluster node ids, and recovery processes may run with no correlation to what happened before.\n\nProposal:\n\na) Make {{ClusterNodeInfo.createInstance()}} much more verbose, so that the default system log contains sufficient information to understand why a certain cluster node id was picked.\n\nb) Drop the logic that skips entries with non-expired leases, so that we get a one-to-one relation between instance ids and cluster node ids. For the unit tests that currently rely on this logic, switch to APIs where the test setup picks the cluster node id.\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-3433_b76b31f7,"{'BugID': 'OAK-3433', 'Summary': 'Background update may create journal entry with incorrect id', 'Description': 'The conflict check does not consider changes that are made visible between the rebase and the background read.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3442_17032c50,"{'BugID': 'OAK-3442', 'Summary': 'Intermittent IllegalMonitorStateException seen while releaseing IndexNode', 'Description': 'At times following exception seen. On this system the index got corrupted because backing index files got deleted from the system and hence index is not accessible. \n\n{noformat}\n21.09.2015 09:26:36.764 *ERROR* [FelixStartLevel] com.adobe.granite.repository.impl.SlingRepositoryManager start: Uncaught Throwable trying to access Repository, calling stopRepository()\njava.lang.IllegalMonitorStateException: attempt to unlock read lock, not locked by current thread\n        at java.util.concurrent.locks.ReentrantReadWriteLock$Sync.unmatchedUnlockException(ReentrantReadWriteLock.java:444)\n        at java.util.concurrent.locks.ReentrantReadWriteLock$Sync.tryReleaseShared(ReentrantReadWriteLock.java:428)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.releaseShared(AbstractQueuedSynchronizer.java:1341)\n        at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.unlock(ReentrantReadWriteLock.java:881)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.release(IndexNode.java:121)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:212)\n        at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:847)\n        at org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:793)\n        at org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:283)\n        at org.apache.jackrabbit.oak.query.QueryImpl.prepare(QueryImpl.java:568)\n        at org.apache.jackrabbit.oak.query.QueryEngineImpl.executeQuery(QueryEngineImpl.java:183)\n        at org.apache.jackrabbit.oak.security.user.UserProvider.getAuthorizableByPrincipal(UserProvider.java:234)\n        at org.apache.jackrabbit.oak.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:116)\n        at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getAuthorizable(PrincipalProviderImpl.java:140)\n        at org.apache.jackrabbit.oak.security.principal.PrincipalProviderImpl.getPrincipal(PrincipalProviderImpl.java:69)\n        at org.apache.jackrabbit.oak.spi.security.principal.CompositePrincipalProvider.getPrincipal(CompositePrincipalProvider.java:50)\n        at org.apache.jackrabbit.oak.spi.security.principal.PrincipalManagerImpl.getPrincipal(PrincipalManagerImpl.java:47)\n        at com.adobe.granite.repository.impl.SlingRepositoryManager.setupPermissions(SlingRepositoryManager.java:997)\n        at com.adobe.granite.repository.impl.SlingRepositoryManager.createRepository(SlingRepositoryManager.java:420)\n        at com.adobe.granite.repository.impl.SlingRepositoryManager.acquireRepository(SlingRepositoryManager.java:290)\n        at org.apache.sling.jcr.base.AbstractSlingRepositoryManager.start(AbstractSlingRepositoryManager.java:304)\n        at com.adobe.granite.repository.impl.SlingRepositoryManager.activate(SlingRepositoryManager.java:267)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:483)\n        at org.apache.felix.scr.impl.helper.BaseMethod.invokeMethod(BaseMethod.java:222)\n        at org.apache.felix.scr.impl.helper.BaseMethod.access$500(BaseMethod.java:37)\n        at org.apache.felix.scr.impl.helper.BaseMethod$Resolved.invoke(BaseMethod.java:615)\n        at org.apache.felix.scr.impl.helper.BaseMethod.invoke(BaseMethod.java:499)\n        at org.apache.felix.scr.impl.helper.ActivateMethod.invoke(ActivateMethod.java:295)\n        at org.apache.felix.scr.impl.manager.SingleComponentManager.createImplementationObject(SingleComponentManager.java:302)\n        at org.apache.felix.scr.impl.manager.SingleComponentManager.createComponent(SingleComponentManager.java:113)\n        at org.apache.felix.scr.impl.manager.SingleComponentManager.getService(SingleComponentManager.java:832)\n        at org.apache.felix.scr.impl.manager.SingleComponentManager.getServiceInternal(SingleComponentManager.java:799)\n        at org.apache.felix.scr.impl.manager.AbstractComponentManager.activateInternal(AbstractComponentManager.java:724)\n        at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:927)\n        at org.apache.felix.scr.impl.manager.DependencyManager$SingleStaticCustomizer.addedService(DependencyManager.java:891)\n        at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1492)\n        at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.customizerAdded(ServiceTracker.java:1413)\n        at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.trackAdding(ServiceTracker.java:1222)\n        at org.apache.felix.scr.impl.manager.ServiceTracker$AbstractTracked.track(ServiceTracker.java:1158)\n        at org.apache.felix.scr.impl.manager.ServiceTracker$Tracked.serviceChanged(ServiceTracker.java:1444)\n        at org.apache.felix.framework.util.EventDispatcher.invokeServiceListenerCallback(EventDispatcher.java:987)\n        at org.apache.felix.framework.util.EventDispatcher.fireEventImmediately(EventDispatcher.java:838)\n        at org.apache.felix.framework.util.EventDispatcher.fireServiceEvent(EventDispatcher.java:545)\n        at org.apache.felix.framework.Felix.fireServiceEvent(Felix.java:4547)\n        at org.apache.felix.framework.Felix.registerService(Felix.java:3521)\n        at org.apache.felix.framework.BundleContextImpl.registerService(BundleContextImpl.java:348)\n        at org.apache.sling.commons.threads.impl.Activator.start(Activator.java:55)\n        at org.apache.felix.framework.util.SecureAction.startActivator(SecureAction.java:697)\n        at org.apache.felix.framework.Felix.activateBundle(Felix.java:2223)\n        at org.apache.felix.framework.Felix.startBundle(Felix.java:2141)\n        at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1368)\n        at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)\n        at java.lang.Thread.run(Thread.java:745)\n{noformat}\n\nAbove exception happens at\n\n{code}\nfor (String path : indexPaths) {\n            try {\n                indexNode = tracker.acquireIndexNode(path);\n\n                if (indexNode != null) {\n                    IndexPlan plan = new IndexPlanner(indexNode, path, filter, sortOrder).getPlan();\n                    if (plan != null) {\n                        plans.add(plan);\n                    }\n                }\n            } finally {\n                if (indexNode != null) {\n                    indexNode.release();\n                }\n            }\n        }\n{code}\n\nIt has been ensured that if indexNode is initialized then it has been acquired. So only way for such an exception to happen is that in a loop of say 2 paths {{indexNode}} got initialized for Loop 1 and then while acquiring in Loop 2 the indexNode still refers to old released value and that would cause the exception. The fix should be simply to null the variable once released'}"
jackrabbit-oak,bugs-dot-jar_OAK-3474_ff81ef72,"{'BugID': 'OAK-3474', 'Summary': ""NodeDocument.getNodeAtRevision can go into property history traversal when latest rev on current doc isn't committed"", 'Description': ""{{NodeDocument.getNodeAtRevision}} tried to look at latest revisions entries for each property in current document. But it just looks at the *last* entry for a given property. In case this last entry isn't committed, the code would go into previous documents to look for a committed value.\n\n(cc [~mreutegg])""}"
jackrabbit-oak,bugs-dot-jar_OAK-3510_01f5a26f,"{'BugID': 'OAK-3510', 'Summary': 'Troublesome ExternalIdentityRef.equals(Object) implementation', 'Description': ""in the light of OAK-3508 i looked at the {{ExternalIdentifyRef}} class and found the following implementation of {{Object.equals(Object)}}:\n\n{code}\npublic boolean equals(Object o) {\n        try {\n            // assuming that we never compare other types of classes\n            return this == o || string.equals(((ExternalIdentityRef) o).string);\n        } catch (Exception e) {\n            return false;\n        }\n    }\n{code}\n\nsince this class is public and exported as part of a public API, i don't think the assumption made in the code is justified. also i would argue that catching {{Exception}} is bad style as is exception driven development. in this particular case it was IMHO perfectly trivial to just get rid of the catch clause altogether.""}"
jackrabbit-oak,bugs-dot-jar_OAK-3511_5138a1e2,"{'BugID': 'OAK-3511', 'Summary': 'Test failure: CompactionMapTest.removeSome', 'Description': 'Said test fails sporadically:\n\n{noformat}\nat org.junit.Assert.assertNull(Assert.java:562)\nat org.apache.jackrabbit.oak.plugins.segment.CompactionMapTest.removeSome(CompactionMapTest.java:156)\n{noformat}\n\nThis is a regression introduced with OAK-3501: the {{recent}} map gets not cleared when {{segmentIdMap}} is empty. This can happen when a recent key is removed again while there are no other changes. '}"
jackrabbit-oak,bugs-dot-jar_OAK-3517_24f7f60a,"{'BugID': 'OAK-3517', 'Summary': 'Node.addNode(String, String) may check nt-mgt-permission against the wrong node', 'Description': 'While I was troubleshooting an issue we\'re having in AEM 6.1, I\'ve noticed an ""impossible"" access denied exception in the logs: the user had permission to add nodes under the node in question but still got an error.\n\nSome testing narrowed the issue down to a difference in behavior between the following two invocations:\n{{someNode.getNode(""child"").addNode(""grandchild"", ""nt:unstructured"");}}\n{{someNode.addNode(""child/grandchild"", ""nt:unstructured"");}}\n\nAs far as I can tell, both should behave identically per the JCR spec, but the second one fails if the user doesn\'t have node type management permission to someNode, even if they have that permission to someNode/child.\n\nI believe the issue is in line 283 of [NodeImpl|https://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-jcr/src/main/java/org/apache/jackrabbit/oak/jcr/session/NodeImpl.java]: it is checking permissions against dlg.getTree(), but it should really check against parent.getTree(), or if possible, the path of the node that\'s about to be created (so glob restrictions can be evaluated).'}"
jackrabbit-oak,bugs-dot-jar_OAK-3530_4d231938,"{'BugID': 'OAK-3530', 'Summary': 'TreeTypeProvider returns wrong type for version related node type definitions', 'Description': 'the following paths with result in type {{VERSION}} instead of {{DEFAULT}} and might lead to unexpected results wrt read access:\n\n- /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:versionStorage\n- /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:activities\n- /jcr:system/jcr:nodeTypes/rep:system/rep:namedChildNodeDefinitions/jcr:configurations\n       '}"
jackrabbit-oak,bugs-dot-jar_OAK-3549_9772f5b2,"{'BugID': 'OAK-3549', 'Summary': 'Initial read of _lastRev creates incorrect RevisionComparator', 'Description': 'The logic in backgroundRead(false) orders the local lastRev \nbefore external lastRev. This the last change done by the\nlocal cluster node will look as if it happend before a potentially\nolder external change. '}"
jackrabbit-oak,bugs-dot-jar_OAK-3579_2565d74a,"{'BugID': 'OAK-3579', 'Summary': 'BackgroundLeaseUpdate not scheduled when asyncDelay=0', 'Description': 'The BackgroundLeaseUpdate extends from NodeStoreTask, which returns from the run() method when asyncDelay is 0. This is fine for the background read and update tasks. However, the lease update task must run even when asyncDelay is set to zero.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3630_fcd64766,"{'BugID': 'OAK-3630', 'Summary': 'Mixin based rules not working for relative properties', 'Description': 'If an indexing rule is defined for mixin then it does not work as expected for relative properties.\n\nIssue here being that most of logic in Aggregate class (which is used for relative property handling also) relies on nodes primaryType and does not account for mixin type'}"
jackrabbit-oak,bugs-dot-jar_OAK-3634_90ad50da,"{'BugID': 'OAK-3634', 'Summary': 'RDB/MongoDocumentStore may return stale documents', 'Description': 'It appears that the implementations of the {{update}} method sometimes populate the memory cache with documents that do not reflect any current or previous state in the persistence (that is, miss changes made by another node).\n\n(will attach test)'}"
jackrabbit-oak,bugs-dot-jar_OAK-369_4e245a76,"{'BugID': 'OAK-369', 'Summary': 'missing support for relative path consisting of parent-element', 'Description': 'could not reopen OAK-95 -> cloning.\nduring testing of user-mgt api found that relpath consisting of a single parent element doesn\'t work (but used to):\n\n{code}\n@Test\n    public void getNode3() throws RepositoryException {\n        Node node = getNode(""/foo"");\n        Node root = node.getNode("".."");\n        assertNotNull(root);\n        assertEquals("""", root.getName());\n        assertTrue(""/"".equals(root.getPath()));\n    }\n:\n\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3733_a5ff019e,"{'BugID': 'OAK-3733', 'Summary': ""Sometimes hierarchy conflict between concurrent add/delete isn't detected"", 'Description': 'I\'m not sure of exact set of event that led to an incident on one of our test clusters. The cluster is running 3 AEM instances based on oak build at 1.3.10.r1713699 backed by a single mongo 3 instance.\n\nUnfortunately, we found the issue too late and logs had rolled over. Here\'s the exception that showed over and over as workflow jobs were (trying to) being processed:\n{noformat}\n....\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: javax.jcr.InvalidItemStateException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision\nr151233e54e1-0-4, before\nr15166378b6a-0-2 (retries 5, 6830 ms)\n        at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:239)\n        at org.apache.jackrabbit.oak.api.CommitFailedException.asRepositoryException(CommitFailedException.java:212)\n        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.newRepositoryException(SessionDelegate.java:669)\n        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:495)\n        at org.apache.jackrabbit.oak.jcr.session.SessionImpl$8.performVoid(SessionImpl.java:419)\n        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.performVoid(SessionDelegate.java:273)\n        at org.apache.jackrabbit.oak.jcr.session.SessionImpl.save(SessionImpl.java:416)\n        at org.apache.sling.jcr.resource.internal.helper.jcr.JcrResourceProvider.commit(JcrResourceProvider.java:634)\n        ... 16 common frames omitted\nCaused by: org.apache.jackrabbit.oak.api.CommitFailedException: OakMerge0004: OakMerge0004: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision\nr151233e54e1-0-4, before\nr15166378b6a-0-2 (retries 5, 6830 ms)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:200)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge(DocumentNodeStoreBranch.java:123)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentRootBuilder.merge(DocumentRootBuilder.java:158)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStore.merge(DocumentNodeStore.java:1497)\n        at org.apache.jackrabbit.oak.core.MutableRoot.commit(MutableRoot.java:247)\n        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.commit(SessionDelegate.java:346)\n        at org.apache.jackrabbit.oak.jcr.delegate.SessionDelegate.save(SessionDelegate.java:493)\n        ... 20 common frames omitted\nCaused by: org.apache.jackrabbit.oak.plugins.document.ConflictException: The node 8:/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned was already added in revision\nr151233e54e1-0-4, before\nr15166378b6a-0-2\n        at org.apache.jackrabbit.oak.plugins.document.Commit.checkConflicts(Commit.java:582)\n        at org.apache.jackrabbit.oak.plugins.document.Commit.createOrUpdateNode(Commit.java:487)\n        at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:371)\n        at org.apache.jackrabbit.oak.plugins.document.Commit.applyToDocumentStore(Commit.java:265)\n        at org.apache.jackrabbit.oak.plugins.document.Commit.applyInternal(Commit.java:234)\n        at org.apache.jackrabbit.oak.plugins.document.Commit.apply(Commit.java:219)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:290)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.persist(DocumentNodeStoreBranch.java:260)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.access$300(DocumentNodeStoreBranch.java:54)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch$InMemory.merge(DocumentNodeStoreBranch.java:498)\n        at org.apache.jackrabbit.oak.plugins.document.DocumentNodeStoreBranch.merge0(DocumentNodeStoreBranch.java:180)\n        ... 26 common frames omitted\n....\n{noformat}\n\nDoing following removed repo corruption and restored w/f processing:\n{noformat}\noak.removeDescendantsAndSelf(""/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned"")\n{noformat}\n\nAttaching [mongoexport output|^mongoexport.zip] for {{/oak:index/event.job.topic/:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel/var/eventing/jobs/assigned/6a389a6a-a8bf-4038-b57b-cb441c6ac557/com.adobe.granite.workflow.transient.job.etc.workflow.models.dam-xmp-writeback.jcr_content.model/2015/11/19/23/54/6a389a6a-a8bf-4038-b57b-cb441c6ac557_10}} (the hierarchy created at {{r151233e54e1-0-4}}). I\'ve renamed a few path elements to make it more reable though (e.g. {{:index/com%2Fadobe%2Fgranite%2Fworkflow%2Ftransient%2Fjob%2Fetc%2Fworkflow%2Fmodels%2Fdam-xmp-writeback%2Fjcr_content%2Fmodel}} -> {{enc_value}}).\n\n[~mreutegg], I\'m assigning it to myself for now, but I think this would require your expertise all the way :).'}"
jackrabbit-oak,bugs-dot-jar_OAK-3763_ab1a0cc2,"{'BugID': 'OAK-3763', 'Summary': 'EmptyNodeState.equals() broken', 'Description': 'EmptyNodeState.equals() returns incorrect results when the other node state is not of type EmptyNodeState and the two states have differing exists() flags.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3769_306a9e00,"{'BugID': 'OAK-3769', 'Summary': ""QueryParse exception when fulltext search performed with term having '/'"", 'Description': 'Running the below query, results in Exception pointed by [1]\n\n/jcr:root/content/dam//element(*,dam:Asset)[jcr:contains(jcr:content/metadata/@cq:tags, \'stockphotography:business/business_abstract\')] order by @jcr:created descending\n\nAlso if you remove the node at /oak:index/damAssetLucene/indexRules/dam:Asset/properties/cqTags  and re-index the /oak:index/damAssetLucene index, the query works.\n\nSeems \'/\' is special character and needs to be escaped by Oak.\n\n[1]\n{noformat}\nCaused by: org.apache.lucene.queryparser.flexible.core.QueryNodeParseException: Syntax Error, cannot parse stockphotography\\:business/business_abstract: Lexical error at line 1, column 45.  Encountered: <EOF> after : ""/business_abstract"" \nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:74)\nat org.apache.lucene.queryparser.flexible.core.QueryParserHelper.parse(QueryParserHelper.java:250)\nat org.apache.lucene.queryparser.flexible.standard.StandardQueryParser.parse(StandardQueryParser.java:168)\nat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.tokenToQuery(LucenePropertyIndex.java:1260)\n... 138 common frames omitted\nCaused by: org.apache.lucene.queryparser.flexible.standard.parser.TokenMgrError: Lexical error at line 1, column 45.  Encountered: <EOF> after : ""/business_abstract""\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParserTokenManager.getNextToken(StandardSyntaxParserTokenManager.java:937)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_scan_token(StandardSyntaxParser.java:945)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3R_4(StandardSyntaxParser.java:827)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_3_2(StandardSyntaxParser.java:739)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.jj_2_2(StandardSyntaxParser.java:730)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Clause(StandardSyntaxParser.java:318)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ModClause(StandardSyntaxParser.java:303)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.ConjQuery(StandardSyntaxParser.java:234)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.DisjQuery(StandardSyntaxParser.java:204)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.Query(StandardSyntaxParser.java:166)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.TopLevelQuery(StandardSyntaxParser.java:147)\nat org.apache.lucene.queryparser.flexible.standard.parser.StandardSyntaxParser.parse(StandardSyntaxParser.java:65)\n... 141 common frames omitted\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3792_94110f21,"{'BugID': 'OAK-3792', 'Summary': 'Provide Simple Exception Name in Credentials Attribute for PW Expiry', 'Description': 'currently upon encountering a pw history exception while changing the password of a user, the credential attribute is set with the FQ class name, instead of the simple name. this requires consumers (e.g. sling) to use oak package names instead of a simple class name to react to the situation.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3798_2ac1dccd,"{'BugID': 'OAK-3798', 'Summary': 'NodeDocument.getNewestRevision() incorrect when there are previous documents', 'Description': 'The method may incorrectly return null when there are previous documents and the base revision is lower than all local changes.\n\nThis is most likely caused by changes done for OAK-3388.'}"
jackrabbit-oak,bugs-dot-jar_OAK-379_621a5101,"{'BugID': 'OAK-379', 'Summary': 'Query test failures on buildbot', 'Description': 'Since revision 1398915 various query tests fail on [buildbot|http://ci.apache.org/builders/oak-trunk/builds/784]:\n\n{code}\nsql1(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak\nsql2(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak\nxpath(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak\nbindVariableTest(org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexQueryTest): No LoginModules configured for jackrabbit.oak\nsql1(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak\nsql2(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak\nxpath(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak\nbindVariableTest(org.apache.jackrabbit.oak.plugins.index.property.PropertyIndexQueryTest): No LoginModules configured for jackrabbit.oak\nsql2Explain(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak\nsql1(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak\nxpath(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak\nbindVariableTest(org.apache.jackrabbit.oak.plugins.index.old.QueryTest): No LoginModules configured for jackrabbit.oak\nsql1(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak\nsql2(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak\nxpath(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak\nbindVariableTest(org.apache.jackrabbit.oak.query.index.TraversingIndexQueryTest): No LoginModules configured for jackrabbit.oak\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3817_2a02a138,"{'BugID': 'OAK-3817', 'Summary': ""Hidden properties (one prefixed with ':') in lucene's analyzer configuration fail to construct analyzers"", 'Description': ""This is similar to OAK-2524 in the sense that lucene doesn't like extra arguments sent its way while constructing analyzers. In some cases (like node move adds {{:source-path}}) we have hidden properties added to index definition nodes and they get passed along to lucene analyzer factories which complaint and fail.""}"
jackrabbit-oak,bugs-dot-jar_OAK-3872_c13708e3,"{'BugID': 'OAK-3872', 'Summary': '[RDB] Updated blob still deleted even if deletion interval lower', 'Description': 'If an existing blob is uploaded again, the timestamp of the existing entry is updated in the meta table. Subsequently if a call to delete (RDBBlobStore#countDeleteChunks) is made with {{maxLastModifiedTime}} parameter of less than the updated time above, the entry in the meta table is not touched but the data table entry is wiped out. \n\nRefer https://github.com/apache/jackrabbit-oak/blob/trunk/oak-core/src/main/java/org/apache/jackrabbit/oak/plugins/document/rdb/RDBBlobStore.java#L510\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-3879_4faf31e3,"{'BugID': 'OAK-3879', 'Summary': ""Lucene index / compatVersion 2: search for 'abc!' does not work"", 'Description': ""When using a Lucene fulltext index with compatVersion 2, then the following query does not return any results. When using compatVersion 1, the correct result is returned.\n\n{noformat}\nSELECT * FROM [nt:unstructured] AS c \nWHERE CONTAINS(c.[jcr:description], 'abc!') \nAND ISDESCENDANTNODE(c, '/content')\n{noformat}\n\nWith compatVersion 1 and 2, searching for just 'abc' works. Also, searching with '=' instead of 'contains' works.""}"
jackrabbit-oak,bugs-dot-jar_OAK-3897_94c6c575,"{'BugID': 'OAK-3897', 'Summary': 'Branch reset does not revert all changes', 'Description': 'This is caused by recent changes done for OAK-3646.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3903_690fb9f4,"{'BugID': 'OAK-3903', 'Summary': 'Commit fails even though change made it to the DocumentStore', 'Description': 'In some rare cases it may happen that the DocumentNodeStore considers a commit as failed even though the changes were applied entirely to the DocumentStore. The issue happens when the update of the commit root is applied to the storage of a DocumentStore but then shortly after the communication between Oak the the storage system fails. On the Oak side the call will be considered as failed, but the change was actually applied.\n\nThe issue can be reproduced with the test attached to OAK-1641 and a replica-set with 3 nodes. Killing the primary node and restarting it a after a while in a loop will eventually lead to a commit that conflicts itself.'}"
jackrabbit-oak,bugs-dot-jar_OAK-3920_99996c25,"{'BugID': 'OAK-3920', 'Summary': 'OakDirectory not usable in readOnly mode with a readOnly builder', 'Description': 'When using {{OakDirectory}} with a read only builder say in LuceneCommand in oak-console following error is seen\n\n{noformat}\nlc info /oak:index/users\nERROR java.lang.UnsupportedOperationException:\nThis builder is read-only.\n       at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.unsupported (ReadOnlyBuilder.java:45)\n       at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:190)\n       at org.apache.jackrabbit.oak.spi.state.ReadOnlyBuilder.child (ReadOnlyBuilder.java:35)\n       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:93)\n       at org.apache.jackrabbit.oak.plugins.index.lucene.OakDirectory.<init> (OakDirectory.java:87)\n       at org.apache.jackrabbit.oak.console.commands.LuceneCommand.getDirectory (LuceneCommand.groovy:128)\n       at org.apache.jackrabbit.oak.console.commands.LuceneCommand.this$4$getDirectory (LuceneCommand.groovy)\n       at org.apache.jackrabbit.oak.console.commands.LuceneCommand$_closure1.doCall (LuceneCommand.groovy:55)\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-3930_b939aa6e,"{'BugID': 'OAK-3930', 'Summary': 'Sysview import of single valued mv property creates sv property', 'Description': 'See test in filevault [0].\n\nit imports a multivalue property that only has 1 value, via [1]. the same test succeeds in jackrabbit 2.0, but fails in oak 1.3.14\n\n[0] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/test/java/org/apache/jackrabbit/vault/packaging/integration/TestUserContentPackage.java#L297-L326\n[1] https://github.com/apache/jackrabbit-filevault/blob/jackrabbit-filevault-3.1.26/vault-core/src/main/java/org/apache/jackrabbit/vault/fs/impl/io/JcrSysViewTransformer.java#L146-L148'}"
jackrabbit-oak,bugs-dot-jar_OAK-395_4ed7bc8e,"{'BugID': 'OAK-395', 'Summary': 'Inconsistency in Node#setProperty in case of null value', 'Description': ""Setting a null value to a single valued property will result\nin 'null' being returned while executing the same on a multivalued\nproperty will return the removed property.\n\njr2 returned the removed property in both cases as far as i \nremember and i would suggest that we don't change that behavior. in\nparticular since the specification IMO doesn't allow to return\nnull-values for these methods.""}"
jackrabbit-oak,bugs-dot-jar_OAK-395_e6c31270,"{'BugID': 'OAK-395', 'Summary': 'Inconsistency in Node#setProperty in case of null value', 'Description': ""Setting a null value to a single valued property will result\nin 'null' being returned while executing the same on a multivalued\nproperty will return the removed property.\n\njr2 returned the removed property in both cases as far as i \nremember and i would suggest that we don't change that behavior. in\nparticular since the specification IMO doesn't allow to return\nnull-values for these methods.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4036_f4324736,"{'BugID': 'OAK-4036', 'Summary': 'LuceneIndexProviderService may miss on registering PreExtractedTextProvider', 'Description': '{{LuceneIndexProviderService}} has an optional dependency on {{PreExtractedTextProvider}}. In such a case it can happen that bind for the provided is invoked before the activate is called. In such a case the provider would not be registered.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4038_557eec4f,"{'BugID': 'OAK-4038', 'Summary': 'o.a.j.o.spi.query.Filter exposes unexported class o.a.j.o.query.ast.SelectorImpl', 'Description': 'The interface {{o.a.j.o.spi.query.Filter}} uses in its public API the class {{o.a.j.o.query.ast.SelectorImpl}}, but while the former is contained in an exported package, the latter is not.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4050_52ca008c,"{'BugID': 'OAK-4050', 'Summary': 'SplitOperations may not retain most recent committed _commitRoot entry', 'Description': 'In some rare cases it may happen that SplitOperations does not retain the most recent committed _commitRoot entry on a document. This may result in an undetected hierarchy conflict.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4066_9a109aa3,"{'BugID': 'OAK-4066', 'Summary': ""Suggestion dictionary don't update after suggestUpdateFrequencyMinutes unless something else causes index update"", 'Description': ""Currently, suggestions building is tied at the end of indexing cycle. Along with that we check if diff between currTime and lastSugguestionBuildTime is more than {{suggestUpdateFrequencyMinutes}} before deciding to build suggestions or not.\n\nThis allows for suggestions not getting updated if:\n* At T1 suggestions are built\n* At T2 an index update takes place but suggestions aren't rebuilt because not enough time has passed since T1\n* Now at T3 (after sufficient time), changes at T2 won't show up for suggestions until some other index change happens.\n\nWe should probably see track about last changes in index (at T2) and use that too while running indexing cycle at T3.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4067_56accddf,"{'BugID': 'OAK-4067', 'Summary': 'AssertionError thrown for Lucene index with empty suggest disctionary ', 'Description': 'Create an index where one field is enabled for suggestion but no content is indexed for that index i.e. no matching content. Then while performing any query following exception is thrown\n\n{noformat}\njava.lang.AssertionError\n\tat org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester.<init>(AnalyzingInfixSuggester.java:167)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper$2.<init>(SuggestHelper.java:127)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:127)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.util.SuggestHelper.getLookup(SuggestHelper.java:123)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.<init>(IndexNode.java:109)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:69)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:162)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:137)\n\tat org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:249)\n\tat org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:1016)\n\tat org.apache.jackrabbit.oak.query.QueryImpl.getBestSelectorExecutionPlan(QueryImpl.java:949)\n\tat org.apache.jackrabbit.oak.query.ast.SelectorImpl.prepare(SelectorImpl.java:288)\n{noformat}\n\nThis happens with {{-ea}} flag i.e. java assertions enabled. It caused [here|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java#L167]'}"
jackrabbit-oak,bugs-dot-jar_OAK-4153_9120fd1b,"{'BugID': 'OAK-4153', 'Summary': ""segment's compareAgainstBaseState wont call childNodeDeleted when deleting last and adding n nodes"", 'Description': ""{{SegmentNodeState.compareAgainstBaseState}} fails to call {{NodeStateDiff.childNodeDeleted}} when for the same parent the only child is deleted and at the same time multiple new, different children are added.\n\nReason is that the [current code|https://github.com/apache/jackrabbit-oak/blob/a9ce70b61567ffe27529dad8eb5d38ced77cf8ad/oak-segment/src/main/java/org/apache/jackrabbit/oak/plugins/segment/SegmentNodeState.java#L558] for '{{afterChildName == MANY_CHILD_NODES}}' *and* '{{beforeChildName == ONE_CHILD_NODE}}' does not handle all cases: it assumes that 'after' contains the 'before' child and doesn't handle the situation where the 'before' child has gone.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4166_374e3f3d,"{'BugID': 'OAK-4166', 'Summary': 'Simple versionable nodes are invalid after migration', 'Description': ""OAK-3836 introduces a support for migrating {{mix:simpleVersionable}} nodes from JCR2 to {{mix:versionable}} nodes in Oak. It changes the mixin type, however it doesn't add required properties: {{jcr:versionHistory}}, {{jcr:baseVersion}} and {{jcr:predecessors}}. As a result, versioning-related methods invoked on such nodes doesn't work correctly.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4170_2a489d05,"{'BugID': 'OAK-4170', 'Summary': 'QueryEngine adding invalid property restriction for fulltext query', 'Description': 'QueryEngine inserts a property restriction of ""is not null"" for any property used in fulltext constraint. For e.g. for query\n\n{noformat}\nselect * from [nt:unstructured] where CONTAINS([jcr:content/metadata/comment], \'december\')\n{noformat}\n\nA property restriction would be added for {{jcr:content/metadata/comment}}. However currently due to bug in {{FulltextSearchImpl}} [1] the property name generated is {{comment/jcr:content/metadata}}.\n\n{code}\n@Override\n    public void restrict(FilterImpl f) {\n        if (propertyName != null) {\n            if (f.getSelector().equals(selector)) {\n                String p = propertyName;\n                if (relativePath != null) {\n                    p = PathUtils.concat(p, relativePath);\n                }                \n                p = normalizePropertyName(p);\n                restrictPropertyOnFilter(p, f);\n            }\n        }\n        f.restrictFulltextCondition(fullTextSearchExpression.currentValue().getValue(Type.STRING));\n    }\n{code}\n\nThis happens because {{relativePath}} is passed as second param to {{PathUtils.concat}}. It should be first param\n\n[1] https://github.com/apache/jackrabbit-oak/blob/1.4/oak-core/src/main/java/org/apache/jackrabbit/oak/query/ast/FullTextSearchImpl.java#L275-L286'}"
jackrabbit-oak,bugs-dot-jar_OAK-421_36e70bd7,"{'BugID': 'OAK-421', 'Summary': 'NodeBuilder.reset might lead to inconsistent builder', 'Description': 'The following test fails:\n{code}\nNodeBuilder root = new MemoryNodeBuilder(BASE);\nNodeBuilder x = root.child(""x"");\nNodeBuilder y = x.child(""y"");\n\nroot.reset(BASE);\nassertTrue(root.hasChildNode(""x""));\nassertFalse(x.hasChildNode(""y""));  // fails\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-428_916cd92f,"{'BugID': 'OAK-428', 'Summary': 'Binaries might get removed by garbage collection while still referenced', 'Description': 'The [Microkernel contract|http://svn.apache.org/repos/asf/jackrabbit/oak/trunk/oak-mk-api/src/main/java/org/apache/jackrabbit/mk/api/MicroKernel.java] specifies a specific format for references to binaries: "":blobId:<blobId>"". Currently oak-core uses a different format and thus risks premature garbage collection of such binaries.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4291_cdb34ffc,"{'BugID': 'OAK-4291', 'Summary': 'FileStore.flush prone to races leading to corruption', 'Description': 'There is a small window in {{FileStore.flush}} that could lead to data corruption: if we crash right after setting the persisted head but before any delay-flushed {{SegmentBufferWriter}} instance flushes (see {{SegmentBufferWriterPool.returnWriter()}}) then that data is lost although it might already be referenced from the persisted head.\n\nWe need to come up with a test case for this. \n\nA possible fix would be to return a future from {{SegmentWriter.flush}} and rely on a completion callback. Such a change would most likely also be useful for OAK-3690. \n'}"
jackrabbit-oak,bugs-dot-jar_OAK-429_c02ecef8,"{'BugID': 'OAK-429', 'Summary': 'MemoryPropertyBuilder.assignFrom leads to ClassCastException on getPropertyState with date properties', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-4300_06c367af,"{'BugID': 'OAK-4300', 'Summary': 'Cost per entry for Lucene index of type v1 should be higher than that of v2', 'Description': 'Currently default cost per entry for Lucene index of type\n# v1 - which uses query time aggregation\n# v2 - which uses index time aggregation\n\nAre same. However given that query time aggregation would require more effort it should result in a higher cost per entry.\n\nThis fact impacts the result in cases like OAK-2081 (see last few comments) where with usage of limits both index are currently considered equals'}"
jackrabbit-oak,bugs-dot-jar_OAK-4307_f303c916,"{'BugID': 'OAK-4307', 'Summary': 'SegmentWriter saves references to external blobs', 'Description': ""The new {{SegmentWriteOperation#internalWriteStream}} method checks whether the input stream to write is a {{SegmentStream}}. If it's, writer will reuse existing block ids, rather than storing the whole stream.\n\nIt should also check whether the blocks in {{SegmentStream}} comes from the same tracker / segment store. Otherwise this will create invalid references if someone invokes the {{internalWriteStream()}} method with a {{SegmentStream}} created externally.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4351_59a83d23,"{'BugID': 'OAK-4351', 'Summary': 'Non-root lucene index throws exception if query constraints match root of sub-tree', 'Description': 'LucenePropetyIndexProvider returns incorrect (normalized) path for root of sub-tree if index is defined on the sub-tree. e.g.\n{{/jcr:root/test//element(*, nt:base)\\[@foo=\'bar\']}} would fail with following defn\n{noformat}\n+ /test\n    - foo=""bar""\n    + test1\n          - foo=""bar""\n    + oak:index\n           - indexRules/nt:base/properties/foo/propertyIndex=""true""\n{noformat}'}"
jackrabbit-oak,bugs-dot-jar_OAK-4353_b0014b7d,"{'BugID': 'OAK-4353', 'Summary': 'IndexOutOfBoundsException in FileStore.writeStream', 'Description': 'When writing streams of specific length I get \n{code}\njava.lang.IndexOutOfBoundsException\nat java.nio.Buffer.checkIndex(Buffer.java:538)\nat java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:359)\nat org.apache.jackrabbit.oak.segment.Segment.getGcGen(Segment.java:318)\nat org.apache.jackrabbit.oak.segment.file.FileStore.writeSegment(FileStore.java:1371)\nat org.apache.jackrabbit.oak.segment.SegmentWriter$SegmentWriteOperation.internalWriteStream(SegmentWriter.java:661)\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-4358_74cbba24,"{'BugID': 'OAK-4358', 'Summary': 'Stale cluster ids can potentially lead to lots of previous docs traversal in NodeDocument.getNewestRevision', 'Description': 'When some (actual test case and conditions still being investigated) of the following conditions are met:\n* There are property value changes from different cluster id\n* There are very old and stale cluster id (probably older incarnations of current node itself)\n* A parallel background split removes all _commitRoot, _revision entries such that the latest one (which is less that baseRev) is very old\n\n, finding newest revision traverses a lot of previous docs. Since root document gets split a lot and is a very common commitRoot (thus participating during checkConflicts in lot of commits), the issue can slow down commits by a lot'}"
jackrabbit-oak,bugs-dot-jar_OAK-4359_002c5845,"{'BugID': 'OAK-4359', 'Summary': ""Lucene index / compatVersion 2: search for 'a=b=c' does not work"", 'Description': ""Similar to OAK-3879, we need to escape '=' (althoug lucene [escape()|https://github.com/apache/lucene-solr/blob/releases/lucene-solr/4.7.1/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java#L988] apparently doesn't escape it).\n\nDue to this searching for {{a=b=c}} throws parse exception from lucene query parser. Also, searching for {{a=b}} gives incorrect result.""}"
jackrabbit-oak,bugs-dot-jar_OAK-4376_037dea72,"{'BugID': 'OAK-4376', 'Summary': 'XPath: queries starting with ""//"" are not always converted correctly', 'Description': 'XPath queries starting with ""//"" are not always converted to the expected SQL-2 query. Examples:\n\n{noformat}\n//element(*, oak:QueryIndexDefinition)/*\nselect [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a\n\n//element(*, oak:QueryIndexDefinition)//*\nselect [jcr:path], [jcr:score], * from [oak:QueryIndexDefinition] as a\n{noformat}\n\nThis is wrong. Instead, a join should be used.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4387_ca05fd06,"{'BugID': 'OAK-4387', 'Summary': 'XPath: querying for nodes named ""text"", ""element"", and ""rep:excerpt"" fails', 'Description': 'Queries that contain ""text"" or ""element"" as a node name currently fail, because the the parser assumes ""text()"" / ""element(...)"". Example query that fails:\n\n{noformat}\n/jcr:root/content/text/jcr:content//element(*,nt:unstructured)\n{noformat}\n\nA workaround is to use the escape mechanism, that is:\n\n{noformat}\n/jcr:root/tmp/_x0074_ext/jcr:content//element(*,nt:unstructured)\n{noformat}\n\nIt looks like \'(\' and \')\' are valid characters in node names, but to query for those characters, they need to be escaped.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4397_e33516d5,"{'BugID': 'OAK-4397', 'Summary': 'DefaultSyncContext.syncMembership may sync group of a foreign IDP', 'Description': ""With the following scenario the {{DefaultSyncContext.syncMembership}} may end up synchronizing (i.e. updating) a group defined by an foreign IDP and even add the user to be synchronized as a new member:\n\n- configuration with different IDPs\n- foreign IDP synchronizes a given external group 'groupA' => rep:externalID points to foreign-IDP (e.g. rep:externalId = 'groupA;foreignIDP')\n- my-IDP contains a group with the same ID (but obviously with a different rep:externalID) and user that has declared group membership pointing to 'groupA' from my IDP\n\nif synchronizing my user first the groupA will be created with a rep:externalId = 'groupA;myIDP'.\nhowever, if the group has been synced before by the foreignIDP the code fails to verify that an existing group 'groupA' really belongs to the same IDP and thus may end up synchronizing the group and updating it's members.\n\nIMHO that's a critical issue as it violates the IDP boundaries.\nthe fix is pretty trivial as it only requires testing for the IDP of the existing group as we do it in other places (even in the same method).""}"
jackrabbit-oak,bugs-dot-jar_OAK-43_668f08f2,"{'BugID': 'OAK-43', 'Summary': 'Incomplete journal when move and copy operations are involved', 'Description': 'Given a node at /source:\n\n{code}\nhead = mk.commit(""/"",\n    "">\\""source\\"" : \\""moved\\"""" +\n    ""*\\""moved\\"" : \\""copy\\"""",\n    head, """");\n{code}\n\nresults in the following journal:\n\n{code}\n>""/source"":""/copy""\n{code}\n\nwhere the freshly created node at /moved is missing.\n\nSimilarly \n\n{code}\nhead = mk.commit(""/"",\n    ""*\\""source\\"" : \\""copy\\"""" +\n    "">\\""copy\\"" : \\""moved\\"""",\n    head, """");\n{code}\n\nresults in\n\n{code}\n+""/moved"":{}\n{code}\n\nwhere moved away node at /source is missing.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4420_d645112f,"{'BugID': 'OAK-4420', 'Summary': 'RepositorySidegrade: oak-segment to oak-segment-tar should migrate checkpoint info', 'Description': 'The sidegrade from {{oak-segment}} to {{oak-segment-tar}} should also take care of moving the checkpoint data and meta. This will save a very expensive full-reindex.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4423_08f0b280,"{'BugID': 'OAK-4423', 'Summary': 'Possible overflow in checkpoint creation', 'Description': 'Creating a checkpoint with {{Long.MAX_VALUE}} lifetime will overflow the value, allowing the store to immediately release the checkpoint.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4423_275eca83,"{'BugID': 'OAK-4423', 'Summary': 'Possible overflow in checkpoint creation', 'Description': 'Creating a checkpoint with {{Long.MAX_VALUE}} lifetime will overflow the value, allowing the store to immediately release the checkpoint.'}"
jackrabbit-oak,bugs-dot-jar_OAK-4431_7441a3d5,"{'BugID': 'OAK-4431', 'Summary': 'Index path property should be considered optional for copy on read logic', 'Description': 'As part of changes done for OAK-4347 logic assumes that indexPath is always non null. This works fine for fresh setup where the indexPath would have been set by the initial indexing. However for upgraded setup this assumption would break as it might happen that index does not get updated with new approach and before that a read is performed.\n\nCurrently with updated code on upgraded setup following exception is seen \n\n{noformat}\nCaused by: javax.security.auth.login.LoginException: java.lang.NullPointerException: Index path property [:indexPath] not found\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:236)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexDefinition.getIndexPathFromConfig(IndexDefinition.java:664)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.getSharedWorkingSet(IndexCopier.java:242)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexCopier.wrapForRead(IndexCopier.java:140)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexNode.open(IndexNode.java:53)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.findIndexNode(IndexTracker.java:179)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.IndexTracker.acquireIndexNode(IndexTracker.java:154)\n        at org.apache.jackrabbit.oak.plugins.index.lucene.LucenePropertyIndex.getPlans(LucenePropertyIndex.java:250)\n{noformat}\n\nFor this specific flow the indexPath can be passed in and not looked up from IndexDefinition'}"
jackrabbit-oak,bugs-dot-jar_OAK-4432_c9765c21,"{'BugID': 'OAK-4432', 'Summary': 'Ignore files in the root directory of the FileDataStore in #getAllIdentifiers', 'Description': 'The call to OakFileDataStore#getAllIdentifiers should ignore the the files directly at the root of the DataStore (These files are used for SharedDataStore etc). This does not cause any functional problems but leads to logging warning in the logs. \nThere is already a check but it fails when the data store root is specified as a relative path.'}"
jackrabbit-oak,bugs-dot-jar_OAK-447_00df38d2,"{'BugID': 'OAK-447', 'Summary': 'Adding a node with the name of a removed node can lead to an inconsistent hierarchy of node builders', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-448_999097e1,"{'BugID': 'OAK-448', 'Summary': 'Node builder for existing node return null for base state', 'Description': '{{MemoryNodeBuilder.getBaseState()}} returns null on builder for an existing node.'}"
jackrabbit-oak,bugs-dot-jar_OAK-478_a7f0e808,"{'BugID': 'OAK-478', 'Summary': 'NPE in the TypeValidator when using the Lucene Index', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-479_3270e761,"{'BugID': 'OAK-479', 'Summary': ""Adding a node to a node that doesn't accept children doesn't fail with ConstraintViolationException"", 'Description': ""More node type fun!\n\nI ran into this via the tck test {{org.apache.jackrabbit.test.api.query.SaveTest#testConstraintViolationException}}.\n\nIt seems adding a node to a node that doesn't accept children (like for example {{nt:query}}) fails with a {{RepositoryException}} that wraps a {{CommitFailedException}} with a message along the lines of: {{Cannot add node 'q2' at /q1}}, further wrapping a weird-looking {{RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b}}\n\nWhile this seems ok enough, the tck test expects a {{ConstraintViolationException}}, so that's why I created this bug.\n\n\nI'll attach a test case shortly.\n\nTrace \n{code}\njavax.jcr.RepositoryException\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.jackrabbit.oak.api.CommitFailedException.throwRepositoryException(CommitFailedException.java:57)\n\tat org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:244)\n\tat org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283)\n\tat org.apache.jackrabbit.oak.jcr.nodetype.NodeTypeTest.illegalAddNode(NodeTypeTest.java:39)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:300)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nCaused by: org.apache.jackrabbit.oak.api.CommitFailedException: Cannot add node 'q2' at /q1\n\tat org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:134)\n\tat org.apache.jackrabbit.oak.spi.commit.CompositeValidator.childNodeAdded(CompositeValidator.java:68)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.childNodeAdded(ValidatingHook.java:155)\n\tat org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.validate(ValidatingHook.java:110)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.validate(ValidatingHook.java:101)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.childNodeAdded(ValidatingHook.java:157)\n\tat org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState$3.childNodeAdded(ModifiedNodeState.java:292)\n\tat org.apache.jackrabbit.oak.spi.state.AbstractNodeState.compareAgainstBaseState(AbstractNodeState.java:157)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeState.compareAgainstBaseState(KernelNodeState.java:243)\n\tat org.apache.jackrabbit.oak.plugins.memory.ModifiedNodeState.compareAgainstBaseState(ModifiedNodeState.java:269)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.validate(ValidatingHook.java:110)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook$ValidatorDiff.validate(ValidatingHook.java:101)\n\tat org.apache.jackrabbit.oak.spi.commit.ValidatingHook.processCommit(ValidatingHook.java:73)\n\tat org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127)\n\tat org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:239)\n\tat org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:1)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:337)\n\tat org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234)\n\tat org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:241)\n\t... 27 more\nCaused by: javax.jcr.RepositoryException: No matching node definition found for org.apache.jackrabbit.oak.plugins.nodetype.ValidatingNodeTypeManager@257f1b\n\tat org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getDefinition(ReadOnlyNodeTypeManager.java:406)\n\tat org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator$EffectiveNodeType.getDefinition(TypeValidator.java:302)\n\tat org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator$EffectiveNodeType.checkAddChildNode(TypeValidator.java:249)\n\tat org.apache.jackrabbit.oak.plugins.nodetype.TypeValidator.childNodeAdded(TypeValidator.java:127)\n\t... 49 more\n{code}\n\n""}"
jackrabbit-oak,bugs-dot-jar_OAK-47_b62f1c26,"{'BugID': 'OAK-47', 'Summary': 'Wrong results and NPE with copy operation', 'Description': 'The following code either results in an NPE or in a wrong result depending on which Microkernel instance is used. \n\n{code}\n    mk.commit("""", ""+\\""/root\\"":{}"", mk.getHeadRevision(), """");\n    mk.commit("""", ""+\\""/root/N0\\"":{}*\\""/root/N0\\"":\\""/root/N1\\""+\\""/root/N0/N4\\"":{}"",\n            mk.getHeadRevision(), """");\n{code}\n\nThe wrong result is \n{code}\n{\n    "":childNodeCount"": 2,\n    ""N0"": {\n        "":childNodeCount"": 1,\n        ""N4"": {\n            "":childNodeCount"": 0\n        }\n    },\n    ""N1"": {\n        "":childNodeCount"": 1,\n        ""N4"": {\n            "":childNodeCount"": 0\n        }\n    }\n}\n{code}\n\nThe expected result is\n{code}\n{\n    "":childNodeCount"": 2,\n    ""N0"": {\n        "":childNodeCount"": 1,\n        ""N4"": {\n            "":childNodeCount"": 0\n        }\n    },\n    ""N1"": {\n        "":childNodeCount"": 0\n    }\n}\n{code}\n\nsimple:fs:target/temp: wrong result\nfs:{homeDir}/target: NPE\nhttp-bridge:fs:{homeDir}/target: NPE\nsimple: wrong result\n\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-498_f2a2edec,"{'BugID': 'OAK-498', 'Summary': 'NamePathMapper should fail on absolute paths escaping root', 'Description': 'The name path mapper should no accept invalid paths of type\n\n{code}\n/..\n{code}\n\nI.e. paths which escape beyond the root of the hierarchy. '}"
jackrabbit-oak,bugs-dot-jar_OAK-499_61381ea2,"{'BugID': 'OAK-499', 'Summary': ""SQL-2 query parser doesn't detect some illegal statements"", 'Description': 'The SQL-2 query parser doesn\'t detect some illegal statements, for example\n\n{code}\nselect * from [nt:base] where name =+ \'Hello\'\nselect * from [nt:base] where name => \'Hello\'\n{code}\n\nBoth are currently interpreted as ""name = \'Hello\'"", which is wrong.'}"
jackrabbit-oak,bugs-dot-jar_OAK-509_b896c926,"{'BugID': 'OAK-509', 'Summary': ""Item names starting with '{X}' cause RepositoryException"", 'Description': ""The exception is RepositoryException: Invalid name or path: {0} foo\n\nE.g. for an item named '{0} foo'.\n\nI guess oak-jcr tries to interpret it as a name in expanded form but does not find a namespace uri for '0'. IIRC these names are valid in Jackrabbit 2.x.""}"
jackrabbit-oak,bugs-dot-jar_OAK-510_f63d745a,"{'BugID': 'OAK-510', 'Summary': 'Multivalued properties with array size 0 forget their type', 'Description': 'thought i remember that i have seen a related TODO or issue before, i\ncouldn\'t find it any more... sorry for that.\n\nwhile cleaning up the node type code i found that one FIXME in the \nReadOnlyNodeTypeManager related to definition generation was only needed\nbecause the TypeValidator failed upon validation of an empty jcr:supertypes\ndefinition. not storing the super types if none has be declared solved the\nproblem for the time being.\n\nhowever, it seems to me that the underlying problem is in a completely\ndifferent area: namely that mv properties with an empty value array\nforget their type.\n\nthis can be verified with the following test:\n{code}\n    @Test\n    public void addEmptyMultiValueName() throws RepositoryException {\n        Node parentNode = getNode(TEST_PATH);\n        Value[] values = new Value[0];\n\n        parentNode.setProperty(""multi name"", values);\n        parentNode.getSession().save();\n\n        Session session2 = createAnonymousSession();\n        try {\n            Property property = session2.getProperty(TEST_PATH + ""/multi name"");\n            assertTrue(property.isMultiple());\n            assertEquals(PropertyType.NAME, property.getType());\n            Value[] values2 = property.getValues();\n            assertEquals(values.length, values2.length);\n            assertEquals(values[0], values2[0]);\n            assertEquals(values[1], values2[1]);\n        } finally {\n            session2.logout();\n        }\n    }\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-520_ec961a38,"{'BugID': 'OAK-520', 'Summary': 'IllegalStateException in MemoryNodeBuilder', 'Description': '{{AuthorizablePropertyTest.testSetPropertyByRelPath()}} sometimes causes an IllegalStateException in {{MemoryNodeBuilder}}. This might be a problem with the latter uncovered by the recent switch to the p2 index mechanism (OAK-511).\n\n{code}\njava.lang.IllegalStateException\n    at com.google.common.base.Preconditions.checkState(Preconditions.java:133)\n    at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.read(MemoryNodeBuilder.java:205)\n    at org.apache.jackrabbit.oak.plugins.memory.MemoryNodeBuilder.getChildNodeNames(MemoryNodeBuilder.java:379)\n    at org.apache.jackrabbit.oak.plugins.index.p2.strategy.ContentMirrorStoreStrategy.remove(ContentMirrorStoreStrategy.java:66)\n    at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexUpdate.apply(Property2IndexUpdate.java:143)\n    at org.apache.jackrabbit.oak.plugins.index.p2.Property2IndexDiff.apply(Property2IndexDiff.java:232)\n    at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.apply(IndexHookManager.java:71)\n    at org.apache.jackrabbit.oak.plugins.index.IndexHookManager.processCommit(IndexHookManager.java:61)\n    at org.apache.jackrabbit.oak.spi.commit.CompositeHook.processCommit(CompositeHook.java:59)\n    at org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:127)\n    at org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:240)\n    at org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:236)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:337)\n    at org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:235)\n    at org.apache.jackrabbit.oak.jcr.SessionDelegate.save(SessionDelegate.java:255)\n    at org.apache.jackrabbit.oak.jcr.SessionImpl.save(SessionImpl.java:283)\n    at org.apache.jackrabbit.oak.jcr.security.user.AbstractUserTest.tearDown(AbstractUserTest.java:72)\n    at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)\n    at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n    at org.junit.runner.JUnitCore.run(JUnitCore.java:157)\n    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:76)\n    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)\n    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120) \n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-531_90c45a02,"{'BugID': 'OAK-531', 'Summary': 'NodeBuilder deleted child nodes can come back', 'Description': ""While working on OAK-520, I've noticed a problem with the NodeBuilder: when we delete an entire hierarchy of nodes and then recreate a part of it, some of the previously deleted nodes can come back.\n\nThis only happens when there are more than 3 levels of nodes.\n\nSo given a hierarchy of nodes: /x/y/z deleted 'x' and simply use the NodeBuilder to traverse down on the same path: .child('x').child('y').\nAt this point the 'z' child reappears even though it was deleted before.\n\n\nI'll attach a test case shortly.""}"
jackrabbit-oak,bugs-dot-jar_OAK-537_a8493efc,"{'BugID': 'OAK-537', 'Summary': 'The Property2Index eagerly and unnecessarily fetches all data', 'Description': 'Currently, the Property2Index (as well as the PropertyIndex and the NodeTypeIndex) loads all paths into a hash set. This is even the case for the getCost operation, which should be fast and therefore not load too much data.\n\nThis strategy can cause ouf-of-memory if the result is too big. Also, loading all data is not necessary unless the user reads all rows.\n\nInstead, the index should only load data on demand. Also, the getCost operation should only estimate the number of read nodes, and not actually read the data.\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-539_ffa818f3,"{'BugID': 'OAK-539', 'Summary': 'Wrong compareTo in micro-kernel Id class', 'Description': 'CompareTo method in Id class fails in some cases.\n\n{code} \n// this works\nfinal Id id1 = Id.fromString( ""0000000000000007"" );\nfinal Id id2 = Id.fromString( ""000000000000000c"" );\n\nassertTrue( id1 + "" should be less than "" + id2, id1.compareTo( id2 ) < 0 );\n\n// but this doesn\'t\nfinal Id id1 = Id.fromString( ""0000000000000070"" );\nfinal Id id2 = Id.fromString( ""00000000000000c0"" );\n\nassertTrue( id1 + "" should be less than "" + id2, id1.compareTo( id2 ) < 0 );\n{code} '}"
jackrabbit-oak,bugs-dot-jar_OAK-543_3ce758b7,"{'BugID': 'OAK-543', 'Summary': 'PutTokenImpl not thread safe', 'Description': '{{PutTokenImpl}} uses prefix increment on a static member to generate presumably unique identifiers. Prefix increment is not atomic though which might result in non unique ids being generated. '}"
jackrabbit-oak,bugs-dot-jar_OAK-546_428e32c6,"{'BugID': 'OAK-546', 'Summary': 'Query: unexpected result on negative limit / offset', 'Description': 'Currently, running a query with limit of -1 never returns any rows, the same as when using limit = 0.\n\nEither the query engine should fail with a negative limit or offset (IllegalArgumentException), or it should ignore negative values (unlimited result rows for limit, probably no offset for offset = -1).\n\nI would prefer IllegalArgumentException, but I can also live with -1 = unlimited, at least for ""limit"".'}"
jackrabbit-oak,bugs-dot-jar_OAK-548_717186d6,"{'BugID': 'OAK-548', 'Summary': 'Moving larger trees cause OutOfMemoryError', 'Description': '{{LargeMoveTest.moveTest}} test runs out of heap space when moving roughly 100000 nodes (128M heap):\n\n{code}\njava.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2786)\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:64)\n\tat java.lang.StringCoding.access$300(StringCoding.java:34)\n\tat java.lang.StringCoding$StringEncoder.encode(StringCoding.java:251)\n\tat java.lang.StringCoding.encode(StringCoding.java:272)\n\tat java.lang.String.getBytes(String.java:946)\n\tat org.apache.jackrabbit.mk.util.IOUtils.writeString(IOUtils.java:84)\n\tat org.apache.jackrabbit.mk.store.BinaryBinding.writeMap(BinaryBinding.java:98)\n\tat org.apache.jackrabbit.mk.model.ChildNodeEntriesMap.serialize(ChildNodeEntriesMap.java:196)\n\tat org.apache.jackrabbit.mk.model.AbstractNode.serialize(AbstractNode.java:169)\n\tat org.apache.jackrabbit.mk.persistence.InMemPersistence.writeNode(InMemPersistence.java:76)\n\tat org.apache.jackrabbit.mk.store.DefaultRevisionStore.putNode(DefaultRevisionStore.java:276)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:568)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree$StagedNode.persist(StagedNodeTree.java:563)\n\tat org.apache.jackrabbit.mk.model.StagedNodeTree.persist(StagedNodeTree.java:80)\n\tat org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:126)\n\tat org.apache.jackrabbit.mk.model.CommitBuilder.doCommit(CommitBuilder.java:94)\n\tat org.apache.jackrabbit.mk.core.MicroKernelImpl.commit(MicroKernelImpl.java:496)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.commit(KernelNodeStoreBranch.java:178)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.setRoot(KernelNodeStoreBranch.java:78)\n\tat org.apache.jackrabbit.oak.core.RootImpl.purgePendingChanges(RootImpl.java:355)\n\tat org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:234)\n\tat org.apache.jackrabbit.oak.core.LargeMoveTest.moveTest(LargeMoveTest.java:78)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n{code}\n\nThis is caused by the inefficient rebase implementation in oak-core as discussed at length in OAK-464.'}"
jackrabbit-oak,bugs-dot-jar_OAK-554_3f51fb09,"{'BugID': 'OAK-554', 'Summary': 'PropertyStates#createProperty ignores namespace mappings when creating states of type NAME and PATH', 'Description': 'as far as i saw we use PropertyStates#createProperty to create and\nset an OAK property from a given JCR value or a list of JCR values.\n\nthis works well for all types of values except for NAME, PATH which \nmay contain values with remapped namespaces which will not be converted\nback to oak-values during the state creation:\n\n{code}\n     List<String> vals = Lists.newArrayList();\n     for (Value value : values) {\n         vals.add(value.getString());\n     }\n     return new MultiGenericPropertyState(name, vals, Type.fromTag(type, true));\n{code}\n\nif am not mistaken {code}value.getString(){code} will return the JCR\nrepresentation of the value instead of the oak representation as it\nwould be needed here.\n\npossible solutions include:\n- passing namepathmapper to the create method\n- only accept oak Value implementation that allows to retrieve the\n  internal representation, which is present in the ValueImpl afaik.'}"
jackrabbit-oak,bugs-dot-jar_OAK-579_7d72e6ed,"{'BugID': 'OAK-579', 'Summary': 'Query: for joins, sometimes no or the wrong index is used', 'Description': ""Currently, no index is used for the join condition. For example, the query:\n\n{code}\nselect * from [nodeTypeA] as a \ninner join [nodeTypeB] as b\non isdescendantnode(b, a) \nwhere lower(a.x) = 'y'\nand b.[property] is not null\n{code}\n\ncurrently doesn't take into account that the path of the selector 'a' is known at the time selector 'b' is accessed (given that selector 'a' is executed first). So in this case, the query would use an index on the property b.[property], even if this index has a very bad selectivity (many nodes with this problem), or the query would use the node type index on [nodeTypeB], even if there are many nodes of this type.\n\nInstead, most likely the query should do a traversal, using the isdescendantnode(b, a) join condition.""}"
jackrabbit-oak,bugs-dot-jar_OAK-596_9b268da0,"{'BugID': 'OAK-596', 'Summary': 'Microkernel.diff returns empty diff when there are differences', 'Description': '{code}\nString rev1 = mk.commit(""/"", ""+\\""node1\\"":{\\""node2\\"":{\\""prop1\\"":\\""val1\\"",\\""prop2\\"":\\""val2\\""}}"", null, null);\nString rev2 = mk.commit(""/"", ""^\\""node1/node2/prop1\\"":\\""val1 new\\"" ^\\""node1/node2/prop2\\"":null"", null, null);\nString diff = mk.diff(rev1, rev2, ""/node1/node2"", 0);\n{code}\n\nHere {{diff}} is empty although there are clearly differences between {{rev1}} and {{rev2}} at depth 0.'}"
jackrabbit-oak,bugs-dot-jar_OAK-606_f0fbacab,"{'BugID': 'OAK-606', 'Summary': 'Node becomes invalid after Session#move()', 'Description': 'moving or renaming an existing (saved) node renders that node instance\ninvalid and any access on that node instance will throw IllegalStateException.'}"
jackrabbit-oak,bugs-dot-jar_OAK-612_df9e6913,"{'BugID': 'OAK-612', 'Summary': 'Calling addNode on a node that has orderable child nodes violates specification', 'Description': 'it seems to me that the current behavior of Node.addNode for a node that \nhas orderable child nodes violates the specification (section 23.3):\n\n{quote}\n23.3 Adding a New Child Node\nWhen a child node is added to a node that has orderable child nodes\nit is added to the end of the list.\n{quote}\n\nhowever, the following test will fail:\n\n{code}\n@Test\n    public void testAddNode() throws Exception {\n        new TestContentLoader().loadTestContent(getAdminSession());\n\n        Session session = getAdminSession();\n        Node test = session.getRootNode().addNode(""test"", ""test:orderableFolder"");\n        assertTrue(test.getPrimaryNodeType().hasOrderableChildNodes());\n\n        Node n1 = test.addNode(""a"");\n        Node n2 = test.addNode(""b"");\n        session.save();\n\n        NodeIterator it = test.getNodes();\n        assertEquals(""a"", it.nextNode().getName());\n        assertEquals(""b"", it.nextNode().getName());\n    }\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-614_6feacf6b,"{'BugID': 'OAK-614', 'Summary': 'AssertionError in MemoryNodeBuilder', 'Description': '{code}\n    NodeBuilder root = ...\n    NodeBuilder child = root.child(""new"");\n\n    root.removeNode(""new"");\n    child.getChildNodeCount();\n{code}\n\nThe last line throws an {{AssertionError}} when no node named ""new"" existed initially. It throws an {{IllegalStateException}} as expected otherwise. '}"
jackrabbit-oak,bugs-dot-jar_OAK-621_00b4b8a0,"{'BugID': 'OAK-621', 'Summary': ""Moving or deleting tree instances with status NEW doesn't change its status to DISCONNECTED"", 'Description': 'Further fall out from OAK-606:\n\n{code}\n        Tree t = tree.addChild(""new"");\n\n        root.move(""/x"", ""/y/x"");\n        assertEquals(Status.DISCONNECTED, t.getStatus());\n{code}\n\nThe assertion fails. '}"
jackrabbit-oak,bugs-dot-jar_OAK-642_7a84b3a8,"{'BugID': 'OAK-642', 'Summary': 'NPE trying to add a node to an nt:folder node', 'Description': 'The following code throws a NPE:\n\n{code}\nSession s = getAdminSession();\ns.getRootNode().addNode(""a"", ""nt:folder"").addNode(""b"");\ns.save();        \n{code}\n\nStack trace:\n{code}\njava.lang.NullPointerException\nat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)\nat org.apache.jackrabbit.oak.namepath.LocalNameMapper.getOakNameOrNull(LocalNameMapper.java:82)\nat org.apache.jackrabbit.oak.namepath.GlobalNameMapper.getOakName(GlobalNameMapper.java:64)\nat org.apache.jackrabbit.oak.namepath.NamePathMapperImpl.getOakName(NamePathMapperImpl.java:62)\nat org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getOakName(ReadOnlyNodeTypeManager.java:92)\nat org.apache.jackrabbit.oak.plugins.nodetype.ReadOnlyNodeTypeManager.getNodeType(ReadOnlyNodeTypeManager.java:186)\nat org.apache.jackrabbit.oak.jcr.NodeImpl$5.perform(NodeImpl.java:265)\nat org.apache.jackrabbit.oak.jcr.NodeImpl$5.perform(NodeImpl.java:1)\nat org.apache.jackrabbit.oak.jcr.SessionDelegate.perform(SessionDelegate.java:136)\nat org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:219)\nat org.apache.jackrabbit.oak.jcr.NodeImpl.addNode(NodeImpl.java:210)\nat org.apache.jackrabbit.oak.jcr.CRUDTest.nodeType(CRUDTest.java:122)\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-644_55a4f738,"{'BugID': 'OAK-644', 'Summary': ""Revisit PrivilegeDefinitionStore's use of null as a child name parameter"", 'Description': ""As discussed on OAK-635, I'm extracting the PrivilegeDefinitionStore code&patch into a dedicated issue.\n\nFollowing the discussion on the dev list, I've filed it as a bug, as nulls are not considered valid input parameters.""}"
jackrabbit-oak,bugs-dot-jar_OAK-678_6c54045d,"{'BugID': 'OAK-678', 'Summary': 'Access to disconnected MemoryNodeBuilder should throw IllegalStateException', 'Description': None}"
jackrabbit-oak,bugs-dot-jar_OAK-738_8ed779dc,"{'BugID': 'OAK-738', 'Summary': 'Assertion error when adding node with expanded name', 'Description': '{code}\nnode.addNode(""{http://foo}new"");\n{code}\n\nresults in an assertion error'}"
jackrabbit-oak,bugs-dot-jar_OAK-740_35a7f014,"{'BugID': 'OAK-740', 'Summary': 'Malformed solr delete query', 'Description': 'Following OAK-734 the solr query tests are failing because of a parsing error on the wildcard delete query.\n\nThe exact query is \'path_exact:/test*\', which apparently upsets the lucene parser somehow.\n\nFull trace:\n\n{code}\nSEVERE: org.apache.solr.common.SolrException: org.apache.lucene.queryparser.classic.ParseException: Cannot parse \'path_exact:/test*\': Lexical error at line 1, column 18.  Encountered: <EOF> after : ""/test*""\n\tat org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:328)\n\tat org.apache.solr.update.DirectUpdateHandler2.deleteByQuery(DirectUpdateHandler2.java:340)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processDelete(RunUpdateProcessorFactory.java:72)\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processDelete(UpdateRequestProcessor.java:55)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalDelete(DistributedUpdateProcessor.java:437)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doDeleteByQuery(DistributedUpdateProcessor.java:835)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:657)\n\tat org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121)\n\tat org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:330)\n\tat org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:261)\n\tat org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:157)\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1699)\n\tat org.apache.solr.client.solrj.embedded.EmbeddedSolrServer.request(EmbeddedSolrServer.java:150)\n\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n\tat org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:285)\n\tat org.apache.solr.client.solrj.SolrServer.deleteByQuery(SolrServer.java:271)\n\tat org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.deleteSubtreeWriter(SolrIndexUpdate.java:161)\n\tat org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexUpdate.apply(SolrIndexUpdate.java:98)\n\tat org.apache.jackrabbit.oak.plugins.index.solr.index.SolrIndexDiff.leave(SolrIndexDiff.java:202)\n\tat org.apache.jackrabbit.oak.spi.commit.CompositeEditor.leave(CompositeEditor.java:74)\n\tat org.apache.jackrabbit.oak.plugins.index.IndexHookManagerDiff.leave(IndexHookManagerDiff.java:117)\n\tat org.apache.jackrabbit.oak.spi.commit.EditorHook$EditorDiff.process(EditorHook.java:115)\n\tat org.apache.jackrabbit.oak.spi.commit.EditorHook.process(EditorHook.java:80)\n\tat org.apache.jackrabbit.oak.spi.commit.EditorHook.processCommit(EditorHook.java:54)\n\tat org.apache.jackrabbit.oak.kernel.KernelNodeStoreBranch.merge(KernelNodeStoreBranch.java:144)\n\tat org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:266)\n\tat org.apache.jackrabbit.oak.core.RootImpl$2.run(RootImpl.java:1)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:337)\n\tat org.apache.jackrabbit.oak.core.RootImpl.commit(RootImpl.java:261)\n\tat org.apache.jackrabbit.oak.query.AbstractQueryTest.test(AbstractQueryTest.java:236)\n\tat org.apache.jackrabbit.oak.plugins.index.solr.query.SolrIndexQueryTest.sql2(SolrIndexQueryTest.java:79)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nCaused by: org.apache.lucene.queryparser.classic.ParseException: Cannot parse \'path_exact:/test*\': Lexical error at line 1, column 18.  Encountered: <EOF> after : ""/test*""\n\tat org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:130)\n\tat org.apache.solr.search.LuceneQParser.parse(LuceneQParserPlugin.java:72)\n\tat org.apache.solr.search.QParser.getQuery(QParser.java:143)\n\tat org.apache.solr.update.DirectUpdateHandler2.getQuery(DirectUpdateHandler2.java:310)\n\t... 58 more\nCaused by: org.apache.lucene.queryparser.classic.TokenMgrError: Lexical error at line 1, column 18.  Encountered: <EOF> after : ""/test*""\n\tat org.apache.lucene.queryparser.classic.QueryParserTokenManager.getNextToken(QueryParserTokenManager.java:1048)\n\tat org.apache.lucene.queryparser.classic.QueryParser.jj_ntk(QueryParser.java:638)\n\tat org.apache.lucene.queryparser.classic.QueryParser.Clause(QueryParser.java:246)\n\tat org.apache.lucene.queryparser.classic.QueryParser.Query(QueryParser.java:181)\n\tat org.apache.lucene.queryparser.classic.QueryParser.TopLevelQuery(QueryParser.java:170)\n\tat org.apache.lucene.queryparser.classic.QueryParserBase.parse(QueryParserBase.java:120)\n\t... 61 more\n{code}'}"
jackrabbit-oak,bugs-dot-jar_OAK-748_503451c1,"{'BugID': 'OAK-748', 'Summary': 'ContentMirrorStoreStrategy #insert fails to enforce uniqueness and is slow', 'Description': ""Following OAK-734 I've noticed that the _ContentMirrorStoreStrategy_ fails to enforce the uniqueness constraints assumed on the #insert method.\n\nIt is also responsible for a slowdown on the #insert method because of the behavior change of the Property2Index (very frequent saves instead of a bulk one).""}"
jackrabbit-oak,bugs-dot-jar_OAK-766_6fc5ea9d,"{'BugID': 'OAK-766', 'Summary': 'TreeImpl#*Location: unable retrieve child location if access to parent is denied', 'Description': 'as a consequence of OAK-709 we now have an issue with the way\nSessionDelegate and Root#getLocation access a node in the hierarchy\nwhich has an ancestor which is not accessible.\n\nspecifically RootImpl#getLocation will be served a NullLocation for the\nfirst ancestor which is not accessible and consequently any accessible\nchild node cannot be accessed.\n\nin order to reproduce the issue you may:\n\n- change AccessControlConfigurationImpl to use PermissionProviderImpl instead\n  of the tmp solution\n- and run o.a.j.oak.jcr.security.authorization.ReadTest#testReadDenied'}"
jackrabbit-oak,bugs-dot-jar_OAK-782_45b110e1,"{'BugID': 'OAK-782', 'Summary': 'MemoryNodeBuilder.setNode() loses property values ', 'Description': '{code}\nbuilder.setNode(""a"", nodeA);\nbuilder.child(""a"").setProperty(...);\n{code}\n\nAfter the 2nd line executed, properties initially present on {{nodeA}} are gone on {{builder.getNodeState()}}.'}"
jackrabbit-oak,bugs-dot-jar_OAK-846_7acb091a,"{'BugID': 'OAK-846', 'Summary': 'Branch conflicts not detected by MongoMK', 'Description': ""MongoMK does not correctly detect conflicts when changes are committed into multiple branches concurrently and then merged back.\n\nConflictTest already covers conflict detection for non-branch commits and mixed branch/non-branch changes, but is missing tests for conflicting branches. I'll commit an ignored test to illustrate the problem.""}"
jackrabbit-oak,bugs-dot-jar_OAK-847_65aa40dd,"{'BugID': 'OAK-847', 'Summary': 'Condition check broken in MemoryDocumentStore', 'Description': 'The Operation.CONTAINS_MAP_ENTRY condition check does not work correctly in the MemoryDocumentStore and may return false even when the condition is not met.'}"
jackrabbit-oak,bugs-dot-jar_OAK-888_6d82cb64,"{'BugID': 'OAK-888', 'Summary': 'PathUtils#getDepth returns 1 for empty path', 'Description': 'PathUtils#getDepths that the root path / has depth 0.\nhowever, passing in a empty string is accepted and returns 1.\n\naccording to the API contract getDepth is counting the number of elements\nin the path which for """" should IMO be zero.'}"
jackrabbit-oak,bugs-dot-jar_OAK-926_e1ae968c,"{'BugID': 'OAK-926', 'Summary': 'MongoMK: split documents when they are too large', 'Description': 'Currently, the MongoMK stores all revisions of a node in the same document. Once there are many revisions, the document gets very large.\n\nThe plan is to split the document when it gets big.\n\nIt looks like this isn\'t just a ""nice to have"", but also a problem for some use cases. Example stack trace:\n\n{code}\n21.07.2013 12:35:47.554 *ERROR* ...\nCaused by: java.lang.IllegalArgumentException: \'ok\' should never be null...\n\tat com.mongodb.CommandResult.ok(CommandResult.java:48)\n\tat com.mongodb.DBCollection.findAndModify(DBCollection.java:375)\n\tat org.apache.jackrabbit.oak.plugins.mongomk.MongoDocumentStore.findAndModify(MongoDocumentStore.java:302)\n\t... 32 more\n{code}\n\nat the same time in the MongoDB log:\n\n{code}\nSun Jul 21 12:35:47.334 [conn7] warning: log line attempted (159k) over max size(10k), \nprinting beginning and end ... \nAssertion: 10334:BSONObj size: 16795219 (0x53460001) is invalid. \nSize must be between 0 and 16793600(16MB) \nFirst element: :childOrder: { r1400279f22d-0-1: ""[]"", ...\n{code}\n'}"
jackrabbit-oak,bugs-dot-jar_OAK-93_0be7e8f0,"{'BugID': 'OAK-93', 'Summary': 'Tree has wrong parent after move', 'Description': 'After a move operation Tree.getParent() still returns the old parent.\n\n{code}\nTree x = r.getChild(""x"");\nTree y = r.getChild(""y"");\n\nroot.move(""x"", ""y/x"");\nassertEquals(""y"", x.getParent().getName());  // Fails\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1008_0c20bfd8,"{'BugID': 'LOG4J2-1008', 'Summary': ""org.apache.logging.log4j.core.config.plugins.util.ResolverUtil.extractPath(URL) incorrectly converts '+' characters to spaces"", 'Description': ""org.apache.logging.log4j.core.config.plugins.util.ResolverUtil.extractPath(URL) incorrectly converts '+' characters to spaces.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-101_c79a743b,"{'BugID': 'LOG4J2-101', 'Summary': 'Attribute ""format"" for SyslogAppender is mandatory', 'Description': 'In the SyslogAppender the configuration attribute ""Format"" must be present, otherwise a NullPointerException is thrown in method ""createAppender"" when the following statement is executed:\n\nif (format.equalsIgnoreCase(RFC5424))\n\nThe fix is simply to add a check so the ""equalsIgnoreCase"" is called only if the format variable isn\'t Null.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1025_a96b455c,"{'BugID': 'LOG4J2-1025', 'Summary': 'Custom java.util.logging.Level gives null Log4j Level and causes NPE', 'Description': 'I use a 3rd party library which uses custom non-standard java.util.logging.Level.\n\nThe Log4j JUL adapter will emit log event with level set to null in that case, which causes NullPointerException in a Log4j filter further on.\n\nThis is not acceptable. When encountering an unrecognised JUL Level, the JUL adapter should either: \n- emit some default Log4j Level\n- throw an Exception with a clear error message immediately\n- silently discard the log event\n- discard the log event and log a warning to the StatusLogger\n\n{code}\n java.lang.NullPointerException\n        at org.apache.logging.log4j.Level.isMoreSpecificThan(Level.java:163)\n        at org.apache.logging.log4j.core.filter.BurstFilter.filter(BurstFilter.java:129)\n        at org.apache.logging.log4j.core.filter.BurstFilter.filter(BurstFilter.java:101)\n        at org.apache.logging.log4j.core.Logger$PrivateConfig.filter(Logger.java:295)\n        at org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:122)\n        at org.apache.logging.log4j.spi.ExtendedLoggerWrapper.isEnabled(ExtendedLoggerWrapper.java:87)\n        at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:699)\n        at org.apache.logging.log4j.jul.WrappedLogger.log(WrappedLogger.java:50)\n        at org.apache.logging.log4j.jul.ApiLogger.log(ApiLogger.java:106)\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-102_7f391872,"{'BugID': 'LOG4J2-102', 'Summary': 'Bad priority in Syslog messages', 'Description': 'In class org.apache.logging.log4j.core.net.Priority the method getPriority returns a bad priority when used for Syslog messages (the only usage at the moment). The bug is in the statement:\n\nfacility.getCode() << 3 + Severity.getSeverity(level).getCode()\n\nThat\'s because the operator ""+"" takes precedence over ""<<"", and so the facility code isn\'t shifted by 3 but by ""3 + Severity.getSeverity(level).getCode()"".'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1046_11960820,"{'BugID': 'LOG4J2-1046', 'Summary': 'Circular Exception cause throws StackOverflowError', 'Description': 'If an exception with a circular-referenced exception (suppressed, or otherwise) is logged, log4j will throw a StackOverflowError:\n{code:java}\n        Exception e1 = new Exception();\n        Exception e2 = new Exception(e1);\n        e1.initCause(e2);\n        LogManager.getLogger().error(""Error"", e1);\n{code}\n\nWill throw the following:\n{code:java}\njava.lang.StackOverflowError\n\tat java.util.Vector.elementData(Vector.java:730)\n\tat java.util.Vector.elementAt(Vector.java:473)\n\tat java.util.Stack.peek(Stack.java:103)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.toExtendedStackTrace(ThrowableProxy.java:555)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:147)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.<init>(ThrowableProxy.java:148)\n        ...\n{code}\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-104_3b12e13d,"{'BugID': 'LOG4J2-104', 'Summary': 'LogManager initialization failed when running from Jdeveloper.', 'Description': 'This issue was incorrectly opened in bugzilla as https://issues.apache.org/bugzilla/show_bug.cgi?id=54053 by Evgeny.\n\n\nSteps to Reproduce:\n//config file presents or not - does not meter.\n\nRun / Debug simple application:\n\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\n\npublic class test_log {\n    public test_log() {\n        super();\n    }\n    \n    static Logger logger = LogManager.getLogger(test_log.class.getName());\n\n    public static void main(String[] args) {\n        test_log test_log = new test_log();\n      \n        logger.entry();\n\n        logger.debug(""test"");\n\n        logger.error(""test Err"");\n\n        logger.exit();\n    \n\n    }\n...\n}\n\nActual Results:\nFailed with error\njava.lang.ExceptionInInitializerError\n\tat view.test_log.<clinit>(test_log.java:13)\nCaused by: java.lang.ClassCastException: oracle.xml.parser.v2.DTD cannot be cast to org.w3c.dom.Element\n\tat java.util.XMLUtils.load(XMLUtils.java:61)\n\tat java.util.Properties.loadFromXML(Properties.java:852)\n\tat org.apache.logging.log4j.LogManager.<clinit>(LogManager.java:77)\n\nAdditional Info:\nWhen xmlparserv2.jar is deleted - application run fine.\nBut - it have to be presents - when deleted, JDeveloper failed to start.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1058_c8fd3c53,"{'BugID': 'LOG4J2-1058', 'Summary': 'Log4jMarker#contains(String) does not respect org.slf4j.Marker contract', 'Description': 'Expected behavior\n==============\n\'org.apache.logging.slf4j.Log4jMarker\' implements \'org.slf4j.Marker\'.\n\n\'org.slf4j.Marker#contains(String name)\' contract states that: ""If \'name\' is null the returned value is always false.""\nhttp://www.slf4j.org/apidocs/org/slf4j/Marker.html#contains%28java.lang.String%29\n\nActual behavior\n=============\n\'org.apache.logging.slf4j.Log4jMarker#contains(final String name)\'  throws \'IllegalArgumentException\' if \'name\' is null\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1061_86d8944f,"{'BugID': 'LOG4J2-1061', 'Summary': 'Log4jMarker#remove(Marker) does not respect org.slf4j.Marker contract', 'Description': 'Passing {{null}} to {{Log4jMarker#remove(Marker)}} throws a {{NullPointerException}} instead of return {{false}}.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1062_4cf831b6,"{'BugID': 'LOG4J2-1062', 'Summary': 'Log4jMarker#add(Marker) does not respect org.slf4j.Marker contract', 'Description': 'Passing {{null}} to {{Log4jMarker#add(Marker)}} throws a {{NullPointerException}} instead of an {{IllegalArgumentException}}.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1067_4786a739,"{'BugID': 'LOG4J2-1067', 'Summary': 'ThrowableProxy getExtendedStackTraceAsString throws NPE on deserialized nested exceptions', 'Description': 'In a similar vein to LOG4J2-914, I also am attempting to use log4j as a daemon log server.  The fix for LOG4J2-914 only solved the NPE problem for one dimensional exceptions. Nested exceptions also cause an NPE in the current implementation.  Here is a test/patch diff for the bug:\n\n{code}\n---\n .../org/apache/logging/log4j/core/impl/ThrowableProxy.java     |  2 +-\n .../org/apache/logging/log4j/core/impl/ThrowableProxyTest.java | 10 ++++++++++\n 2 files changed, 11 insertions(+), 1 deletion(-)\n\ndiff --git a/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java b/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java\nindex 67d55ec..307de58 100644\n--- a/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java\n+++ b/log4j-core/src/main/java/org/apache/logging/log4j/core/impl/ThrowableProxy.java\n@@ -207,7 +207,7 @@ public class ThrowableProxy implements Serializable {\n             return;\n         }\n         sb.append(""Caused by: "").append(cause).append(EOL);\n-        this.formatElements(sb, cause.commonElementCount, cause.getThrowable().getStackTrace(),\n+        this.formatElements(sb, cause.commonElementCount, cause.getStackTrace(),\n                 cause.extendedStackTrace, ignorePackages);\n         this.formatCause(sb, cause.causeProxy, ignorePackages);\n     }\ndiff --git a/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java b/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java\nindex 7019aa2..6eb5dbc 100644\n--- a/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java\n+++ b/log4j-core/src/test/java/org/apache/logging/log4j/core/impl/ThrowableProxyTest.java\n@@ -146,6 +146,16 @@ public class ThrowableProxyTest {\n \n         assertEquals(proxy.getExtendedStackTraceAsString(), proxy2.getExtendedStackTraceAsString());\n     }\n+    \n+    @Test\n+    public void testSerialization_getExtendedStackTraceAsStringWithNestedThrowable() throws Exception {\n+        final Throwable throwable = new RuntimeException(new IllegalArgumentException(""This is a test""));\n+        final ThrowableProxy proxy = new ThrowableProxy(throwable);\n+        final byte[] binary = serialize(proxy);\n+        final ThrowableProxy proxy2 = deserialize(binary);\n+\n+        assertEquals(proxy.getExtendedStackTraceAsString(), proxy2.getExtendedStackTraceAsString());\n+    }\n \n     @Test\n     public void testSerializationWithUnknownThrowable() throws Exception {\n-- \n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1068_e7bbeceb,"{'BugID': 'LOG4J2-1068', 'Summary': 'Exceptions not logged when using TcpSocketServer + SerializedLayout', 'Description': 'This issue was reported in BugZilla bug 57036: https://bz.apache.org/bugzilla/show_bug.cgi?id=57036.   The description there covers the problem well:\n\n""... in the Method format(final LogEvent event, final StringBuilder toAppendTo) in ExtendedThrowablePatternConverter writing the Stacktrace in the logfile on condition that the Throwable throwable from Log4jLogEvent is not null, but on the Socketserver the Throwable throwable is always null, because it\'s defined as transient.""\n\nI couldn\'t find the bug here in Jira, so I\'m reporting again in case it has been lost in the migration.\n\nIt\'s a major problem with a simple fix, so seems like it should be a high priority.\n\nI\'ve worked around it for now by plugging in my own ExtendedThrowablePatternConverter.  My fix is to change this line:\n\nif (throwable != null && options.anyLines() {\n\nto this:\n\nif ((throwable != null || proxy != null) && options.anyLines()) {\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1069_e9b628ec,"{'BugID': 'LOG4J2-1069', 'Summary': 'Improper handling of JSON escape chars when deserializing JSON log events', 'Description': 'There is an error in the handling of JSON escape characters while determining the log event boundaries in a JSON stream.  This error is causing log events with JSON escaped characters in the message string to be skipped.  The existing tests do not appear to cover this case, and other serialization types are not affected.  Here is a test/fix patch: \n\n{code}\ndiff --git a/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java b/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java\nindex 1b81644..8ed2732 100644\n--- a/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java\n+++ b/log4j-core/src/main/java/org/apache/logging/log4j/core/net/server/JsonInputStreamLogEventBridge.java\n@@ -55,8 +55,10 @@ public class JsonInputStreamLogEventBridge extends InputStreamLogEventBridge {\n         boolean inEsc = false;\n         for (int i = start; i < charArray.length; i++) {\n             final char c = charArray[i];\n-            if (!inEsc) {\n-                inEsc = false;\n+            if (inEsc) {\n+            \t// Skip this char and continue\n+            \tinEsc = false;\n+            } else { \n                 switch (c) {\n                 case EVENT_START_MARKER:\n                     if (!inStr) {\ndiff --git a/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java b/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java\nindex 891e278..2bdb3c3 100644\n--- a/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java\n+++ b/log4j-core/src/test/java/org/apache/logging/log4j/core/net/server/AbstractSocketServerTest.java\n@@ -69,7 +69,9 @@ public abstract class AbstractSocketServerTest {\n     private static final String MESSAGE = ""This is test message"";\n \n     private static final String MESSAGE_2 = ""This is test message 2"";\n-\n+    \n+    private static final String MESSAGE_WITH_SPECIAL_CHARS = ""{This}\\n[is]\\""n\\""a\\""\\r\\ntrue:\\n\\ttest,\\nmessage"";\n+    \n     static final int PORT_NUM = AvailablePortFinder.getNextAvailable();\n \n     static final String PORT = String.valueOf(PORT_NUM);\n@@ -158,6 +160,13 @@ public abstract class AbstractSocketServerTest {\n             testServer(m1, m2);\n         }\n     }\n+    \n+    \n+    @Test\n+    public void testMessagesWithSpecialChars() throws Exception {\n+        testServer(MESSAGE_WITH_SPECIAL_CHARS);\n+    }\n+    \n \n     private void testServer(final int size) throws Exception {\n         final String[] messages = new String[size];\n{code}\n\nThe test provided is simplistic and does not attempt to cover all possible special characters as the bug has to do with escaped characters in general.  XML and java serialization handle the special chars in my test string without issue - I did not attempt to locate similar cases in the other serialization types.\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-107_88641f49,"{'BugID': 'LOG4J2-107', 'Summary': 'Nesting pattern layout options is broken', 'Description': 'This pattern:\n\n{code:xml}\n<PatternLayout pattern=""%highlight{%d{dd MMM yyyy HH:mm:ss,SSS}{GMT+0} [%t] %-5level: %msg%n%throwable}"" />\n{code}\n\nIncorrectly outputs:\n\n{noformat}\n02 Nov 2012 16:51:14,907{GMT+0 [main] FATAL: Fatal message.\n}02 Nov 2012 16:51:14,910{GMT+0 [main] ERROR: Error message.\n}02 Nov 2012 16:51:14,914{GMT+0 [main] WARN : Warning message.\n}02 Nov 2012 16:51:14,917{GMT+0 [main] INFO : Information message.\n}02 Nov 2012 16:51:14,920{GMT+0 [main] DEBUG: Debug message.\n}02 Nov 2012 16:51:14,924{GMT+0 [main] TRACE: Trace message.\n}02 Nov 2012 16:51:14,929{GMT+0 [main] ERROR: Error message.\n{noformat}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1099_3f41ff48,"{'BugID': 'LOG4J2-1099', 'Summary': 'AbstractStringLayout implements Serializable, but is not Serializable', 'Description': '{{org.apache.logging.log4j.core.layout.AbstractLayout}} line 34 :\n{code}\n    // TODO: Charset is not serializable. Implement read/writeObject() ?\n    private final Charset charset;\n{code}\n\nThe developer has recognised that this class claims to be serializable, but is not actually serializable.\n\nThis actually has wide impact due to the fact that the Logger is holding onto the Layout via the {{org.apache.logging.log4j.core.Logger.PrivateConfig#config}} (XML in my case). Many projects, including Spring, do not use static Loggers and prefer getClass type approaches off of their abstract classes, i.e.:\n{code}\nprotected final Log logger = LogFactory.getLog(getClass());\n{code} \nThis actually can lead to use of spring session beans, which are serialized with the session, trying to serialize the logger also and failing due to this bug.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-113_fc3e9d2d,"{'BugID': 'LOG4J2-113', 'Summary': 'StructuredDataFilter defines ""pairs"" as attribute instead of element', 'Description': 'org.apache.logging.log4j.core.filter.StructuredDataFilter method createFilter defines parameter ""pairs"" as follows:\n\n@PluginAttr(""pairs"") KeyValuePair[] pairs\n\nIt should have been:\n\n@PluginElement(""pairs"") KeyValuePair[] pairs\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-114_afcf92eb,"{'BugID': 'LOG4J2-114', 'Summary': 'StructuredDataMessage is incorrectly validating value length instead of key length', 'Description': 'During execution of method SLF4JLogger.log, the following exception is thrown during creation of the StructuredMessage with a key longer than 32 characters.\n\njava.lang.IllegalArgumentException: Structured data values are limited to 32 characters. key: memo value: This is a very long test memo to demonstrate the issue\n\nThe validation should be on key length and not the value length.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1153_8acedb4e,"{'BugID': 'LOG4J2-1153', 'Summary': 'Unable to define only rootLogger in a properties file.', 'Description': 'I\'ve changed the version of log4j2 from *2.3* to *2.4* in order to load the  configuration via properties file. So i have converted the xml file, that defines only {{<Root>}} in {{<Loggers>}} element, into a properties file. \nThis is a preview of the xml file :\n{code:xml}\n  <Loggers>\n      <Root level=""info"">\n         <AppenderRef ref=""ConsoleAppender""/>\n     </Root>\n  </Loggers>\n{code}\nAnd this is a preview of the properties file :\n\n{code}\nrootLogger.level = info\nrootLogger.appenderRefs = console\nrootLogger.appenderRef.console.ref = ConsoleAppender\n{code}\n\nThis configuration throw a null pointer exception :\n{noformat}Exception in thread ""main"" java.lang.ExceptionInInitializerError\nCaused by: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:132)\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:44)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:491)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:461)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:493)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:510)\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:199)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:146)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)\n\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:264)\n\tat org.apache.log4j.Logger$PrivateManager.getContext(Logger.java:59)\n\tat org.apache.log4j.Logger.getLogger(Logger.java:41)\n{noformat}\n\nIn order to make this configuration work, i had to add the {{loggers}} component and fill the identifiers. My question is why in xml file we can define only a root logger and it works fine, and in a properties file it does not work ? \n '}"
logging-log4j2,bugs-dot-jar_LOG4J2-1153_9f924f10,"{'BugID': 'LOG4J2-1153', 'Summary': 'Unable to define only rootLogger in a properties file.', 'Description': 'I\'ve changed the version of log4j2 from *2.3* to *2.4* in order to load the  configuration via properties file. So i have converted the xml file, that defines only {{<Root>}} in {{<Loggers>}} element, into a properties file. \nThis is a preview of the xml file :\n{code:xml}\n  <Loggers>\n      <Root level=""info"">\n         <AppenderRef ref=""ConsoleAppender""/>\n     </Root>\n  </Loggers>\n{code}\nAnd this is a preview of the properties file :\n\n{code}\nrootLogger.level = info\nrootLogger.appenderRefs = console\nrootLogger.appenderRef.console.ref = ConsoleAppender\n{code}\n\nThis configuration throw a null pointer exception :\n{noformat}Exception in thread ""main"" java.lang.ExceptionInInitializerError\nCaused by: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:132)\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:44)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:491)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:461)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:493)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:510)\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:199)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:146)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)\n\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:264)\n\tat org.apache.log4j.Logger$PrivateManager.getContext(Logger.java:59)\n\tat org.apache.log4j.Logger.getLogger(Logger.java:41)\n{noformat}\n\nIn order to make this configuration work, i had to add the {{loggers}} component and fill the identifiers. My question is why in xml file we can define only a root logger and it works fine, and in a properties file it does not work ? \n '}"
logging-log4j2,bugs-dot-jar_LOG4J2-1251_424068f7,"{'BugID': 'LOG4J2-1251', 'Summary': 'JUL bridge broken', 'Description': 'org.apache.logging.log4j.jul.ApiLogger doesnt behave the same depending where we come from (logger.info() vs logger.log() typically)\n\nThe main difference is the message factory used.\n\nfor this statement:\n\n{code}\nlogger.info(""{foo}"");\n{code}\n\na SimpleMessage will be emitted but for\n\n{code}\nlogger.log(recordWithSameContent);\n{code}\n\na MessageFormatMessage will be emitted making the log statement failling.\n\norg.apache.logging.log4j.jul.ApiLogger#log(java.util.logging.LogRecord) should be reworked to handle such a case.\n\nHere how to reproduce it:\n\n{code}\nLogger.getLogger(""foo"").info(""{test}"");\nLogger.getLogger(""foo"").log(new LogRecord(Level.INFO, ""{test}""));\n{code}\n\nThe fix is as simple as testing org.apache.logging.log4j.jul.ApiLogger#log(java.util.logging.LogRecord) and if null don\'t call logger.getMessageFactory().newMessage(record.getMessage(), record.getParameters()) but logger.getMessageFactory().newMessage(record.getMessage())'}"
logging-log4j2,bugs-dot-jar_LOG4J2-127_029e79da,"{'BugID': 'LOG4J2-127', 'Summary': 'Methods info, warn, error, fatal with marker and message do not pass the marker', 'Description': 'The follwing methods do not log the message, because the marker is not passed to isXxxEnabled:\n\nAbstractLogger.info(Marker, Message)\nAbstractLogger.warn(Marker, Message)\nAbstractLogger.error(Marker, Message)\nAbstractLogger.fatal(Marker, Message)\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1310_c6318b63,"{'BugID': 'LOG4J2-1310', 'Summary': 'JndiLookup mindlessly casts to String and should use String.valueOf()', 'Description': 'The value returned from Context.lookup() is cast to String which can cause problems if the value is, well, anything else.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1372_1d12bf0e,"{'BugID': 'LOG4J2-1372', 'Summary': 'XMLLayout indents, but not the first child tag (<Event>)', 'Description': 'I am using log4j 2.5 to print the logs via XMLLayout. I have set compact=""true"", hence the new line and indents of sub tags work correctly. However I have noticed that the first child tag is not indented correctly. \n\nFollowing is such a sample where <Events> and <Event> are at the same indent level (0 indent). \n\n{code:xml}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Events xmlns=""http://logging.apache.org/log4j/2.0/events"">\n<Event xmlns=""http://logging.apache.org/log4j/2.0/events"" timeMillis=""1460974404123"" thread=""main"" level=""INFO"" loggerName=""com.foo.Bar"" endOfBatch=""true"" loggerFqcn=""org.apache.logging.log4j.spi.AbstractLogger"" threadId=""11"" threadPriority=""5"">\n  <Message>First Msg tag must be in level 2 after correct indentation</Message>\n</Event>\n\n<Event xmlns=""http://logging.apache.org/log4j/2.0/events"" timeMillis=""1460974404133"" thread=""main"" level=""INFO"" loggerName=""com.foo.Bar"" endOfBatch=""true"" loggerFqcn=""org.apache.logging.log4j.spi.AbstractLogger"" threadId=""11"" threadPriority=""5"">\n  <Message>Second Msg tag must also be in level 2 after correct indentation</Message>\n</Event>\n\n</Events>\n{code}\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1372_ffedf33f,"{'BugID': 'LOG4J2-1372', 'Summary': 'XMLLayout indents, but not the first child tag (<Event>)', 'Description': 'I am using log4j 2.5 to print the logs via XMLLayout. I have set compact=""true"", hence the new line and indents of sub tags work correctly. However I have noticed that the first child tag is not indented correctly. \n\nFollowing is such a sample where <Events> and <Event> are at the same indent level (0 indent). \n\n{code:xml}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Events xmlns=""http://logging.apache.org/log4j/2.0/events"">\n<Event xmlns=""http://logging.apache.org/log4j/2.0/events"" timeMillis=""1460974404123"" thread=""main"" level=""INFO"" loggerName=""com.foo.Bar"" endOfBatch=""true"" loggerFqcn=""org.apache.logging.log4j.spi.AbstractLogger"" threadId=""11"" threadPriority=""5"">\n  <Message>First Msg tag must be in level 2 after correct indentation</Message>\n</Event>\n\n<Event xmlns=""http://logging.apache.org/log4j/2.0/events"" timeMillis=""1460974404133"" thread=""main"" level=""INFO"" loggerName=""com.foo.Bar"" endOfBatch=""true"" loggerFqcn=""org.apache.logging.log4j.spi.AbstractLogger"" threadId=""11"" threadPriority=""5"">\n  <Message>Second Msg tag must also be in level 2 after correct indentation</Message>\n</Event>\n\n</Events>\n{code}\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-139_50e19247,"{'BugID': 'LOG4J2-139', 'Summary': 'NPE while using SocketAppender', 'Description': 'I try to use the SocketAppender and get the following ERROR/NPE:\n\n2013-01-06 00:54:14,024 DEBUG Generated plugins in 0.000032000 seconds\n2013-01-06 00:54:14,044 DEBUG Calling createLayout on class org.apache.logging.log4j.core.layout.PatternLayout for element PatternLayout with params(pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n"", Configuration(/Users/jhuxhorn/Documents/Projects/huxi/lilith/sandbox/log4j2-sandbox/build/resources/main/log4j2.xml), null, charset=""null"")\n2013-01-06 00:54:14,045 DEBUG Generated plugins in 0.000029000 seconds\n2013-01-06 00:54:14,049 DEBUG Calling createAppender on class org.apache.logging.log4j.core.appender.ConsoleAppender for element Console with params(PatternLayout(%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n), null, target=""SYSTEM_OUT"", name=""Console"", suppressExceptions=""null"")\n2013-01-06 00:54:14,049 DEBUG Calling createLayout on class org.apache.logging.log4j.core.layout.SerializedLayout for element SerializedLayout\n2013-01-06 00:54:14,052 DEBUG Calling createAppender on class org.apache.logging.log4j.core.appender.SocketAppender for element Socket with params(host=""localhost"", port=""4560"", protocol=""null"", reconnectionDelay=""null"", name=""Socket"", immediateFlush=""null"", suppressExceptions=""null"", SerializedLayout(org.apache.logging.log4j.core.layout.SerializedLayout@5cc4211b), null)\n2013-01-06 00:54:14,054 ERROR Unable to invoke method createAppender in class org.apache.logging.log4j.core.appender.SocketAppender for element Socket java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.logging.log4j.core.config.BaseConfiguration.createPluginObject(BaseConfiguration.java:711)\n\tat org.apache.logging.log4j.core.config.BaseConfiguration.createConfiguration(BaseConfiguration.java:477)\n\tat org.apache.logging.log4j.core.config.BaseConfiguration.createConfiguration(BaseConfiguration.java:469)\n\tat org.apache.logging.log4j.core.config.BaseConfiguration.doConfigure(BaseConfiguration.java:156)\n\tat org.apache.logging.log4j.core.config.BaseConfiguration.start(BaseConfiguration.java:114)\n\tat org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:251)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:267)\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:134)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:75)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:30)\n\tat org.apache.logging.log4j.LogManager.getLogger(LogManager.java:165)\n\tat org.apache.logging.log4j.LogManager.getLogger(LogManager.java:174)\n\tat de.huxhorn.lilith.sandbox.Log4j2Sandbox.main(Log4j2Sandbox.java:43)\nCaused by: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.appender.SocketAppender.createSocketManager(SocketAppender.java:95)\n\tat org.apache.logging.log4j.core.appender.SocketAppender.createAppender(SocketAppender.java:86)\n\t... 17 more\n\n2013-01-06 00:54:14,056 ERROR Null object returned for Socket in appenders\n\n\nMy configuration looks like this:\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<configuration status=""debug"">\n\t<appenders>\n\t\t<Console name=""Console"" target=""SYSTEM_OUT"">\n\t\t\t<PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} --- %msg%n""/>\n\t\t</Console>\n\t\t<Socket name=""Socket"" host=""localhost"" port=""4560"">\n\t\t\t<SerializedLayout />\n\t\t</Socket>\n\t</appenders>\n\t<loggers>\n\t\t<root level=""all"">\n\t\t\t<appender-ref ref=""Console""/>\n\t\t\t<appender-ref ref=""Socket""/>\n\t\t</root>\n\t</loggers>\n</configuration>\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1402_7792679c,"{'BugID': 'LOG4J2-1402', 'Summary': 'Exception when using log4j2.properties and logger with dot', 'Description': 'After upgrading log4j2 from 2.5 to 2.6 I get the following exception:\n\n{quote}\nException in thread ""main"" org.apache.logging.log4j.core.config.ConfigurationException: No name attribute provided for Logger org\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationBuilder.createLogger(PropertiesConfigurationBuilder.java:215)\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationBuilder.build(PropertiesConfigurationBuilder.java:140)\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:52)\n\tat org.apache.logging.log4j.core.config.properties.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:34)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:510)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:450)\n\tat org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:257)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:560)\n\tat org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:577)\n\tat org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:212)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:152)\n\tat org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:45)\n\tat org.apache.logging.log4j.LogManager.getContext(LogManager.java:194)\n\tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:103)\n\tat org.apache.logging.log4j.jcl.LogAdapter.getContext(LogAdapter.java:39)\n\tat org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)\n\tat org.apache.logging.log4j.jcl.LogFactoryImpl.getInstance(LogFactoryImpl.java:40)\n\tat org.apache.logging.log4j.jcl.LogFactoryImpl.getInstance(LogFactoryImpl.java:55)\n\tat org.apache.commons.logging.LogFactory.getLog(LogFactory.java:655)\n\tat org.springframework.context.support.AbstractApplicationContext.<init>(AbstractApplicationContext.java:159)\n\tat org.springframework.context.support.AbstractApplicationContext.<init>(AbstractApplicationContext.java:223)\n\tat org.springframework.context.support.AbstractRefreshableApplicationContext.<init>(AbstractRefreshableApplicationContext.java:88)\n\tat org.springframework.context.support.AbstractRefreshableConfigApplicationContext.<init>(AbstractRefreshableConfigApplicationContext.java:58)\n\tat org.springframework.context.support.AbstractXmlApplicationContext.<init>(AbstractXmlApplicationContext.java:61)\n\tat org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:136)\n\tat org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)\nx.y.z.Start.main(Start.java:12)\n{quote}\n\nThe parameter ""key"" has the value ""org"" and the parameter properties has the value {code}{activiti.engine.impl.level=info, activiti.engine.impl.name=org.activiti.engine.impl}{code}\n\nThe log4j2.properties in use:\n\n{code}\n# Root logger option\nrootLogger.level = info\nrootLogger.appenderRefs = stdout\nrootLogger.appenderRef.stdout.ref = STDOUT\n\n# Redirect log messages to console\nappenders = stdout\nappender.stdout.type = Console\nappender.stdout.name = STDOUT\nappender.stdout.layout.type = PatternLayout\nappender.stdout.layout.pattern = %d %-5p [%t] %c - %m%n\n{code}\n\nSadly I have not been able to reproduce the issue in a simple standalone application.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-1406_a523dcd5,"{'BugID': 'LOG4J2-1406', 'Summary': '2.6 is re-logging prior throwable instead of logging the throwable that is currently passed in by application code', 'Description': 'Hi,\n\nI just want to make sure you saw the issue I submitted PR for a couple days ago.  It seems a very serious issue in 2.6:\n\nhttps://github.com/apache/logging-log4j2/pull/31\n\nThanks,\nTrask\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-143_1461f1f6,"{'BugID': 'LOG4J2-143', 'Summary': 'MessagePatternConverter throws a NullPointerException if the log message is null', 'Description': 'If the application does\n\nlogger.debug(msg)\n\nwhere the value of msg is null MessagePatternConverter will get a NullPointerException.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-147_17296089,"{'BugID': 'LOG4J2-147', 'Summary': ""ThreadContextMapFilter doesn't match properly when a single keyvalue is provided"", 'Description': ""I was testing out a global ThreadContextMapFilter and noticed it wasn't matching properly.  I took a closer look at the code and found because it wasn't matching the value to the value on the context but rather the key.\n\nI changed it to use the value as the argument to equals and this fixed it.  Here is the diff of what I am running with.\n\n\ndiff --git a/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java b/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java\nindex 9ad6cab..b3f3838 100644\n--- a/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFi\n+++ b/core/src/main/java/org/apache/logging/log4j/core/filter/ThreadContextMapFilter.java\n@@ -96,7 +96,7 @@ public class ThreadContextMapFilter extends MapFilter {\n                 }\n             }\n         } else {\n-            match = key.equals(ThreadContext.get(key));\n+            match = value.equals(ThreadContext.get(key));\n         }\n         return match ? onMatch : onMismatch;\n     }\n\n""}"
logging-log4j2,bugs-dot-jar_LOG4J2-177_f91ce934,"{'BugID': 'LOG4J2-177', 'Summary': 'ERROR StatusLogger An exception occurred processing Appender udpsocket java.lang.NullPointerException', 'Description': 'This seems to be a race condition of some kind. Two threads are exiting at almost exactly the same time, and both print a message as they exit. This exception happens every 3 or 4 times the code is run, so it\'s easily reproducible.\n\nERROR StatusLogger An exception occurred processing Appender udpsocket java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.net.DatagramOutputStream.flush(DatagramOutputStream.java:93)\n\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:146)\n\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:117)\n\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:335)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:316)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:281)\n\tat org.apache.logging.log4j.core.Logger.log(Logger.java:108)\n\tat org.apache.logging.log4j.spi.AbstractLogger.trace(AbstractLogger.java:250)\n\tat com.galmont.automation.framework.gui.handlers.RunCycleHandler$2.run(RunCycleHandler.java:128)\n\tat java.lang.Thread.run(Unknown Source)\n\nMy configuration file:\n\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<configuration status=""OFF"">\n\t\n\t<appenders>\n\t\t<Console name=""console"" target=""SYSTEM_OUT"">\n\t\t\t<PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" />\n\t\t</Console>\n\t\t<Socket name=""udpsocket"" host=""localhost"" port=""90"" protocol=""UDP"">\n\t\t\t<PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" />\n\t\t</Socket>\n\t\t<RollingFile name=""RollingFile"" fileName=""logs/app.log"" filePattern=""logs/$${date:yyyy-MM}/app-%d{MM-dd-yyyy}-%i.log.gz"">\n\t\t\t<PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" />\n\t\t\t<Policies>\n\t\t\t\t<SizeBasedTriggeringPolicy size=""20 MB"" />\n\t\t\t</Policies>\n\t\t</RollingFile>\n\t</appenders>\n\t\n\t<loggers>\n\t\t<root level=""trace"">\n\t\t\t<appender-ref ref=""console"" />\n\t\t\t<appender-ref ref=""udpsocket"" />\n\t\t\t<appender-ref ref=""RollingFile"" />\n\t\t</root>\n\t</loggers>\n\t\n</configuration>'}"
logging-log4j2,bugs-dot-jar_LOG4J2-210_aeb6fc9d,"{'BugID': 'LOG4J2-210', 'Summary': 'MapMessage does not enclose key in quotes when generating XML', 'Description': 'MapMessage does not enclose key in quotes when generating XML'}"
logging-log4j2,bugs-dot-jar_LOG4J2-219_ed951c76,"{'BugID': 'LOG4J2-219', 'Summary': 'Named logger without root logger ends up with empty Appenders map - does not log anything', 'Description': 'On the log4j-user mailing list, Peter DePasquale gave this test case that demonstrates the problem:\n\nNote that the configuration has no root logger, but only contains a named logger.\n\nIn a debugger I found that the LoggerConfig for ""logtest.LogTest"" ended up with an empty ""appenders"" Map<String, AppenderControl<?>>. The appenderRefs list did contain an AppenderRef object but in #callAppenders there are no AppenderControl objects to call...  \n\n(Sorry, I have been unable to find out the underlying cause yet.)\n\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<configuration status=""warn"">\n\t<appenders>\n\t\t<File name=""tracelog"" fileName=""trace-log.txt"" \n\t\t\t\timmediateFlush=""true"" append=""false"">\n\t\t\t<PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n""/>\n\t\t</File>\n\t</appenders>\n\t\n\t<loggers>\n\t\t<logger name=""logtest.LogTest"" level=""trace"">\n\t\t\t<appender-ref ref=""tracelog""/>\n\t\t</logger>\n\t</loggers>\n</configuration>\n\n\n\npackage logtest;\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport org.apache.logging.log4j.core.config.XMLConfigurationFactory;\n\npublic class LogTest {\n\tpublic static void main(String[] args) {\n\t\tSystem.setProperty(XMLConfigurationFactory.CONFIGURATION_FILE_PROPERTY,\n\t\t\t\t""log4j2-roottest.xml"");\n\n\t\tLogger logger = LogManager.getLogger(LogTest.class);\n\t\tlogger.trace(""This is a trace message"");\n\t\tlogger.info(""This is an info message"");\n\t\tlogger.warn(""This is a warning message"");\n\t}\n}\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-234_2d7d6311,"{'BugID': 'LOG4J2-234', 'Summary': 'RegexFilter crashes as context-wide filter', 'Description': 'If a RegexFilter is used as a context-wide filter,\nthen a call like\n  logger.isDebugEnabled()\nleads to a Null-Pointer-Exception, because the RegexFilter is called with the message ""null"".\nThe stack-trace (2.0-beta5) is:\n\tat org.apache.logging.log4j.core.filter.RegexFilter.filter(RegexFilter.java:60)\n\tat org.apache.logging.log4j.core.filter.CompositeFilter.filter(CompositeFilter.java:176)\n\tat org.apache.logging.log4j.core.Logger$PrivateConfig.filter(Logger.java:317)\n\tat org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:128)\n\tat org.apache.logging.log4j.spi.AbstractLogger.isTraceEnabled(AbstractLogger.java:1129)\n\n\nIn the MarkerFilter is the code\n  return marker != null && ...\ni.e. it is only necessary to change line 60 to\n  return msg != null && filter(msg.toString)\nin RegexFilter (I do not know how to do this correctly...)\n\nIn line 77, this check is done; in line 66 and 72 the same problem may arise...\n\nGreetings,\nGerald Kroisandt\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-258_7b38965d,"{'BugID': 'LOG4J2-258', 'Summary': 'HTML layout does not output meta element for charset.', 'Description': 'HTML layout does not output meta element for charset.\nHEAD element should include a child element, like:\n{code:xml}\n<meta charset=""UTF-8""/>\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-259_09175c8b,"{'BugID': 'LOG4J2-259', 'Summary': 'HTML layout does not specify charset in content type', 'Description': 'HTML layout does not specify charset in content type'}"
logging-log4j2,bugs-dot-jar_LOG4J2-260_9d817953,"{'BugID': 'LOG4J2-260', 'Summary': 'XML layout does not specify charset in content type', 'Description': 'XML layout does not specify charset in content type'}"
logging-log4j2,bugs-dot-jar_LOG4J2-268_8faf7f77,"{'BugID': 'LOG4J2-268', 'Summary': 'Berkeley (persistent) agent for FlumeAppender only works with MapMessages (and thus not slf4j)', 'Description': ""If I try and use the persistent FlumeAppender with slf4j then I get a NullPointerException in FlumePersistentManager.send because there is no GUID header.\n\n(My repro here was using a copy of Flume modified to use log4j2 - while this particular repro is exotic I'm confident that the general case detailed above will be very common).\n\nThere is no GUID header because the FlumeEvent constructor only creates one if the message is a MapMessage.\n\nIf the user is using slf4j then all messages are PersistentMessages - and thus will cause this logging to fail.\n\nThe GUID is required because it's used as a key in the BerkeleyDB storage.\n\nMy attempts at a simple fix ran afoul of the key lookup from the headers in FlumePersisentManager.WriterThread.run(). ""}"
logging-log4j2,bugs-dot-jar_LOG4J2-293_25cb587a,"{'BugID': 'LOG4J2-293', 'Summary': 'classloader URI scheme broken or insufficient when using Log4jContextListener', 'Description': ""I'm trying to migrate to Log4j2, and things looked promising when I spotted Log4jContextListener.\n\nHowever, there are too many holes.\n\nFirstly, I tried using classpath: as a scheme, and nothing blew up, so I assumed I'd got it right.\n\nThen I *looked at the code* (which shouldn't be how we find out) and eventually discovered some code relating to a 'classloader' scheme.\n\nStill silent failure.  It seems that the classpath is not being searched, perhaps just the WAR classloader, not the JARs in WEB-INF/lib.\n\nNext I tried omitting the / (i.e. using classloader:log4j2.xml) and got a NullPointerException.\n\nCan you please document what schemes are supported and what you expect them to do, and *not fail silently* when a configuration file is specified, but nothing happens.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-293_ca59ece6,"{'BugID': 'LOG4J2-293', 'Summary': 'classloader URI scheme broken or insufficient when using Log4jContextListener', 'Description': ""I'm trying to migrate to Log4j2, and things looked promising when I spotted Log4jContextListener.\n\nHowever, there are too many holes.\n\nFirstly, I tried using classpath: as a scheme, and nothing blew up, so I assumed I'd got it right.\n\nThen I *looked at the code* (which shouldn't be how we find out) and eventually discovered some code relating to a 'classloader' scheme.\n\nStill silent failure.  It seems that the classpath is not being searched, perhaps just the WAR classloader, not the JARs in WEB-INF/lib.\n\nNext I tried omitting the / (i.e. using classloader:log4j2.xml) and got a NullPointerException.\n\nCan you please document what schemes are supported and what you expect them to do, and *not fail silently* when a configuration file is specified, but nothing happens.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-302_300bc575,"{'BugID': 'LOG4J2-302', 'Summary': 'NDCPatternConverter broken in beta7', 'Description': 'After an upgrade from version 2.0-beta4 to beta7 the NDCPatternConverter writes an object-ID instead of the content of the NDC-stack. \n\nWe are using an pattern with ""[%0.50x]"". In beta4 the resulting output looks like ""[cbi@CE03178]"" which means username and machine. Now in beta7 it looks like ""[logging.log4j.spi.MutableThreadContextStack@875ef7]"".\n\nI analysed the issue in NDCPatternConverter.format(...) method, where event.getContextStack() is called and the result is passed to StringBuilder.append(...), which means, that the toString()-method will be invoked. \nIn beta4 getContextStack() returns an instance of ImmutableStack. This class inherits its toString() method from AbstractList, where the elements of the collection will be formatted human-readable. \nNow in beta7 there comes an instance of MutableThreadContextStack which isn\'t derived from AbstractList but implements the Collection-Interface. The toString() method comes from Object and returns the name of the class and an object-ID instead of the context of the unterlying stack/collection.\n\nIn my opinion you just need to copy or derive the toString() method from AbstractList to solve this issue. Thank you in advance!\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-310_3f1e0fdc,"{'BugID': 'LOG4J2-310', 'Summary': 'SMTPAppender does not send mails with error or fatal level without prior info event', 'Description': 'When using an SMTPAppender a mail is only delivered on a fatal event if there occured an info event before.\nPrior fatal events are ignored by SMTPAppender - other Appenders log them.\n\nA more detailed explanation/discussion including an example program can be found at:\nhttp://stackoverflow.com/questions/17657983/log4j2-smtpappender-does-not-send-mails-with-error-or-fatal-level'}"
logging-log4j2,bugs-dot-jar_LOG4J2-344_8dead3bb,"{'BugID': 'LOG4J2-344', 'Summary': 'Log4j2 doesnt work with Weblogic 12c', 'Description': 'I get a ""Context destroyed before it was initialized"" exception, the problem seems to be that the servlet filters init method is not being called by WebLogic, not sure why...'}"
logging-log4j2,bugs-dot-jar_LOG4J2-359_1df1db27,"{'BugID': 'LOG4J2-359', 'Summary': 'Log4jServletContextListener does not work on Weblogic 12.1.1 (12c) with web-app version ""2.5""', 'Description': 'I have Weblogic 12c running. My web-app is version ""2.5"".\n\nFollowing is a snippet from my web.xml \n\n{code:xml}\n<web-app xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""\n\txmlns=""http://java.sun.com/xml/ns/javaee""\n\txsi:schemaLocation=""http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd""\n\tid=""WebApp_ID"" version=""2.5"">\n\t<display-name>pec-service</display-name>\n\t<context-param>\n\t\t<param-name>log4jConfiguration</param-name>\n\t\t<param-value>file:/C:/log4j/dev.log4j.xml</param-value>\n\t</context-param>\n\n\t<listener> \n\t\t<listener-class>org.apache.logging.log4j.core.web.Log4jServletContextListener</listener-class> \n\t</listener>\t\n\n\t<filter>\n\t\t<filter-name>log4jServletFilter</filter-name>\n\t\t<filter-class>org.apache.logging.log4j.core.web.Log4jServletFilter</filter-class> \n\t</filter>\n\t<filter-mapping>\n\t\t<filter-name>log4jServletFilter</filter-name> \n\t\t<url-pattern>/*</url-pattern>\n\t\t<dispatcher>REQUEST</dispatcher>\n\t\t<dispatcher>FORWARD</dispatcher> \n\t\t<dispatcher>INCLUDE</dispatcher>\n\t\t<dispatcher>ERROR</dispatcher>\n\t</filter-mapping>\n\t\n</web-app>\n{code}\n\nHowever, on my server startup I am getting the following error - \n{code}\n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <HTTP> <BEA-101162> <User defined listener org.apache.logging.log4j.core.web.Log4jServletContextListener failed: java.lang.IllegalStateException: Context destroyed before it was initialized..\njava.lang.IllegalStateException: Context destroyed before it was initialized.\n\tat org.apache.logging.log4j.core.web.Log4jServletContextListener.contextDestroyed(Log4jServletContextListener.java:51)\n\tat weblogic.servlet.internal.EventsManager$FireContextListenerAction.run(EventsManager.java:583)\n\tat weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n\tat weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120)\n\tat weblogic.servlet.provider.WlsSubjectHandle.run(WlsSubjectHandle.java:57)\n\tTruncated. see log file for complete stacktrace\n> \n<Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149265> <Failure occurred in the execution of deployment request with ID ""1376691143681"" for task ""2"". Error is: ""weblogic.application.ModuleException""\nweblogic.application.ModuleException\n\tat weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708)\n\tat weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:213)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:208)\n\tat weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35)\n\tTruncated. see log file for complete stacktrace\nCaused By: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229)\n\tat weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726)\n\tat weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740)\n\tTruncated. see log file for complete stacktrace\n> \n<Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149202> <Encountered an exception while attempting to commit the 7 task for the application ""_auto_generated_ear_"".> \n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149004> <Failures were detected while initiating start task for application ""_auto_generated_ear_"".> \n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149078> <Stack trace for message 149004\nweblogic.application.ModuleException\n\tat weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708)\n\tat weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:213)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:208)\n\tat weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35)\n\tTruncated. see log file for complete stacktrace\nCaused By: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229)\n\tat weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726)\n\tat weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740)\n\tTruncated. see log file for complete stacktrace\n{code}\n\nIf I remove the listener & the filter, it works fine.\n\n{color:red}\nI did some research and found that even though the web-app is version ""2.5"", the {code}Log4jServletContainerInitializer{code} is getting invoked. \n{color}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-359_296ea4a5,"{'BugID': 'LOG4J2-359', 'Summary': 'Log4jServletContextListener does not work on Weblogic 12.1.1 (12c) with web-app version ""2.5""', 'Description': 'I have Weblogic 12c running. My web-app is version ""2.5"".\n\nFollowing is a snippet from my web.xml \n\n{code:xml}\n<web-app xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""\n\txmlns=""http://java.sun.com/xml/ns/javaee""\n\txsi:schemaLocation=""http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd""\n\tid=""WebApp_ID"" version=""2.5"">\n\t<display-name>pec-service</display-name>\n\t<context-param>\n\t\t<param-name>log4jConfiguration</param-name>\n\t\t<param-value>file:/C:/log4j/dev.log4j.xml</param-value>\n\t</context-param>\n\n\t<listener> \n\t\t<listener-class>org.apache.logging.log4j.core.web.Log4jServletContextListener</listener-class> \n\t</listener>\t\n\n\t<filter>\n\t\t<filter-name>log4jServletFilter</filter-name>\n\t\t<filter-class>org.apache.logging.log4j.core.web.Log4jServletFilter</filter-class> \n\t</filter>\n\t<filter-mapping>\n\t\t<filter-name>log4jServletFilter</filter-name> \n\t\t<url-pattern>/*</url-pattern>\n\t\t<dispatcher>REQUEST</dispatcher>\n\t\t<dispatcher>FORWARD</dispatcher> \n\t\t<dispatcher>INCLUDE</dispatcher>\n\t\t<dispatcher>ERROR</dispatcher>\n\t</filter-mapping>\n\t\n</web-app>\n{code}\n\nHowever, on my server startup I am getting the following error - \n{code}\n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <HTTP> <BEA-101162> <User defined listener org.apache.logging.log4j.core.web.Log4jServletContextListener failed: java.lang.IllegalStateException: Context destroyed before it was initialized..\njava.lang.IllegalStateException: Context destroyed before it was initialized.\n\tat org.apache.logging.log4j.core.web.Log4jServletContextListener.contextDestroyed(Log4jServletContextListener.java:51)\n\tat weblogic.servlet.internal.EventsManager$FireContextListenerAction.run(EventsManager.java:583)\n\tat weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:321)\n\tat weblogic.security.service.SecurityManager.runAs(SecurityManager.java:120)\n\tat weblogic.servlet.provider.WlsSubjectHandle.run(WlsSubjectHandle.java:57)\n\tTruncated. see log file for complete stacktrace\n> \n<Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149265> <Failure occurred in the execution of deployment request with ID ""1376691143681"" for task ""2"". Error is: ""weblogic.application.ModuleException""\nweblogic.application.ModuleException\n\tat weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708)\n\tat weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:213)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:208)\n\tat weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35)\n\tTruncated. see log file for complete stacktrace\nCaused By: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229)\n\tat weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726)\n\tat weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740)\n\tTruncated. see log file for complete stacktrace\n> \n<Aug 16, 2013 3:12:32 PM PDT> <Error> <Deployer> <BEA-149202> <Encountered an exception while attempting to commit the 7 task for the application ""_auto_generated_ear_"".> \n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149004> <Failures were detected while initiating start task for application ""_auto_generated_ear_"".> \n<Aug 16, 2013 3:12:32 PM PDT> <Warning> <Deployer> <BEA-149078> <Stack trace for message 149004\nweblogic.application.ModuleException\n\tat weblogic.servlet.internal.WebAppModule.startContexts(WebAppModule.java:1708)\n\tat weblogic.servlet.internal.WebAppModule.start(WebAppModule.java:781)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:213)\n\tat weblogic.application.internal.flow.ModuleStateDriver$3.next(ModuleStateDriver.java:208)\n\tat weblogic.application.utils.StateMachineDriver.nextState(StateMachineDriver.java:35)\n\tTruncated. see log file for complete stacktrace\nCaused By: java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:44)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializer(WebAppServletContext.java:1271)\n\tat weblogic.servlet.internal.WebAppServletContext.initContainerInitializers(WebAppServletContext.java:1229)\n\tat weblogic.servlet.internal.WebAppServletContext.preloadResources(WebAppServletContext.java:1726)\n\tat weblogic.servlet.internal.WebAppServletContext.start(WebAppServletContext.java:2740)\n\tTruncated. see log file for complete stacktrace\n{code}\n\nIf I remove the listener & the filter, it works fine.\n\n{color:red}\nI did some research and found that even though the web-app is version ""2.5"", the {code}Log4jServletContainerInitializer{code} is getting invoked. \n{color}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-368_a8a24357,"{'BugID': 'LOG4J2-368', 'Summary': 'PatternLayout in 1.2 bridge missing constructor', 'Description': 'java.lang.NoSuchMethodError: org.apache.log4j.PatternLayout.<init>(Ljava/lang/String;)V\n\tat org.apache.velocity.runtime.log.Log4JLogChute.initAppender(Log4JLogChute.java:117) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.log.Log4JLogChute.init(Log4JLogChute.java:85) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.log.LogManager.createLogChute(LogManager.java:157) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.log.LogManager.updateLog(LogManager.java:269) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.RuntimeInstance.initializeLog(RuntimeInstance.java:871) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.RuntimeInstance.init(RuntimeInstance.java:262) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.RuntimeInstance.requireInitialization(RuntimeInstance.java:302) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.runtime.RuntimeInstance.getTemplate(RuntimeInstance.java:1531) ~[velocity-1.7-dep.jar:1.7]\n\tat org.apache.velocity.app.VelocityEngine.mergeTemplate(VelocityEngine.java:343) ~[velocity-1.7-dep.jar:1.7]'}"
logging-log4j2,bugs-dot-jar_LOG4J2-378_ef8517e4,"{'BugID': 'LOG4J2-378', 'Summary': 'Logging generates file named ${sys on some systems', 'Description': 'In a webapp I\'m setting a system property in my apps ServletContextListener, and using that system property in my log4j2.xml file, like so:\n{code}\n<appender type=""FastFile"" name=""File"" fileName=""${sys:catalina.home}/logs/${sys:application-name}.log"">\n{code}\nOn my Windows machine, a log file named ""${sys."" (always 0 bytes) is being created instead of a log file with the application-name. The same war deployed on one of our linux servers does not create a ${sys."" file and instead creates a log file with the intended application-name. \n\nI should note that the files DO appear in the directory that sys:catalina.home should resolve to. They appear elsewhere when I don\'t use sys:catalina.home so I\'m quite sure that this variable is resolving correctly and it is the sys:application-name which is the problem.\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-385_7c2ce5cf,"{'BugID': 'LOG4J2-385', 'Summary': 'Unable to roll log files monthly', 'Description': 'Attempting to use FastRollingFile appender and configure log file rollover to occur monthly.  When {{filePattern=""logs/app-%d\\{yyyy-MM}.log.gz""}} is used, at application startup an archive file is created immediately (app-2013-01.log.gz) even if no log previously existed.  A log file is created, but only a single entry is made into the log.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-392_731c84b5,"{'BugID': 'LOG4J2-392', 'Summary': 'Intermittent errors with appenders', 'Description': 'I intermittently receive following errors after upgrading to beta 8. EVERYTHING was working well with beta 6:\n* 1st error (happens most frequently)\n2013-09-05 10:48:37,722 ERROR Attempted to append to non-started appender LogFile\n\n* 2nd error:\n2013-09-05 10:49:38,268 ERROR Attempted to append to non-started appender LogFile\n2013-09-05 10:49:38,268 ERROR Unable to write to stream log/ui-selenium-tests.log for appender LogFile\n2013-09-05 10:49:38,269 ERROR An exception occurred processing Appender LogFile org.apache.logging.log4j.core.appender.AppenderRuntimeException: Error writing to RandomAccessFile log/ui-selenium-tests.log\n\tat org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.flush(FastRollingFileManager.java:108)\n\tat org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.write(FastRollingFileManager.java:89)\n\tat org.apache.logging.log4j.core.appender.OutputStreamManager.write(OutputStreamManager.java:129)\n\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:115)\n\tat org.apache.logging.log4j.core.appender.FastRollingFileAppender.append(FastRollingFileAppender.java:97)\n\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:102)\n\tat org.apache.logging.log4j.core.appender.AsyncAppender$AsyncThread.run(AsyncAppender.java:228)\nCaused by: java.io.IOException: Write error\n\tat java.io.RandomAccessFile.writeBytes(Native Method)\n\tat java.io.RandomAccessFile.write(Unknown Source)\n\tat org.apache.logging.log4j.core.appender.rolling.FastRollingFileManager.flush(FastRollingFileManager.java:105)\n\t... 6 more\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-395_a19ecc9e,"{'BugID': 'LOG4J2-395', 'Summary': 'log4j.configurationFile via classpath URI', 'Description': ""Can't specify log4j.configurationFile as a classpath URI.\n\nFor eg: -Dlog4j.configurationFile=classpath:log4j/log4j.xml""}"
logging-log4j2,bugs-dot-jar_LOG4J2-398_2c966ad9,"{'BugID': 'LOG4J2-398', 'Summary': 'DateLookup not parsed for FastRollingFile appender', 'Description': 'I\'m trying to create a Log4j2 configuration file that will create a log file using DateLookup so that the current date and time are in the filename (so it matches the logging used in our other products).  This is what the appender configuration looks like:\n\n{code:borderStyle=solid|language=XML}\n<FastRollingFile name=""Rolling"" fileName=""log/$${date:yyyyMMdd-HHmmss} - myApp.log"" filePattern=""log/$${date:yyyyMMdd-HHmmss} - myApp-%i.log"">\n\t<immediateFlush>true</immediateFlush>\n\t<suppressExceptions>false</suppressExceptions>\n\t<PatternLayout>\n\t\t<pattern>%d %p %c{1.} [%t] $${env:USER} %m%n</pattern>\n\t</PatternLayout>\n\t<Policies>\n\t\t<OnStartupTriggeringPolicy />\n\t\t<SizeBasedTriggeringPolicy size=""100 MB""/>\n\t</Policies>\n</FastRollingFile>\n{code}\n\nHowever when the log file is generated the filename is ""${date"".  I\'ve tried different variations and haven\'t been able to get this lookup to work at all.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-410_8f0c4871,"{'BugID': 'LOG4J2-410', 'Summary': ' java.io.NotSerializableException: org.slf4j.impl.SLF4JLogger', 'Description': 'When i use a the logger object in a model,and the model will set to some cache like memcached,this exception happens.Maybe this class and some other related class need implements Serializable interface?'}"
logging-log4j2,bugs-dot-jar_LOG4J2-430_238ce8aa,"{'BugID': 'LOG4J2-430', 'Summary': 'RFC5424Layout not working with parametrized messages', 'Description': 'Syslog (i.e RFC5424Layout) does not work with parametrized messages. If I do something like this:\n{code}\nlogger.info(""Hello {}"", ""World"");\n{code}\n\nI get this at the syslog server: \n{code}\nOct 16 18:24:33 10.0.0.3 myApp Hello {}\n{code}\n\nThis is the config file I\'m using:\n{code:xml:title=log4j2.xml}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Configuration>\n  <Appenders>\n    <Console name=""STDOUT"" target=""SYSTEM_OUT"">\n      <PatternLayout pattern=""%d{ISO8601}: %-5p [%t-%c{2}] - %m%n""/>\n    </Console>\n    <Syslog name=""syslog"" format=""RFC5424"" host=""10.0.0.1"" port=""514"" protocol=""TCP"" appName=""myApp"" facility=""LOCAL0"" newLine=""true"" includeMDC=""true"" id=""App"" reconnectionDelay=""1000""/>\n  </Appenders>\n  <Loggers>\n    <Root level=""debug"">\n      <AppenderRef ref=""STDOUT""/>\n      <AppenderRef ref=""syslog""/>\n    </Root>\n  </Loggers>\n</Configuration>\n{code}\n\nThe log to stdout is ok though.\n\nAttached you find my patch for this bug.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-447_0343e9c7,"{'BugID': 'LOG4J2-447', 'Summary': 'XMLLayout does not include marker name', 'Description': 'Log4j2 supports the notion of markers, but this is not represented by the XMLLayout.\n\nFor example, using SerializedLayout with SocketAppender will send marker information, but using XMLLayout with SocketAppender will not.\n\nIt would be very helpful to have just the name of the leaf marker sent with the log event, not the corresponding marker hierarchy.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-464_484c865f,"{'BugID': 'LOG4J2-464', 'Summary': 'JSON Syntax: LoggerConfig - multiple AppenderRef entries', 'Description': 'How does one assign multiple AppenderRef entries to a logger when using JSON syntax? I\'ve tried numerous formats but none of them appear to work. Exampe below.\n\n""loggers"": {\n        ""logger"": [\n            {\n                ""name"": ""helloWorld"",\n                ""level"": ""info"",\n                ""additivity"": ""true"",\n                ""AppenderRef"": [\n                    {\n                        ""ref"": ""File Routing Appender""\n                    },\n                    {\n                        ""ref"": ""Database Routing Appender""\n                    }\n                ]\n            }\n        ],\n        ""root"": {\n            ""level"": ""info"",\n            ""AppenderRef"": {\n                ""ref"": ""Console Appender""\n            }\n        }\n    }\n\n2013-12-11 08:32:07,012 DEBUG Calling createLogger on class org.apache.logging.log4j.core.config.LoggerConfig for element logger with params(additivity=""true"", level=""info"", name=""helloWorld"", includeLocation=""null"", AppenderRef={}, Properties={}, Configuration(Hello World Config), null)\n\nThe only way I\'ve been able to hack this (up to a maximum of two AppenderRefs) is to use the appender-ref alias in conjunction with AppenderRef e.g.:\n""loggers"": {\n            ""logger"": [\n                \n                {\n                    ""name"": ""helloWorld"",\n                    ""level"": ""info"",\n                    ""additivity"": ""true"",\n                    ""AppenderRef"": {\n                    \t""ref"":""File Routing Appender""\n                    },\n                    ""appender-ref"": {\n                    \t""ref"":""Database Routing Appender""\n                    }\n                }\n            ],\n            ""root"": {\n                ""level"": ""info"",\n                ""AppenderRef"": {\n                    ""ref"": ""Console Appender""\n                }\n            }\n   \t\t}\n2013-12-11 08:51:54,977 DEBUG Calling createLogger on class org.apache.logging.log4j.core.config.LoggerConfig for element logger with params(additivity=""true"", level=""info"", name=""helloWorld"", includeLocation=""null"", AppenderRef={File Routing Appender, Database Routing Appender}, Properties={}, Configuration(Hello World Config), null)'}"
logging-log4j2,bugs-dot-jar_LOG4J2-466_7b9e48e8,"{'BugID': 'LOG4J2-466', 'Summary': ""Cannot load log4j2 config file if path contains plus '+' characters"", 'Description': ""Hello,\n\nI was trying to programmatically load a XML config file from the temporary data directory of a MacOS X system. The temp path consists of serveral '\\+' characters like MacOS automatically generates this path so we have to take it this way. Even I would agree that it is not nice to have '\\+' chars in a path name.\n\nWhen I tried to load the XML config the framework permanently loaded the DefaultConfig and not the desired XML configuration.\nBy stepping through the debugger I figured out that this was caused by the method fileFromURI() in org.apache.logging.log4j.core.helpers.FileUtils.java .\nThe misbehaviour was basically caused by the call of URL.decode() which converts '+' to ' ' (space) of a given String. \nNow I self-compiled the whole framework without the call of URL.decode() and the XML configuration loaded properly. \nI can not see why this call is necessary in this method so in my opinion this should be removed.\n\n\nKind regards""}"
logging-log4j2,bugs-dot-jar_LOG4J2-470_50340d0c,"{'BugID': 'LOG4J2-470', 'Summary': 'Resolution of ${hostName} in log4j2.xml file only works after first reference', 'Description': 'I am using $\\{hostName} to include the hostname in the log file.  When I use it it resolves to ""$\\{hostName}"" the first time it is referred to in the log and then the proper hostname after that.\n\nExample configuration (comment out the ""Properties"" section to duplicate):\n{code}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Configuration monitorInterval=""60""\tname=""SMSLog4JConfiguration""> <!-- add this to spit out debug about configuration: \'status=""debug""\'  -->\n\n\t<!-- This seems to be a bug, but the $hostName seems to need to be referenced\n\t     once before it can be used.  Maybe it gets correct in a future log4j2 release --> \n\t<Properties>\n\t\t<Property name=""theHostName"">${hostName}</Property>\n\t</Properties>\n\n\n\t<Appenders>\n\t\t<RollingFile name=""RollingFileAppender"" fileName=""/applicationlogs/CTMSApplicationService-${hostName}.log""\n\t\t\tfilePattern=""/applicationlogs/${hostName}-%d{MM-dd-yyyy}-%i.log"">\n\t\t\t<Policies>\n\t\t\t\t<OnStartupTriggeringPolicy />\n\t\t\t\t<TimeBasedTriggeringPolicy interval=""24"" modulate=""true"" />\n\t\t\t</Policies>\n\t\t\t<PatternLayout pattern=""[%d{ISO8601}] [%t] %-5level %logger{6} - %msg%n"" />\n\t\t</RollingFile>\n\t</Appenders>\n\t<Loggers>\n\t\t<!-- default for ""includeLocation"" is false, but I want to be clear -->\n\t\t<Root level=""debug"" includeLocation=""false"">\n\t\t\t<AppenderRef ref=""RollingFileAppender"" />\n\t\t</Root>\n\t</Loggers>\n</Configuration>\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-478_11763dee,"{'BugID': 'LOG4J2-478', 'Summary': 'The message and ndc fields are not JavaScript escaped in JSONLayout', 'Description': 'The output of the JSONLayout includes the ""message"" field as is.  If there are any embedded newlines, quote, etc, this renders the JSON output as invalid.  To correct this, the ""message"" field should be properly JavaScript escaped.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-492_24a3bed4,"{'BugID': 'LOG4J2-492', 'Summary': 'MalformedObjectNameException: Invalid escape sequence... under Jetty', 'Description': ""Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.\n\nMy debug line is also included:\n\n{noformat}\nloggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e\n2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)\n        at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)\n        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)\n        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)\n        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)\n        at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)\n        at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)\n        at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)\n        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)\n        at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)\n        at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)\n        at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)\n        at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)\n        at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)\n        at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)\n        at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)\n        at org.eclipse.jetty.server.Server.doStart(Server.java:282)\n        at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)\n        at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)\n        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)\n        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)\n        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)\n        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)\nCaused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at javax.management.ObjectName.construct(ObjectName.java:582)\n        at javax.management.ObjectName.<init>(ObjectName.java:1382)\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)\n        ... 60 more\n\n2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext\n2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly.\n2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized.\n2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog\n2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080\n[INFO] Started Jetty Server\n[INFO] Starting scanner at interval of 10 seconds.\n\n{noformat}""}"
logging-log4j2,bugs-dot-jar_LOG4J2-492_61ccbb95,"{'BugID': 'LOG4J2-492', 'Summary': 'MalformedObjectNameException: Invalid escape sequence... under Jetty', 'Description': ""Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.\n\nMy debug line is also included:\n\n{noformat}\nloggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e\n2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)\n        at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)\n        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)\n        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)\n        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)\n        at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)\n        at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)\n        at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)\n        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)\n        at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)\n        at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)\n        at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)\n        at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)\n        at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)\n        at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)\n        at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)\n        at org.eclipse.jetty.server.Server.doStart(Server.java:282)\n        at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)\n        at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)\n        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)\n        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)\n        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)\n        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)\nCaused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at javax.management.ObjectName.construct(ObjectName.java:582)\n        at javax.management.ObjectName.<init>(ObjectName.java:1382)\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)\n        ... 60 more\n\n2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext\n2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly.\n2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized.\n2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog\n2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080\n[INFO] Started Jetty Server\n[INFO] Starting scanner at interval of 10 seconds.\n\n{noformat}""}"
logging-log4j2,bugs-dot-jar_LOG4J2-492_a759d8ae,"{'BugID': 'LOG4J2-492', 'Summary': 'MalformedObjectNameException: Invalid escape sequence... under Jetty', 'Description': ""Although it is not stopping my webapp from running, I am encountering the following exception when running jetty (via Maven) for a webapp using a trunk build of log4j2.\n\nMy debug line is also included:\n\n{noformat}\nloggerContext.getName()= WebAppClassLoader=1320771902@4eb9613e\n2014-01-09 13:28:52,904 ERROR Could not register mbeans java.lang.IllegalStateException: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:81)\n        at org.apache.logging.log4j.core.jmx.Server.registerContexts(Server.java:266)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:185)\n        at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:150)\n        at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:387)\n        at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:151)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:105)\n        at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:33)\n        at org.apache.logging.log4j.LogManager.getContext(LogManager.java:222)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:103)\n        at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:63)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initializeNonJndi(Log4jWebInitializerImpl.java:136)\n        at org.apache.logging.log4j.core.web.Log4jWebInitializerImpl.initialize(Log4jWebInitializerImpl.java:82)\n        at org.apache.logging.log4j.core.web.Log4jServletContainerInitializer.onStartup(Log4jServletContainerInitializer.java:41)\n        at org.eclipse.jetty.plus.annotation.ContainerInitializer.callStartup(ContainerInitializer.java:106)\n        at org.eclipse.jetty.annotations.ServletContainerInitializerListener.doStart(ServletContainerInitializerListener.java:107)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:81)\n        at org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:58)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:96)\n        at org.eclipse.jetty.server.handler.ScopedHandler.doStart(ScopedHandler.java:115)\n        at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)\n        at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)\n        at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1242)\n        at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:717)\n        at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:494)\n        at org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:298)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:172)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:229)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:95)\n        at org.eclipse.jetty.server.Server.doStart(Server.java:282)\n        at org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:65)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:520)\n        at org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:365)\n        at org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:523)\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)\n        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)\n        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)\n        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)\n        at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)\nCaused by: javax.management.MalformedObjectNameException: Invalid escape sequence '\\=' in quoted value\n        at javax.management.ObjectName.construct(ObjectName.java:582)\n        at javax.management.ObjectName.<init>(ObjectName.java:1382)\n        at org.apache.logging.log4j.core.jmx.LoggerContextAdmin.<init>(LoggerContextAdmin.java:79)\n        ... 60 more\n\n2014-01-09 13:28:52.989:INFO:/wallboard:Initializing Spring root WebApplicationContext\n2014-01-09 13:29:04.645:INFO:/wallboard:Log4jServletContextListener ensuring that Log4j starts up properly.\n2014-01-09 13:29:04.651:INFO:/wallboard:Log4jServletFilter initialized.\n2014-01-09 13:29:04.778:WARN:oejsh.RequestLogHandler:!RequestLog\n2014-01-09 13:29:04.872:INFO:oejs.AbstractConnector:Started SelectChannelConnector@0.0.0.0:8080\n[INFO] Started Jetty Server\n[INFO] Starting scanner at interval of 10 seconds.\n\n{noformat}""}"
logging-log4j2,bugs-dot-jar_LOG4J2-523_837dcd89,"{'BugID': 'LOG4J2-523', 'Summary': 'LocalizedMessage serialization is broken', 'Description': 'You can serialize a LocalizedMessage  but you get an exception when it is deserialized, an EOF exception.\n\nSee the tests in https://svn.apache.org/repos/asf/logging/log4j/log4j2/trunk/log4j-api/src/test/java/org/apache/logging/log4j/message/LocalizedMessageTest.java'}"
logging-log4j2,bugs-dot-jar_LOG4J2-56_3eb44094,"{'BugID': 'LOG4J2-56', 'Summary': 'Level.toLevel throws IllegalArgumentException instead of returning default Level', 'Description': 'org.apache.logging.log4j.Level.toLevel(String, Level) ( Level.java line 100) uses enum static method valueOf(String) which throws IllegalArgumentException instead of returning null when enum const doesnt exists. This makes the methods Level.toLevel throw the exception instead of return default value.\n\nSolution:\n\nYou can:\na) sorround it with a try-catch statement, like:\n        try {\n\t\t\treturn valueOf(sArg);\n\t\t} catch (Exception e) {\n\t\t\t//exception doesnt matter\n\t\t\treturn defaultLevel;\n\t\t}\n\nb) translate manually de String to a enum constant, like:\n        for (Level level : values()) {\n\t\t\tif (level.name().equals(sArg)) {\n\t\t\t\treturn level;\n\t\t\t}\n\t\t}\n        return defaultLevel;\n\nI prefer b) because it saves the try-catch context and the for is nearly the same that the valueOf should do.\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-581_bb02fa15,"{'BugID': 'LOG4J2-581', 'Summary': 'No header output in RollingRandomAccessFile', 'Description': 'No header output in RollingRandomAccessFile due to DummyOutputStream used when creating RollingRandomAccessFileManager. \n{code:title=RollingRandomAccessFileManager.java}\n...\n162:                return new RollingRandomAccessFileManager(raf, name, data.pattern, +new DummyOutputStream()+, data.append,\n163:                        data.immediateFlush, size, time, data.policy, data.strategy, data.advertiseURI, data.layout);\n{code}\nWhen the superclass constructor (OutputStreamManager) writes header, it outputs thus header to nowhere:\n{code:title=OutputStreamManager.java}\n35:    protected OutputStreamManager(final OutputStream os, final String streamName, final Layout<?> layout) {\n36:        super(streamName);\n37:        this.os = os;\n38:        if (layout != null) {\n39:            this.footer = layout.getFooter();\n40:            this.header = layout.getHeader();\n41:            if (this.header != null) {\n42:                try {\n43:!!!                 this.os.write(header, 0, header.length);\n44:                } catch (final IOException ioe) {\n45:                    LOGGER.error(""Unable to write header"", ioe);\n46:                }\n47:            }\n48:        } else {\n49:            this.footer = null;\n50:            this.header = null;\n51:        }\n52:    }\n{code}\nThe same fragment from RollingFileManager.java where header output works fine:\n{code:title=RollingFileManager.java}\n306:                os = new FileOutputStream(name, data.append);\n307:                if (data.bufferedIO) {\n308:                    os = new BufferedOutputStream(os);\n309:                }\n310:                final long time = file.lastModified(); // LOG4J2-531 create file first so time has valid value\n311:                return new RollingFileManager(name, data.pattern, +os+, data.append, size, time, data.policy,\n312:                    data.strategy, data.advertiseURI, data.layout);\n{code}\n\nIn this case the ""os"" variable is a real stream which points to the file.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-619_3b4b370e,"{'BugID': 'LOG4J2-619', 'Summary': 'Unable to recover after loading corrupted XML', 'Description': 'Steps to reproduce:\n1) auto-reloading of log4j 2.x configuration from XML is enabled\n2) system is started and producing logs\n3) change XML configuration, so it\'s not valid XML any longer\n4) Wait till it would be picked up -> no more logging info is produced, exception can be found from logs (see below).\n5) Fix XML configuration -> it\'s not getting reloaded anymore, only java restart can fix the problem.\n\nlog4j2.xml org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 7; The processi\nng instruction target matching ""[xX][mM][lL]"" is not allowed.\n        at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:257)\n        at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)\n        at org.apache.logging.log4j.core.config.XMLConfiguration.<init>(XMLConfiguration.java:145)\n        at org.apache.logging.log4j.core.config.XMLConfiguration.reconfigure(XMLConfiguration.java:286)\n        at org.apache.logging.log4j.core.LoggerContext.onChange(LoggerContext.java:421)\n        at org.apache.logging.log4j.core.config.FileConfigurationMonitor.checkConfiguration(FileConfigurationMonitor.java:79)\n        at org.apache.logging.log4j.core.Logger$PrivateConfig.filter(Logger.java:279)\n        at org.apache.logging.log4j.core.Logger.isEnabled(Logger.java:117)\n        at org.apache.logging.log4j.spi.AbstractLoggerWrapper.isEnabled(AbstractLoggerWrapper.java:82)\n        at org.apache.logging.log4j.spi.AbstractLogger.isDebugEnabled(AbstractLogger.java:1071)\n        at org.slf4j.impl.SLF4JLogger.isDebugEnabled(SLF4JLogger.java:174)\n        at org.apache.commons.logging.impl.SLF4JLocationAwareLog.isDebugEnabled(SLF4JLocationAwareLog.java:67)\n....\n\nERROR No logging configuration'}"
logging-log4j2,bugs-dot-jar_LOG4J2-639_a5a1f1a2,"{'BugID': 'LOG4J2-639', 'Summary': 'NPE in AsyncLogger.log(..)', 'Description': 'Our production environment suffers from\n{noformat}\njava.lang.NullPointerException at org.apache.logging.log4j.core.async.AsyncLogger.log(AsyncLogger.java:273)\n    at org.apache.logging.log4j.spi.AbstractLoggerWrapper.log(AbstractLoggerWrapper.java:121)\n    at org.apache.logging.log4j.spi.AbstractLogger.info(AbstractLogger.java:1006) \n    at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:873) \n    at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)\n{noformat}\n\nIt looks like something in our app is still logging despite the AsyncLogger having been stopped (and the disruptor field set to null).\n\nThe logger could print out a more informative message in this situation.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-668_60f64cc1,"{'BugID': 'LOG4J2-668', 'Summary': 'AsyncAppender ignores RingBufferLogEvents from AsyncLoggers (when all loggers async by setting context selector)', 'Description': ""AsyncAppender's #append method currently has this code:\n{code}\nif (!(logEvent instanceof Log4jLogEvent)) {\n    return; // only know how to Serialize Log4jLogEvents\n}\n{code}\n\nWhen all loggers are made asynchronous by setting Log4jContextSelector to {{org.apache.logging.log4j.core.async.AsyncLoggerContextSelector}}, they produce {{RingBufferLogEvent}} instances, not {{Log4jLogEvent}}. These log events will be dropped by AsyncAppender.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-676_3b2e880e,"{'BugID': 'LOG4J2-676', 'Summary': 'Failed to write log event to CouchDB due to error: Connection pool shut down', 'Description': 'I\'m trying to setup a NoSQL logger using Apache CouchDB. After logging a single message, the logger fails with the following exception:\n\n{color: blue}\n  2014-06-22 10:22:18,590 ERROR An exception occurred processing Appender databaseAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Failed to write log event to CouchDB due to error: Connection pool shut down\n\tat org.apache.logging.log4j.core.appender.db.nosql.couchdb.CouchDBConnection.insertObject(CouchDBConnection.java:57)\n\tat org.apache.logging.log4j.core.appender.db.nosql.NoSQLDatabaseManager.writeInternal(NoSQLDatabaseManager.java:148)\n\tat org.apache.logging.log4j.core.appender.db.AbstractDatabaseManager.write(AbstractDatabaseManager.java:159)\n\tat org.apache.logging.log4j.core.appender.db.AbstractDatabaseAppender.append(AbstractDatabaseAppender.java:103)\n\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:97)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:425)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:406)\n\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:367)\n\tat org.apache.logging.log4j.core.Logger.log(Logger.java:112)\n\tat org.apache.logging.log4j.spi.AbstractLogger.error(AbstractLogger.java:577)\n\tat be.pw999.kbomap.controller.KboMapController.getJson(KboMapController.java:65)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:125)\n\tat org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:195)\n\tat org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:91)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:346)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:341)\n\tat org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:101)\n\tat org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:224)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n\tat org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n\tat org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n\tat org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n\tat org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:198)\n\tat org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:946)\n\tat org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:323)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:372)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:335)\n\tat org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:218)\n\tat org.apache.catalina.core.StandardWrapper.service(StandardWrapper.java:1682)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:344)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n\tat org.apache.logging.log4j.core.web.Log4jServletFilter.doFilter(Log4jServletFilter.java:66)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:256)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:214)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:316)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:160)\n\tat org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:734)\n\tat org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:673)\n\tat com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:99)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:174)\n\tat org.apache.catalina.connector.CoyoteAdapter.doService(CoyoteAdapter.java:357)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:260)\n\tat com.sun.enterprise.v3.services.impl.ContainerMapper.service(ContainerMapper.java:188)\n\tat org.glassfish.grizzly.http.server.HttpHandler.runService(HttpHandler.java:191)\n\tat org.glassfish.grizzly.http.server.HttpHandler.doHandle(HttpHandler.java:168)\n\tat org.glassfish.grizzly.http.server.HttpServerFilter.handleRead(HttpServerFilter.java:189)\n\tat org.glassfish.grizzly.filterchain.ExecutorResolver$9.execute(ExecutorResolver.java:119)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeFilter(DefaultFilterChain.java:288)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.executeChainPart(DefaultFilterChain.java:206)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.execute(DefaultFilterChain.java:136)\n\tat org.glassfish.grizzly.filterchain.DefaultFilterChain.process(DefaultFilterChain.java:114)\n\tat org.glassfish.grizzly.ProcessorExecutor.execute(ProcessorExecutor.java:77)\n\tat org.glassfish.grizzly.nio.transport.TCPNIOTransport.fireIOEvent(TCPNIOTransport.java:838)\n\tat org.glassfish.grizzly.strategies.AbstractIOStrategy.fireIOEvent(AbstractIOStrategy.java:113)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.run0(WorkerThreadIOStrategy.java:115)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy.access$100(WorkerThreadIOStrategy.java:55)\n\tat org.glassfish.grizzly.strategies.WorkerThreadIOStrategy$WorkerThreadRunnable.run(WorkerThreadIOStrategy.java:135)\n\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:564)\n\tat org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:544)\n\tat java.lang.Thread.run(Thread.java:722)\nCaused by: java.lang.IllegalStateException: Connection pool shut down\n\tat org.apache.http.util.Asserts.check(Asserts.java:34)\n\tat org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:169)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.requestConnection(PoolingHttpClientConnectionManager.java:217)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:158)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:195)\n\tat org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:86)\n\tat org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:108)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:186)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:72)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)\n\tat org.lightcouch.CouchDbClientBase.executeRequest(CouchDbClientBase.java:409)\n\tat org.lightcouch.CouchDbClientBase.put(CouchDbClientBase.java:517)\n\tat org.lightcouch.CouchDbClientBase.save(CouchDbClientBase.java:273)\n\tat org.apache.logging.log4j.core.appender.db.nosql.couchdb.CouchDBConnection.insertObject(CouchDBConnection.java:51)\n\t... 66 more]]\n{color}\n\n\nThe log4j2.xml file is:\n{code:xml}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Configuration status=""info"">\n  <Appenders>\n    <NoSql name=""databaseAppender"">\n      <CouchDb databaseName=""kbomaplog"" protocol=""http"" server=""127.0.0.1"" port=""5984""\n               username=""loguser"" password=""meh"" />\n    </NoSql>\n  </Appenders>\n  <Loggers>\n    <Root level=""info"">\n      <AppenderRef ref=""databaseAppender""/>\n    </Root>\n  </Loggers>\n</Configuration>\n{code}\n\n\nAnd the piece of code I\'m using to test is:\n{code}\n\tprivate Logger logger = LogManager.getLogger(KboMapController.class);\n\t\n\t/**\n\t * Does nothing special. Returns a simple JSON object with a count of the code table for testing purposes.\n\t * \n\t * @param id unused\n\t * @param test unused\n\t * @return a JSON representation of the filled in {@link Enterprise} object.\n\t */\n\t@GET\n\t@Produces(MediaType.APPLICATION_JSON)\n\t@Path(""{id}/{test}"")\n\tpublic Enterprise getJson(@PathParam(""id"") String id, @PathParam(""test"") String test) {\n\t\ttry {\n\t\t\tlogger.error(""whoaaaaaah"");\n\t\t\treturn new Enterprise(""SUCCESS"", ""COUNT="" + dao.count());\n\t\t} catch (SQLException e) {\n\t\t\treturn new Enterprise(""ERROR"", e.getMessage());\n\t\t}\n\n\t}\n{code}\n\nThe same issue occurs when the logger is {{static final}}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-71_2afe3dff,"{'BugID': 'LOG4J2-71', 'Summary': 'RollingFileAppender does not create parent directories for the archive files and fails to roll.', 'Description': 'FileRenameAction is not creating the parent directories for the archive files. This cause the file rename and file copy to fail.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-742_4b77622b,"{'BugID': 'LOG4J2-742', 'Summary': 'XInclude not working with relative path', 'Description': 'When using XInclude in a log4j2 configuration, it uses the CWD of the running application instead of the location of the log4j configuration as base.\nI.e. running the application from within eclipse, the CWD of eclipse is used as base for finding the document to be included.\n\nIMO, the problem is in XmlConfiguration:\n{code}\n            final InputStream configStream = configSource.getInputStream();\n            try {\n                buffer = toByteArray(configStream);\n            } finally {\n                configStream.close();\n            }\n            final InputSource source = new InputSource(new ByteArrayInputStream(buffer));\n            final Document document = newDocumentBuilder().parse(source);\n{code}\nThere is no way the DOMParser can know, where the base should be, because it is just parsing an InputStream and has no file location. \n\nThe fix would be to add source.setSystemId(configSource.getLocation()) before parsing the document.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-763_97203de8,"{'BugID': 'LOG4J2-763', 'Summary': 'Async loggers convert message parameters toString at log record writing not at log statement execution', 'Description': 'http://javaadventure.blogspot.com/2014/07/log4j-20-async-loggers-and-immutability.html\n\nWhen using parameterized messages, the toString() method of the log messages is not called when the log message is enqueued, rather after the log message has been dequeued for writing. If any of the message parameters are mutable, they can thus have changed state before the log message is written, thus resulting in the logged message content being incorrect.\n\nFrom the blog post, code that demonstrates the problem:\n{code}\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport java.util.concurrent.atomic.AtomicLong;\n\npublic class App {\n    private static final AtomicLong value = new AtomicLong();\n    public String toString() {\n        return Long.toString(value.get());\n    }\n    public long next() {\n        return value.incrementAndGet();\n    }\n\n    public static void main(String[] args) {\n        for (int i = 0; i < 32; i++) {\n            new Thread() {\n                final Logger logger = LogManager.getLogger(App.class);\n                 final App instance = new App();\n                @Override\n                 public void run() {\n                     for (int i = 0; i < 100000; i++) {\n                         logger.warn(""{} == {}"", instance.next(), instance);\n                     }\n                 }\n            }.start();\n        }\n    }\n}\n{code}\n\nHere is the first few lines of logging output\n{code}\n2014-07-28 15:59:45,729 WARN t.App [Thread-13] 13 == 13 \n2014-07-28 15:59:45,730 WARN t.App [Thread-29] 29 == 29 \n2014-07-28 15:59:45,729 WARN t.App [Thread-15] 15 == 15 \n2014-07-28 15:59:45,729 WARN t.App [Thread-6] 6 == 6 \n2014-07-28 15:59:45,730 WARN t.App [Thread-30] 30 == 30 \n2014-07-28 15:59:45,729 WARN t.App [Thread-20] 20 == 20 \n2014-07-28 15:59:45,729 WARN t.App [Thread-8] 8 == 8 \n2014-07-28 15:59:45,730 WARN t.App [Thread-28] 28 == 28 \n2014-07-28 15:59:45,729 WARN t.App [Thread-19] 19 == 19 \n2014-07-28 15:59:45,729 WARN t.App [Thread-18] 18 == 18 \n2014-07-28 15:59:45,729 WARN t.App [Thread-5] 5 == 6 \n2014-07-28 15:59:45,731 WARN t.App [Thread-13] 33 == 37 \n2014-07-28 15:59:45,731 WARN t.App [Thread-8] 39 == 39 \n2014-07-28 15:59:45,731 WARN t.App [Thread-28] 40 == 41 \n2014-07-28 15:59:45,731 WARN t.App [Thread-18] 42 == 43 \n2014-07-28 15:59:45,731 WARN t.App [Thread-5] 43 == 43\n{code}\n\nTo make my previous code work with Asynchronous loggers (other than by fixing the mutable state) I would need to log like this:\n\n{code}\nif (logger.isWarnEnabled()) {\n    logger.warn(""{} == {}"", instance.next(), instance.toString());\n}\n{code}\n\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-763_b2ec5106,"{'BugID': 'LOG4J2-763', 'Summary': 'Async loggers convert message parameters toString at log record writing not at log statement execution', 'Description': 'http://javaadventure.blogspot.com/2014/07/log4j-20-async-loggers-and-immutability.html\n\nWhen using parameterized messages, the toString() method of the log messages is not called when the log message is enqueued, rather after the log message has been dequeued for writing. If any of the message parameters are mutable, they can thus have changed state before the log message is written, thus resulting in the logged message content being incorrect.\n\nFrom the blog post, code that demonstrates the problem:\n{code}\nimport org.apache.logging.log4j.LogManager;\nimport org.apache.logging.log4j.Logger;\nimport java.util.concurrent.atomic.AtomicLong;\n\npublic class App {\n    private static final AtomicLong value = new AtomicLong();\n    public String toString() {\n        return Long.toString(value.get());\n    }\n    public long next() {\n        return value.incrementAndGet();\n    }\n\n    public static void main(String[] args) {\n        for (int i = 0; i < 32; i++) {\n            new Thread() {\n                final Logger logger = LogManager.getLogger(App.class);\n                 final App instance = new App();\n                @Override\n                 public void run() {\n                     for (int i = 0; i < 100000; i++) {\n                         logger.warn(""{} == {}"", instance.next(), instance);\n                     }\n                 }\n            }.start();\n        }\n    }\n}\n{code}\n\nHere is the first few lines of logging output\n{code}\n2014-07-28 15:59:45,729 WARN t.App [Thread-13] 13 == 13 \n2014-07-28 15:59:45,730 WARN t.App [Thread-29] 29 == 29 \n2014-07-28 15:59:45,729 WARN t.App [Thread-15] 15 == 15 \n2014-07-28 15:59:45,729 WARN t.App [Thread-6] 6 == 6 \n2014-07-28 15:59:45,730 WARN t.App [Thread-30] 30 == 30 \n2014-07-28 15:59:45,729 WARN t.App [Thread-20] 20 == 20 \n2014-07-28 15:59:45,729 WARN t.App [Thread-8] 8 == 8 \n2014-07-28 15:59:45,730 WARN t.App [Thread-28] 28 == 28 \n2014-07-28 15:59:45,729 WARN t.App [Thread-19] 19 == 19 \n2014-07-28 15:59:45,729 WARN t.App [Thread-18] 18 == 18 \n2014-07-28 15:59:45,729 WARN t.App [Thread-5] 5 == 6 \n2014-07-28 15:59:45,731 WARN t.App [Thread-13] 33 == 37 \n2014-07-28 15:59:45,731 WARN t.App [Thread-8] 39 == 39 \n2014-07-28 15:59:45,731 WARN t.App [Thread-28] 40 == 41 \n2014-07-28 15:59:45,731 WARN t.App [Thread-18] 42 == 43 \n2014-07-28 15:59:45,731 WARN t.App [Thread-5] 43 == 43\n{code}\n\nTo make my previous code work with Asynchronous loggers (other than by fixing the mutable state) I would need to log like this:\n\n{code}\nif (logger.isWarnEnabled()) {\n    logger.warn(""{} == {}"", instance.next(), instance.toString());\n}\n{code}\n\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-793_73400bfb,"{'BugID': 'LOG4J2-793', 'Summary': ""Log4jLogger only accepts Log4jMarker, not SLF4J's Marker"", 'Description': 'We\'re using Log4j 2 via SLF4J. A Logger\'s log methods have signatures like this:\npublic abstract void warn(org.slf4j.Marker marker, java.lang.String msg)\n\nIf you use an object that is an Marker but not a Log4jMarker this fails at org.apache.logging.slf4j.Log4jLogger.getMarker(Log4jLogger.java:378) due to ""cannot be cast to org.apache.logging.slf4j.Log4jMarker"".\n\nUse case: we have a defined set of Markers. There\'s an enum for this, implementing SLF4J\'s marker interface. Obviously with Log4j we cannot use this enum.\n\nI think an org.apache.logging.slf4j.Log4jLogger cannot expect an org.apache.logging.slf4j.Log4jMarker.\n\n\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-811_7bb1ad47,"{'BugID': 'LOG4J2-811', 'Summary': 'SimpleLogger throws ArrayIndexOutOfBoundsException for an empty array', 'Description': 'There seems to be an issue with SimpleLogger implementation provided by log4j2. The issue seems to be in the new improved API supporting placeholders and var args when called with an Object Array of size 0.\n\nfor e.g logger.error(""Hello World {} in {} "" , new Object[0]);\n\nA statement above results in an error as shown below\n\nERROR StatusLogger Unable to locate a logging implementation, using SimpleLogger\nException in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1\n       at org.apache.logging.log4j.simple.SimpleLogger.logMessage(SimpleLogger.java:157)\n       at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:1347)\n       at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1312)\n       at org.apache.logging.log4j.spi.AbstractLogger.error(AbstractLogger.java:539)\n       at TestError.main(TestError.java:21)\n\n\nSolution to place a check in SimpleLogger for checking the size of the array . '}"
logging-log4j2,bugs-dot-jar_LOG4J2-813_0bea17d7,"{'BugID': 'LOG4J2-813', 'Summary': 'MarkerManager Log4jMarker.hasParents() returns opposite of correct result', 'Description': 'Log4JMarker.hasParents() will return false when the marker has parents, and true when it has none. \n\nThe javadoc in the Marker interface indicates it should function the other way around: \n{quote}\n""Indicates whether this Marker has references to any other Markers.  Return true if the Marker has parent Markers""\n{quote}\n\nThe code for the implementation (that I could find) demonstrates that it would function in the opposite way as it is described in that javadoc: \n{code}\n@Override \npublic boolean hasParents() { \n     return this.parents == null; \n}\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-832_411dad65,"{'BugID': 'LOG4J2-832', 'Summary': 'ThrowableProxy fails if a class in logged stack trace throws java.lang.Error from initializer', 'Description': ""When the Logger attempts to log a message with an exception stack trace, it uses the ThrowableProxy class to introspect classes in the stack trace frames.\n\nIf the class sun.reflect.misc.Trampoline is in the stack trace, the introspection performed by ThrowableProxy will fail causing a java.lang.Error to be thrown by the Logger call.\n\nThe sun.reflect.misc.Trampoline class is used by the sun.reflect.misc.MethodUtil class to perform reflection-based method invocations. MethodUtil is widely used by libraries to perform method invocations. I've encountered this problem when invoking methods over JMX and inside Jetty.\n\nI am classifying this as a blocker because it means that any logging statement that is logging a Throwable message containing a MethodUtil-based reflection stack trace can cause a java.lang.Error to be thrown by Log4j2. \n\nI will attach a unit test for this failure.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-834_d3989b40,"{'BugID': 'LOG4J2-834', 'Summary': 'ThrowableProxy throws NoClassDefFoundError', 'Description': 'In method *loadClass* we expect {{ClassNotFoundException}}. But if class comes from another java machine we can get {{NoClassDefFoundError}}.\n\nPossible fix:\n{code:java}\nprivate Class<?> loadClass(final ClassLoader lastLoader, final String className) {\n       // XXX: this is overly complicated\n       Class<?> clazz;\n       if (lastLoader != null) {\n           try {\n               clazz = Loader.initializeClass(className, lastLoader);\n               if (clazz != null) {\n                   return clazz;\n               }\n           } catch (final Throwable ignore) {\n               // Ignore exception.\n           }\n       }\n       try {\n           clazz = Loader.loadClass(className);\n       } catch (final ClassNotFoundException | LinkageError ignored) {\n           try {\n               clazz = Loader.initializeClass(className, this.getClass().getClassLoader());\n           } catch (final ClassNotFoundException | LinkageError ignore) {\n               return null;\n           }\n       }\n       return clazz;\n   }\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-892_f9b0bbee,"{'BugID': 'LOG4J2-892', 'Summary': ""JUL adapter does not map Log4j's FATAL level to a JUL level"", 'Description': ""JUL module does not map Log4j's FATAL level.\nSee {{org.apache.logging.log4j.jul.DefaultLevelConverter}}.""}"
logging-log4j2,bugs-dot-jar_LOG4J2-914_f8a42197,"{'BugID': 'LOG4J2-914', 'Summary': 'ThrowableProxy.getExtendedStackTraceAsString causes NullPointerException ', 'Description': 'I\'m trying to write a poc with Log4j 2.1, where distributed processes are logging to a remote server. The server is currently running the bundled TcpSocketServer.createSerializedSocketServer with a custom layout plugin. \n\nA process is logging an exception. I can then see in the custom layout plugin at the log server that the LogEvent doesn\'t contain a thrown, but that it contains a thrownProxy. So far so good. I\'m then trying to get hold of a String representation of the message + stacktrace. I thought that I would be able to e.g invoke ThrowableProxy.getExtendedStackTraceAsString(), but that causes a NullPointerException since the throwable in the ThrowableProxy also is null after deserialization. Looks like ThrowableProxy assumes that throwable isn\'t null in a few methods. \n\nThe exception that is logged by the client process is a simple new Exception(""A message"");\n\nThe pom.xml that I\'m using:\n{code:xml}\n<dependency>\n\t<groupId>org.apache.logging.log4j</groupId>\n\t<artifactId>log4j-api</artifactId>\n\t<version>2.1</version>\n</dependency>\n<dependency>\n\t<groupId>org.apache.logging.log4j</groupId>\n\t<artifactId>log4j-core</artifactId>\n\t<version>2.1</version>\n</dependency>\n<dependency>\n\t<groupId>com.lmax</groupId>\n\t<artifactId>disruptor</artifactId>\n\t<version>3.3.0</version>\n</dependency>\n{code}\nThe stacktrace that I get in the server:\n{code}\n2014-12-05 14:30:44,601 ERROR An exception occurred processing Appender XXXXX java.lang.NullPointerException\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.getExtendedStackTraceAsString(ThrowableProxy.java:340)\n\tat org.apache.logging.log4j.core.impl.ThrowableProxy.getExtendedStackTraceAsString(ThrowableProxy.java:323)\n{code}\nWorkaround:\nTo invoke ThrowableProxy. getExtendedStackTrace() and format the stacktrace + message with my own format methods.\n'}"
logging-log4j2,bugs-dot-jar_LOG4J2-94_d8af1c93,"{'BugID': 'LOG4J2-94', 'Summary': 'Variable substitution: ${sys:foo} defaults to <property name="":foo"">, should default to <property name=""foo"">', 'Description': 'The following configuration doesn\'t work (${sys:log.level} can\'t be resolved even though default value is provided).\n\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<configuration status=""OFF"">\n  <properties>\n    <property name=""log.level"">error</property>\n    <property name="":log.level"">ACTUALLY_GETS_USED</property>\n  </properties>\n  <appenders>\n    <Console name=""Console"" target=""SYSTEM_OUT"">\n      <PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n""/>\n    </Console>\n  </appenders>\n  <loggers>\n    <root level=""${sys:log.level}"">\n      <appender-ref ref=""Console""/>\n    </root>\n  </loggers>\n</configuration>\n\n\nIn org.apache.logging.log4j.core.lookup.Interpolator.lookup(LogEvent, String), on line 110,\nvar = var.substring(prefixPos) should be var = var.substring(prefixPos + 1) instead.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-964_16ad8763,"{'BugID': 'LOG4J2-964', 'Summary': 'StringFormattedMessage serialization is incorrect', 'Description': 'The method {{writeObject(final ObjectOutputStream out)}} of the class {{org.apache.logging.log4j.message.StringFormattedMessage}} does not write the stringArgs array into the output stream. This causes {{readObject(final ObjectInputStream in)}} to throw an {{EOFException}} when trying to deserialize.\n\nThere is another bug in the same method. The line {{stringArgs[i] = obj.toString();}} throws a {{NullPointerException}} when obj is null.'}"
logging-log4j2,bugs-dot-jar_LOG4J2-965_43517f15,"{'BugID': 'LOG4J2-965', 'Summary': 'System.out no longer works after the Console appender and JANSI are initialized', 'Description': 'h3. Demonstration\n\nThe underlining project demonstrate the bug.\n\nThe project\'s build.gradle file:\n\n{code:title=build.gradle}\napply plugin: \'java\'\n\nversion = \'1.0\'\n\nrepositories {\n    mavenCentral()\n}\n\ndef log4j2Version = \'2.2\'\ndef log4j2GroupId = ""org.apache.logging.log4j""\n\ndependencies {\n    compile log4j2GroupId + \':log4j-core:\' + log4j2Version\n    compile log4j2GroupId + "":log4j-jcl:"" + log4j2Version\n    compile log4j2GroupId + "":log4j-slf4j-impl:"" + log4j2Version\n    compile \'org.fusesource.jansi:jansi:1.11\'\n}\n{code}\n\nA log4j2.xml in classpath:\n\n{code:title=log4j2.xml}\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<Configuration status=""WARN"">\n    <Appenders>\n        <File name=""root"" fileName=""${sys:user.home}/logs/windowsbug.log"">\n            <PatternLayout>\n                <Pattern>%d %p %c{1.} [%t] %m%n</Pattern>\n            </PatternLayout>\n        </File>\n    </Appenders>\n    <Loggers>\n        <Root level=""info"">\n            <AppenderRef ref=""root""/>\n        </Root>\n    </Loggers>\n</Configuration>\n{code}\n\nAnd the main class:\n\n{code:title=Log4j2WindowsBug.java}\nimport org.slf4j.LoggerFactory;\n\n/**\n * @author khotyn 15/3/2 下午8:17\n */\npublic class Log4j2WindowsBug {\n\n    public static void main(String[] args) {\n        System.out.println(""Able to print on Windows"");\n        LoggerFactory.getLogger(Log4j2WindowsBug.class);\n        System.out.println(""Unable to print on Windows"");\n    }\n}\n{code}\n\nThe output of the demo under Windows is:\n\n{code}\nAble to print on Windows\n{code}\n\nThe third line did not print to Windows console.\n\nh3. Reason\n\nIt seems that log4j2 will wrapper System.out to WindowsAnsiOutputStream if jansi is available in classpath. And in OutputStreamManager\'s close method, the wrapper WindowsAnsiOutputStream is not considered, and cause the underling System.out closed.\n\n{code:title=OutputStreamManager.java}\n    protected synchronized void close() {\n        final OutputStream stream = os; // access volatile field only once per method\n        if (stream == System.out || stream == System.err) {\n            return;\n        }\n        try {\n            stream.close();\n        } catch (final IOException ex) {\n            LOGGER.error(""Unable to close stream "" + getName() + "". "" + ex);\n        }\n    }\n{code}'}"
logging-log4j2,bugs-dot-jar_LOG4J2-991_3cee912e,"{'BugID': 'LOG4J2-991', 'Summary': 'Async root logger config is defaulting includeLocation to true without use of Log4jContextSelector system property', 'Description': 'I\'m using the approach detailed here - https://logging.apache.org/log4j/2.x/manual/async.html - under ""Mixing Synchronous and Asynchronous Loggers"" where we have the <asyncRoot> logger defined. I noticed this was slow so looked into it and noticed the location was being captured but I thought this should default to false for async loggers. Looking into this, the line here - https://github.com/apache/logging-log4j2/blob/master/log4j-core/src/main/java/org/apache/logging/log4j/core/async/AsyncLoggerConfig.java#L239 - the call to includeLocation() is actually calling LoggerConfig.includeLocation() which checks for the existence of the system property (which we don\'t have set), therefore include location defaults to true. I think instead it should be calling the includeLocation() static method inside of AsyncLoggerConfig here - https://github.com/apache/logging-log4j2/blob/master/log4j-core/src/main/java/org/apache/logging/log4j/core/async/AsyncLoggerConfig.java#L204 - which would end up defaulting this to false correctly as the includeLocation value is actually null since I didn\'t explicitly configured it.'}"
maven,bugs-dot-jar_MNG-1205_1bdeeccc,"{'BugID': 'MNG-1205', 'Summary': ""dependency with scope:system & flag optional = true doesn't appear in the class path"", 'Description': ""Dependency with scope:system & flag  optional = true doesn't appear in the class path\nTry to call m2 install  or compiler:compile in the attached project. Compilation will fail ...\n\nbest regards\nJuBu""}"
maven,bugs-dot-jar_MNG-1509_4e955c05,"{'BugID': 'MNG-1509', 'Summary': ""Profile activation by os doesn't work"", 'Description': ""Profile activation by os doesn't work.\n\nOperatingSystemProfileActivator is missing in components.xml.\n\nImplementation of OperatingSystemProfileActivator.isActive is wrong.\n\n\n""}"
maven,bugs-dot-jar_MNG-1703_b68c84b8,"{'BugID': 'MNG-1703', 'Summary': '<pluginManagement><dependencies> is not propagated to child POMs', 'Description': ""<executions> section in <pluginManagement> isn't propagated to child POMs (as <configuration> is).\nThe workaround is to use <plugins> with <inherited>true</inherited>\nIs this on purpose ?""}"
maven,bugs-dot-jar_MNG-1797_5d99b35c,"{'BugID': 'MNG-1797', 'Summary': 'Dependency excludes apply to every subsequent dependency, not just the one it is declared under.', 'Description': ""If you specify ANY dependency excludes, all dependencies after that one in the pom will also exclude what you specified.  They appear to be cumulative on every dependency in that they bleed over into sibling dependencies.  \nIt's easy to test if you add an exclusion to a random dependency. This exclusion should exclude a required transitive dependency that is included by a dependency lower in the list.  You will find that the dependency lower in the list no longer includes the required dependency because it is using the filter you declared in the other dependency.\n""}"
maven,bugs-dot-jar_MNG-1856_faa5cf27,"{'BugID': 'MNG-1856', 'Summary': 'legacy layout tag in a profile does not show up in child pom.', 'Description': 'the legacy layout tag in a profile does not show up in an inherited pom.\n\nGiven the following pom.xml:\n\n<project>\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>xxx</groupId>\n  <artifactId>yyy</artifactId>\n  <version>1.0-SNAPSHOT</version>\n  <packaging>pom</packaging>\n  <profiles>\n    <profile>\n      <id>maven-1</id>\n      <activation>\n        <property>\n          <name>maven1</name>\n        </property>\n      </activation>\n      <distributionManagement>\n        <repository>\n          <id>maven-1-repo</id>\n          <name>Maven1 Repository</name>\n          <url>sftp://...</url>\n          <layout>legacy</layout>\n        </repository>\n      </distributionManagement>    \n    </profile>\n  </profiles>\n</project>\n\ngives for:\n\nmvn projecthelp:effective-pom -Dmaven1\n\nthe following result:\n\n...\n  <distributionManagement>\n    <repository>\n      <id>maven-1-repo</id>\n      <name>Maven1 Repository</name>\n      <url>sftp://...</url>\n      <layout>legacy</layout>\n    </repository>\n  </distributionManagement>\n</project>\n\nwhich is CORRECT, however if I inherit from this pom with the following pom.xml:\n\n<project>\n  <parent>\n    <groupId>xxx</groupId>\n    <artifactId>yyy</artifactId>\n    <version>1.0-SNAPSHOT</version>\n  </parent>\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>uuu</groupId>\n  <artifactId>vvv</artifactId>\n  <version>2.0-SNAPSHOT</version>\n</project>\n\ngives for:\n\nmvn projecthelp:effective-pom -Dmaven1\n\nthe following result:\n\n...\n  <distributionManagement>\n    <repository>\n      <id>maven-1-repo</id>\n      <name>Maven1 Repository</name>\n      <url>sftp://...</url>\n    </repository>\n  </distributionManagement>\n</project>\n\nwhich is INCORRECT, the layout tag is missing.\n\nThis issue may be related to:\n\nhttp://jira.codehaus.org/browse/MNG-731\nhttp://jira.codehaus.org/browse/MNG-1756\n'}"
maven,bugs-dot-jar_MNG-1895_24db0eb9,"{'BugID': 'MNG-1895', 'Summary': 'Dependencies in two paths are not added to resolution when scope needs to be updated in the nearest  due to any of nearest parents', 'Description': 'scopes are not correctly calculated for this case\n\nmy pom: a compile, b test\na: c compile, d compile\nb: d compile\n\nthen d ends in test scope because d is closer to my project through the path b-d\n\nI think scope importance should also be taken into account\n'}"
maven,bugs-dot-jar_MNG-1895_806eaeb0,"{'BugID': 'MNG-1895', 'Summary': 'Dependencies in two paths are not added to resolution when scope needs to be updated in the nearest  due to any of nearest parents', 'Description': 'scopes are not correctly calculated for this case\n\nmy pom: a compile, b test\na: c compile, d compile\nb: d compile\n\nthen d ends in test scope because d is closer to my project through the path b-d\n\nI think scope importance should also be taken into account\n'}"
maven,bugs-dot-jar_MNG-1999_ad38e46b,"{'BugID': 'MNG-1999', 'Summary': 'Reporting inheritance does not work properly', 'Description': ""I have a parent project and some subprojects. The parent project's pom has:\n\n<reporting>\n   <excludeDefaults>true</excludeDefaults>\n</reporting>\n\nHowever, it does not get inherited to subprojects and so all default reports are generated for all subprojects.\n\nI'm not sure if this is a bug or a lack of feature but I would be good to have it.\n\nFYI, I'm speaking about here:\nhttp://svn.apache.org/viewcvs.cgi/directory/trunks/apacheds/""}"
maven,bugs-dot-jar_MNG-2174_778f044e,"{'BugID': 'MNG-2174', 'Summary': '<pluginManagement><plugins><plugin><dependencies> do not propogate to child POM plugins (potentially scoped to only affecting child POM plugins that live within a <profile>)', 'Description': '<pluginManagement><plugins><plugin><dependencies> do not propogate to child POM plugins.\n\nKenny believe this works OKAY if the childs are using the parent <pluginManagement> preconfigured plugins in their main <build> section however it does NOT work if the childs are trying to use those preconfigured plugins via their own <profiles>. Configuration propogates through okay but dependencies are missing and have to be respecified in the child POMs.\n\n\n'}"
maven,bugs-dot-jar_MNG-2221_cc859f5c,"{'BugID': 'MNG-2221', 'Summary': 'Multiple Executions of Plugin at Difference Inhertiance levels causes plugin executions to run multiple times', 'Description': ""Can occur in a variety of ways, but the attached test case shows a parent pom defining an antrun-execution, and then a child specifying another execution with a different id.  Both executions run twice when running from the child.\n\nI believe this is the same as Kenney Westerhof's comment: http://jira.codehaus.org/browse/MNG-2054#action_62477""}"
maven,bugs-dot-jar_MNG-2281_f0fcef7e,"{'BugID': 'MNG-2281', 'Summary': '1.0-beta-3 should be < 1.0-SNAPSHOT', 'Description': None}"
maven,bugs-dot-jar_MNG-2408_b92af0e4,"{'BugID': 'MNG-2408', 'Summary': 'Improve handling of ""no plugin version found"" error after intermittent errors', 'Description': ""If you follow the instructions at http://maven.apache.org/guides/getting-started/index.html#How%20do%20I%20make%20my%20first%20Maven%20project? to use the archetype plugin to create a new project skeleton, the suggested command line fails as below.\n\nIt seems there is a typo of some kind in the suggested command line, I am not familiar enough with maven 2 to know for sure.\n\nGraham-Leggetts-Computer:~/src/standard/alchemy/maven minfrin$ mvn archetype:create -DgroupId=za.co.standardbank.alchemy -DartifactId=alchemy-trader   \n[INFO] Scanning for projects...\n[INFO] Searching repository for plugin with prefix: 'archetype'.\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD ERROR\n[INFO] ------------------------------------------------------------------------\n[INFO] The plugin 'org.apache.maven.plugins:maven-archetype-plugin' does not exist or no valid version could be found\n[INFO] ------------------------------------------------------------------------\n[INFO] For more information, run Maven with the -e switch\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 1 second\n[INFO] Finished at: Tue Jun 27 09:43:14 SAST 2006\n[INFO] Final Memory: 1M/2M\n[INFO] ------------------------------------------------------------------------\n""}"
maven,bugs-dot-jar_MNG-2712_06090da4,"{'BugID': 'MNG-2712', 'Summary': ""update policy 'daily' not honored"", 'Description': ""under certain circumstances, the '<updatePolicy>daily</updatePolicy>' isn't honored.\n\nThis is the case where the remote metadata file doesn't exist, or contains <version>RELEASE/LATEST (which should never happen)..\n\nThe timestamp used to compare for the update is 0L, because the local file doesn't exist.\nThen the remote file is retrieved, which also doesn't exist, and no metadatafile is created.\nThe next time an up2date check is done, again against timestamp 0 for a non-existent file.\n\nThis means that if you define a custom snapshot repo in settings.xml or a pom, and you have 500 transitive\ndeps, the repo's that don't have that artifact are consulted 500 times for each mvn invocation.\n\nA build that normally takes about 20 seconds takes more than 10 minutes because of this bug.""}"
maven,bugs-dot-jar_MNG-2931_d7422212,"{'BugID': 'MNG-2931', 'Summary': ""DefaultArtifactCollector changes the version of the originatingArtifact if it's in the dependencyManagement with another version"", 'Description': 'DefaultDependencyTreeBuilder\nhttps://svn.apache.org/repos/asf/maven/shared/trunk/maven-dependency-tree/src/main/java/org/apache/maven/shared/dependency/tree/DefaultDependencyTreeBuilder.java\n\ncalls collect like this\n\n            collector.collect( project.getDependencyArtifacts(), project.getArtifact(), managedVersions, repository,\n                               project.getRemoteArtifactRepositories(), metadataSource, null,\n                               Collections.singletonList( listener ) );\n\nProblem: \nThis pom \nhttp://repo1.maven.org/maven2/org/codehaus/plexus/plexus-component-api/1.0-alpha-22/plexus-component-api-1.0-alpha-22.pom\nextends\nhttp://repo1.maven.org/maven2/org/codehaus/plexus/plexus-containers/1.0-alpha-22/plexus-containers-1.0-alpha-22.pom\nthat in dependencyManagement has org.codehaus.plexus:plexus-component-api:1.0-alpha-19\n\nso during collect project.getArtifact().getVersion() is changed to the managedVersion instead of the original one\n\nEither this is a bug or an exception should be thrown when originatingArtifact is in managedVersions\n\n'}"
maven,bugs-dot-jar_MNG-3092_5ffd8903,"{'BugID': 'MNG-3092', 'Summary': 'Resolution of version ranges with non-snapshot bounds can resolve to a snapshot version', 'Description': 'Contrary to the 2.0 design docs:\n\n""Resolution of dependency ranges should not resolve to a snapshot (development version) unless it is included as an explicit boundary.""\n-- from http://docs.codehaus.org/display/MAVEN/Dependency+Mediation+and+Conflict+Resolution#DependencyMediationandConflictResolution-Incorporating%7B%7BSNAPSHOT%7D%7Dversionsintothespecification\n\nThe following is equates to true:\n\nVersionRange.createFromVersionSpec( ""[1.0,1.1]"" ).containsVersion( new DefaultArtifactVersion( ""1.1-SNAPSHOT"" ) )\n\nThe attached patch only allows snapshot versions to be contained in a range if they are equal to one of the boundaries.  Note that this is a strict equality, so [1.0,1.2-SNAPSHOT] will not contain 1.1-SNAPSHOT.'}"
maven,bugs-dot-jar_MNG-3131_56cd921f,"{'BugID': 'MNG-3131', 'Summary': 'Error message is misleading if a missing plugin parameter is of a type like List', 'Description': ""Here is a sample output I got when I was working on the changes-plugin:\n\n{code}\n[INFO] One or more required plugin parameters are invalid/missing for 'changes:announcement-mail'\n\n[0] inside the definition for plugin: 'maven-changes-plugin'specify the following:\n\n<configuration>\n  ...\n  <smtpHost>VALUE</smtpHost>\n</configuration>.\n\n[1] inside the definition for plugin: 'maven-changes-plugin'specify the following:\n\n<configuration>\n  ...\n  <toAddresses>VALUE</toAddresses>\n</configuration>.\n{code}\n\nNotice the second parameter toAdresses. It is of the type List, so the correct configuration would be something like this\n\n{code}\n<configuration>\n  ...\n  <toAddresses>\n    <toAddress>VALUE</toAddress>\n  </toAddresses>\n</configuration>.\n{code}\n\nI haven't found where in the code base the handling of List/Map/Array parameters is. That code could probably be borrowed/reused in maven-core/src/main/java/org/apache/maven/plugin/PluginParameterException.java which is the class responsible for formating the above messages.""}"
maven,bugs-dot-jar_MNG-3131_f6f4ef5e,"{'BugID': 'MNG-3131', 'Summary': 'Error message is misleading if a missing plugin parameter is of a type like List', 'Description': ""Here is a sample output I got when I was working on the changes-plugin:\n\n{code}\n[INFO] One or more required plugin parameters are invalid/missing for 'changes:announcement-mail'\n\n[0] inside the definition for plugin: 'maven-changes-plugin'specify the following:\n\n<configuration>\n  ...\n  <smtpHost>VALUE</smtpHost>\n</configuration>.\n\n[1] inside the definition for plugin: 'maven-changes-plugin'specify the following:\n\n<configuration>\n  ...\n  <toAddresses>VALUE</toAddresses>\n</configuration>.\n{code}\n\nNotice the second parameter toAdresses. It is of the type List, so the correct configuration would be something like this\n\n{code}\n<configuration>\n  ...\n  <toAddresses>\n    <toAddress>VALUE</toAddress>\n  </toAddresses>\n</configuration>.\n{code}\n\nI haven't found where in the code base the handling of List/Map/Array parameters is. That code could probably be borrowed/reused in maven-core/src/main/java/org/apache/maven/plugin/PluginParameterException.java which is the class responsible for formating the above messages.""}"
maven,bugs-dot-jar_MNG-3616_912a565f,"{'BugID': 'MNG-3616', 'Summary': 'Null Pointer Exception when mirrorOf missing from mirror in settings.xml', 'Description': 'When attempting to generate any archetype from the mvn archetype:generate command I get a null pointer exception thrown if I have mirrors defined in my settings.xml file but fail to have the mirrorOf element set. \n\nThe stack trace for the archetype generation is:\n\nChoose a number: (1/2/3/4/5/6/7/8/9/10/11/12/13/14/15/16/17/18/19/20/21/22/23/2\n4/25/26/27/28/29/30/31/32/33/34/35/36/37/38/39/40/41/42/43/44) 15: : 6\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] : java.lang.NullPointerException\nnull\n[INFO] ------------------------------------------------------------------------\n[INFO] Trace\norg.apache.maven.BuildFailureException\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Defa\nultLifecycleExecutor.java:579)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandalone\nGoal(DefaultLifecycleExecutor.java:512)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(Defau\nltLifecycleExecutor.java:482)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHan\ndleFailures(DefaultLifecycleExecutor.java:330)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegmen\nts(DefaultLifecycleExecutor.java:227)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLi\nfecycleExecutor.java:142)\nat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:336)\nat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:129)\nat org.apache.maven.cli.MavenCli.main(MavenCli.java:287)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:585)\nat org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)\nat org.codehaus.classworlds.Launcher.launch(Launcher.java:255)\nat org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)\n\nat org.codehaus.classworlds.Launcher.main(Launcher.java:375)\nCaused by: org.apache.maven.plugin.MojoFailureException\nat org.apache.maven.archetype.mojos.CreateProjectFromArchetypeMojo.execu\nte(CreateProjectFromArchetypeMojo.java:202)\nat org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPlugi\nnManager.java:451)\nat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(Defa\nultLifecycleExecutor.java:558)\n... 16 more\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 7 seconds\n[INFO] Finished at: Wed May 28 17:49:39 EST 2008\n[INFO] Final Memory: 8M/14M\n[INFO] ------------------------------------------------------------------------\n\nC:\\Documents and Settings\\frank\\My Documents\\Development\\Sandbox>mvn -v\nMaven version: 2.0.9\nJava version: 1.5.0_08\nOS name: ""windows xp"" version: ""5.1"" arch: ""x86"" Family: ""windows""\n\nThe mirrored settings from the settings.xml file are:\n\n<mirrors>\n<mirror>\n<id>nexus-central</id>\n<url>http://maven.ho.bushlife.com.au:8081/nexus/content/groups/public</url>\n</mirror>\n</mirrors>\n\n\nAs a user you receive a null pointer exception because of something missing in the settings.xml file.\n\nAt the very least you should receive an error message indicating the problem. If you can have a situation where the mirrorOf setting is optional, then it should not be throwing a null pointer exception but handling it better.\n'}"
maven,bugs-dot-jar_MNG-3991_2169c4a3,"{'BugID': 'MNG-3991', 'Summary': 'POM validator allows <scope>optional</scope> but it is not valid.', 'Description': ""In my project I did a mistake and I wrote\n{code}\n<dependency>\n\t<groupId>org.slf4j</groupId>\n\t<artifactId>slf4j-log4j12</artifactId>\n\t<version>1.5.0</version>\n\t<scope>optional</scope>\n</dependency>\n{code}\n\nbut in fact I intended to write\n{code}\n<dependency>\n\t<groupId>org.slf4j</groupId>\n\t<artifactId>slf4j-log4j12</artifactId>\n\t<version>1.5.0</version>\n\t<optional>true</optional>\n</dependency>\n{code}\n\nI'm very surprised that Maven doesn't detect such a mistake during the validate phase. Could it be possible to add a check to allow only valid scopes.\n\nThanks""}"
maven,bugs-dot-jar_MNG-4383_0f3d4d24,"{'BugID': 'MNG-4383', 'Summary': 'Uninterpolated expressions should cause an error for dependency versions', 'Description': 'I declared a dependency as such:\n{noformat}<dependency>\n    <groupId>org.slf4j</groupId>\n    <artifactId>slf4j-api</artifactId>\n    <version>${slf4j.version}</version>\n</dependency>{noformat} \n\nBut did not define the *slf4j.version* property. Obviously ${...} is an expression and if the expression is not resolved, why allow it? Here was the output:\n\n{noformat} Downloading: http://repo1.maven.org/maven2/org/slf4j/slf4j-api/${slf4j.version}/slf4j-api-${slf4j.version}.pom{noformat} \n\nIn terms of usability, an obvious expression should be interpolated. If it cannot, it should be a runtime error.'}"
maven,bugs-dot-jar_MNG-4474_269c956e,"{'BugID': 'MNG-4474', 'Summary': '[regression] Wagon manager does not respect instantiation strategy of wagons', 'Description': 'Calling {{WagonManager.getWagon()}} multiple times from the same thread (and with the same thread context class loader) will always return the same wagon instance, even if the wagon uses ""per-lookup"" instantiation.'}"
maven,bugs-dot-jar_MNG-4512_8cb04253,"{'BugID': 'MNG-4512', 'Summary': '[regression] Profile activation based on JDK version range fails if current version is close to range boundary', 'Description': 'The POM snippet\n{code:xml}\n<profiles>\n  <profile>\n    <id>test</id>\n    <activation>\n      <jdk>[1.5,1.6)</jdk>\n    </activation>\n  </profile>\n</profiles>\n{code}\nyields\n{noformat}\n[ERROR]   The project org.apache.maven.its.mng:test:0.1 has 1 error\n[ERROR]     Failed to determine activation for profile test: For input string: ""0_07""\njava.lang.NumberFormatException: For input string: ""0_07""\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n        at java.lang.Integer.parseInt(Integer.java:456)\n        at java.lang.Integer.parseInt(Integer.java:497)\n        at o.a.m.model.profile.activation.JdkVersionProfileActivator.getRelationOrder(JdkVersionProfileActivator.java:124)\n        at o.a.m.model.profile.activation.JdkVersionProfileActivator.isInRange(JdkVersionProfileActivator.java:96)\n        at o.a.m.model.profile.activation.JdkVersionProfileActivator.isActive(JdkVersionProfileActivator.java:70)\n{noformat}\nwhen run with JDK 1.6.0_07 in this case. \n'}"
maven,bugs-dot-jar_MNG-4518_f5ebc72d,"{'BugID': 'MNG-4518', 'Summary': 'Profile activation based on JRE version misbehaves if java.version has build number', 'Description': 'For this POM snippet\n{code:xml}\n<profiles>\n  <profile>\n    <id>test</id>\n    <activation>\n      <jdk>[1.6,)</jdk>\n    </activation>\n  </profile>\n</profiles>\n{code}\nrunning ""mvn help:active-profiles -V"" delivers:\n{noformat}\n>mvn help:active-profiles -V\nApache Maven 2.2.1 (r801777; 2009-08-06 21:16:01+0200)\nJava version: 1.6.0_07\nJava home: D:\\Programme\\Java\\jdk-1.6.0_07\\jre\nDefault locale: de_DE, platform encoding: Cp1252\nOS name: ""windows xp"" version: ""5.1"" arch: ""x86"" Family: ""windows""\n[INFO] Scanning for projects...\n[INFO] Searching repository for plugin with prefix: \'help\'.\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Maven Integration Test :: MNG-\n[INFO]    task-segment: [help:active-profiles] (aggregator-style)\n[INFO] ------------------------------------------------------------------------\n[INFO] [help:active-profiles {execution: default-cli}]\n[INFO]\nActive Profiles for Project \'org.apache.maven.its.mng:test:jar:0.1\':\n\nThere are no active profiles.\n{noformat}\nThis is due to the version 1.6.0_07 which when parsed with Maven version rules is considered older than 1.6.'}"
maven,bugs-dot-jar_MNG-4529_03a383e3,"{'BugID': 'MNG-4529', 'Summary': 'maven fails on IBM JDK 1.5.0 with exception IllegalAccessException: Field is final', 'Description': 'On Windows XP, and IBM JDK 1.5.0, maven 2.2.1 fails with the exception IllegalAccessException: Field is final. (See the complete stacktrace is at the end)\n\nHow to duplicate:\n1. (IMPORTANT) Delete maven local repository at <user home>\\.m2\\repository directory completely!\n2. Unzip myapp.zip\n3. Run command ""mvn package -e""\n\nMore info:\nThe exception will happen when maven trying to set value to some static final fields. Re-run the command will see another new exception (old exception will not happend again). If you try maven in debug mode (to debug it with Eclipse), the exception will NOT appear. The complete information about maven, JDK, etc. are in the attached file: ibm.output.txt. The other output file (sun.output.txt) is the successful result when running using Sun JDK 1.5.0\n\nRoot cause:\nIn StringSearchModelInterpolator.java of maven 2.2.1, there is a code snippet that tries to using Reflection to change values of fields (even if the fields are final)\n\nLine 175: \n  fields[i].setAccessible( true );\nLine 189:\n  fields[i].set( target, interpolated );\n\nIf fields[i] is a final field, on Sun JDK 1.5.0, the operation is successful but on IBM JDK 1.5.0, an exception will be thrown. \n\n\n\n================ Complete stacktrace ==========================\norg.apache.maven.lifecycle.LifecycleExecutionException: Unable to build project for plugin \'org.apache.maven.plugins:maven-compiler-plugin\': Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2]\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.verifyPlugin(DefaultLifecycleExecutor.java:1557)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.getMojoDescriptor(DefaultLifecycleExecutor.java:1851)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.bindLifecycleForPackaging(DefaultLifecycleExecutor.java:1311)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.constructLifecycleMappings(DefaultLifecycleExecutor.java:1275)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:534)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)\n\tat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)\n\tat org.apache.maven.cli.MavenCli.main(MavenCli.java:362)\n\tat org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:615)\n\tat org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)\n\tat org.codehaus.classworlds.Launcher.launch(Launcher.java:255)\n\tat org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)\n\tat org.codehaus.classworlds.Launcher.main(Launcher.java:375)\nCaused by: org.apache.maven.plugin.InvalidPluginException: Unable to build project for plugin \'org.apache.maven.plugins:maven-compiler-plugin\': Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2]\n\tat org.apache.maven.plugin.DefaultPluginManager.checkRequiredMavenVersion(DefaultPluginManager.java:293)\n\tat org.apache.maven.plugin.DefaultPluginManager.verifyVersionedPlugin(DefaultPluginManager.java:205)\n\tat org.apache.maven.plugin.DefaultPluginManager.verifyPlugin(DefaultPluginManager.java:184)\n\tat org.apache.maven.plugin.DefaultPluginManager.loadPluginDescriptor(DefaultPluginManager.java:1642)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.verifyPlugin(DefaultLifecycleExecutor.java:1540)\n\t... 19 more\nCaused by: org.apache.maven.project.InvalidProjectModelException: Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom for project unknown:maven-compiler-plugin at Artifact [org.apache.maven.plugins:maven-compiler-plugin:pom:2.0.2]\n\tat org.apache.maven.project.DefaultMavenProjectBuilder.buildInternal(DefaultMavenProjectBuilder.java:884)\n\tat org.apache.maven.project.DefaultMavenProjectBuilder.buildFromRepository(DefaultMavenProjectBuilder.java:255)\n\tat org.apache.maven.plugin.DefaultPluginManager.checkRequiredMavenVersion(DefaultPluginManager.java:277)\n\t... 23 more\nCaused by: org.apache.maven.project.interpolation.ModelInterpolationException: Failed to interpolate field: public static final java.lang.String org.codehaus.plexus.util.xml.Xpp3Dom.CHILDREN_COMBINATION_MODE_ATTRIBUTE on class: org.codehaus.plexus.util.xml.Xpp3Dom\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator$InterpolateObjectAction.traverseObjectWithParents(StringSearchModelInterpolator.java:318)\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator$InterpolateObjectAction.run(StringSearchModelInterpolator.java:135)\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator$InterpolateObjectAction.run(StringSearchModelInterpolator.java:102)\n\tat java.security.AccessController.doPrivileged(AccessController.java:192)\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator.interpolateObject(StringSearchModelInterpolator.java:80)\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator.interpolate(StringSearchModelInterpolator.java:62)\n\tat org.apache.maven.project.DefaultMavenProjectBuilder.processProjectLogic(DefaultMavenProjectBuilder.java:990)\n\tat org.apache.maven.project.DefaultMavenProjectBuilder.buildInternal(DefaultMavenProjectBuilder.java:880)\n\t... 25 more\nCaused by: java.lang.IllegalAccessException: Field is final\n\tat sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl.set(UnsafeQualifiedStaticObjectFieldAccessorImpl.java:77)\n\tat java.lang.reflect.Field.set(Field.java:684)\n\tat org.apache.maven.project.interpolation.StringSearchModelInterpolator$InterpolateObjectAction.traverseObjectWithParents(StringSearchModelInterpolator.java:189)\n\t... 32 more\n'}"
maven,bugs-dot-jar_MNG-4565_c6529932,"{'BugID': 'MNG-4565', 'Summary': 'Requiring multiple profile activation conditions to be true does not work', 'Description': ""According to the documentation at http://www.sonatype.com/books/mvnref-book/reference/profiles-sect-activation.html a profile is activated when all activation conditions are met (which makes sense of course). But when I try to use this it does not work. It seems maven does an OR instead of an AND (which is not rearly as useful and is the opposite of what the documentation says at the previous link).\n\nFor example, if I have one profile that is activated like this:\n\n{code:xml}         <activation>\n            <activeByDefault>false</activeByDefault>\n            <os>\n               <name>linux</name>\n            </os>\n         </activation>{code}\n\nand another profile that is activated like this:\n\n{code:xml}        <activation>\n            <activeByDefault>false</activeByDefault>\n            <os>\n               <name>linux</name>\n            </os>\n            <property>\n                <name>release</name>\n                <value>true</value>\n            </property>\n         </activation>{code}\n\nThen I would expect the second profile to only be activated if the OS is linux and the release property is defined.\n\nWhen I run 'mvn help:active-profiles' however, maven shows that both profiles are active even though the release property is not defined.\n""}"
maven,bugs-dot-jar_MNG-4648_83389c34,"{'BugID': 'MNG-4648', 'Summary': 'NullPointerException thrown from DefaultPluginRealmCache#pluginHashCode method if project-level plugin dependency misses version', 'Description': ""As a user i would like to see better error reporting from DefaultPluginRealmCache#pluginHashCode method\n\nCurrently it calculates hash value based on a dependency metadata, but if I omit version it fails with NullPointer exception.\n\nIt would be more user friendly to validate metadata prior to calculating hash value and to display more meaningful error to the end user.\n\nTest scenario:\n - configure plugin and create dependencies\n - add dependency but DO NOT specify version\n - run maven such that plugin is invoked\nmaven will fail without reporting which dependency doesn't have version""}"
maven,bugs-dot-jar_MNG-4695_bb39b480,"{'BugID': 'MNG-4695', 'Summary': 'Missing Error during pom validation: ""You cannot have two plugin executions with the same (or missing) <id/> elements.""', 'Description': 'Maven 2.2.1 gives an error ""You cannot have two plugin executions with the same (or missing) <id/> elements."", if there are two executions for the same plugin without an id element. \nMaven 3.0 beta1 doesn\'t throw this usefull validation error. It uses the first configuration for both executions.\n\nA relatet issue, where this error has occured with an attached example: http://jira.codehaus.org/browse/MRPM-76\n'}"
maven,bugs-dot-jar_MNG-4761_8cdb461f,"{'BugID': 'MNG-4761', 'Summary': 'Plugin-level dependency scope causes some plugin classpaths to be incorrect', 'Description': ""Plugin-level dependencies should use RUNTIME scope at all times. Using any other scope may alter the weighting given to the subgraph-choice algorithm used in transitive dependency resolution.\n\n\nPlugin-level dependencies use compile scope by default. When transitive resolution takes place, compile scope takes precedence over runtime scope, causing the transitive dependency sub-graph of the plugin-level dependency to be activated over those of the plugin itself.\n\nThis happens even when the plugin's transitive dep is NEARER to the top level than the one brought in by that plugin-level dependency itself.\n\nThe result is that when a dep that's farther away is chosen over a nearer one, it can then be disabled by Maven choosing to disable its parent dep (the one that brought it in) in another part of the transitive resolution process.\n\nThis is a very subtle case where Maven is doing the wrong thing. The attached test case should make it clearer.""}"
maven,bugs-dot-jar_MNG-4837_3fca2bb2,"{'BugID': 'MNG-4837', 'Summary': 'Interpolation error due to cyclic expression for one of the POM coordinates gets needlessly repeated', 'Description': ""This simple POM\n{code:xml}\n<project>\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>${groupId}</groupId>\n  <artifactId>test</artifactId>\n  <version>0.1</version>\n  <packaging>jar</packaging>\n\n  <distributionManagement>\n    <repository>\n      <id>maven-core-it</id>\n      <url>file:///${basedir}/repo</url>\n    </repository>\n  </distributionManagement>\n</project>\n{code}\ncauses the following model errors:\n{noformat}\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     Resolving expression: '${groupId}': Detected the following recursive expression cycle: [groupId] -> [Help 2]\n[ERROR]     'groupId' with value '${groupId}' does not match a valid id pattern. @ line 25, column 12\n{noformat}\nNote the excessive repetition of the groupId related cycle although this expression actually appears only once in the POM.""}"
maven,bugs-dot-jar_MNG-4915_1c3abfba,"{'BugID': 'MNG-4915', 'Summary': 'Versions in pom.xml are not checked for invalid characters', 'Description': 'It seems that the pom.xml is not checking if the version contains invalid characters. If I have following fragment inside pom:\n\n<dependency>\n  <groupId>junit</groupId>\n  <artifactId>junit</artifactId>\n  <version>>>3.5</version>\n</dependency>\n\nthen Maven is trying to download JUnit version >>3.5\n\n'}"
maven,bugs-dot-jar_MNG-4918_691a03a7,"{'BugID': 'MNG-4918', 'Summary': 'MavenProject#clone() doubles active profiles', 'Description': 'The error occured in our Project with more than 60 submodules and aggregating JavaDoc. Due to the forking of the LifeCycle many clones of the MavenProject object seem to be performed. Since MavenProject#clone doubles the entries in the list of active profiles we start with one active profile and after a few dozen clones the list of active profiles exceeds 10.000.000 elements. This will than kill the VM by OOME. \n\nmavenProject.getActiveProfiles().size() == 1 \nmavenProject.clone().getActiveProfiles().size() == 2 \nmavenProject.clone().clone().getActiveProfiles().size() == 4\nand so on '}"
maven,bugs-dot-jar_MNG-4933_469d0096,"{'BugID': 'MNG-4933', 'Summary': 'With a resource directory as . maven raise an java.lang.StringIndexOutOfBoundsException:217', 'Description': 'I exexute a release:prepare-with-pom\nI debug this execution and I found when the directory is equal to basedir.getPath() an exception is raised.\n\nAnd I have this definition in my pom.xml\n      <resource>\n        <directory>.</directory>\n        <includes>\n          <include>plugin.xml</include>\n          <include>plugin.properties</include>\n          <include>icons/**</include>\n        </includes>\n      </resource>\n\nTrace:\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom (default-cli) on project servicelayer: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1 -> [Help 1]\norg.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom (default-cli) on project servicelayer: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:211)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:148)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:140)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)\n        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)\n        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)\n        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:316)\n        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:153)\n        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:451)\n        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:188)\n        at org.apache.maven.cli.MavenCli.main(MavenCli.java:134)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)\n        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)\nCaused by: org.apache.maven.plugin.PluginExecutionException: Execution default-cli of goal org.apache.maven.plugins:maven-release-plugin:2.1.1:prepare-with-pom failed: String index out of range: -1\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:116)\n        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:195)\n        ... 19 more\nCaused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1\n        at java.lang.String.substring(String.java:1937)\n        at java.lang.String.substring(String.java:1904)\n        at org.apache.maven.project.path.DefaultPathTranslator.unalignFromBaseDirectory(DefaultPathTranslator.java:217)\n        at org.apache.maven.project.path.DefaultPathTranslator.unalignFromBaseDirectory(DefaultPathTranslator.java:181)\n        at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.createReleaseModel(GenerateReleasePomsPhase.java:274)\n        at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.generateReleasePom(GenerateReleasePomsPhase.java:141)\n        at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.generateReleasePoms(GenerateReleasePomsPhase.java:129)\n        at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.execute(GenerateReleasePomsPhase.java:105)\n        at org.apache.maven.shared.release.phase.GenerateReleasePomsPhase.execute(GenerateReleasePomsPhase.java:92)\n        at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:203)\n        at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:140)\n        at org.apache.maven.shared.release.DefaultReleaseManager.prepare(DefaultReleaseManager.java:103)\n        at org.apache.maven.plugins.release.PrepareReleaseMojo.prepareRelease(PrepareReleaseMojo.java:279)\n        at org.apache.maven.plugins.release.PrepareWithPomReleaseMojo.execute(PrepareWithPomReleaseMojo.java:47)\n        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:107)\n        ... 20 more\n[ERROR]\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]'}"
maven,bugs-dot-jar_MNG-4941_c4002945,"{'BugID': 'MNG-4941', 'Summary': ""PluginDescriptorBuilder doesn't populate expression/default-value fields for mojo parameters"", 'Description': 'As noted by Guo Du in his patch for MPH-81, the mojo descriptors created by the {{PluginDescriptorBuilder}} come out with parameter descriptors that always return null for the parameter expression and default value.'}"
maven,bugs-dot-jar_MNG-5003_a7d9b689,"{'BugID': 'MNG-5003', 'Summary': 'MavenPluginManager serves m2e partially initialized mojo descriptors in some cases', 'Description': ""Not sure if this affect command line maven, but m2e sometimes gets MojoDescriptor instances with null classRealm and implementationClass. Looks like the problem has to do with incomplete plugin descriptor setup when after reused cached plugin realm. I'll try to provide a fix and corresponding unit test.""}"
maven,bugs-dot-jar_MNG-5075_2eb419ed,"{'BugID': 'MNG-5075', 'Summary': 'MavenProject.getParent throws undocumented ISE', 'Description': 'http://bugzilla-attachments-197994.netbeans.org/bugzilla/attachment.cgi?id=107899 shows a stack trace encountered when calling {{MavenProject.getParent}} on a project with some errors (probably POMs missing in the local repository).\n\nThis method has no Javadoc comment, so it is hard to know exactly what it is permitted/supposed to do, but {{hasParent}} implies that {{null}} is a valid return value, and there is no {{throws IllegalStateException}} clause. The attached patch brings the behavior in line with that signature. (I think I got the {{PlexusTestCase}} infrastructure working with all the required wiring but it may be possible to simplify the test case.)\n\nCleaner might be to just declare {{getParent}} (and also {{hasParent}}?) to throw {{ProjectBuildingException}}, though this would be a source-incompatible change. (Only binary-incompatible for clients which are already catching {{IllegalStateException}}!)'}"
maven,bugs-dot-jar_MNG-5209_87884c7b,"{'BugID': 'MNG-5209', 'Summary': 'MavenProject.getTestClasspathElements can return null elements', 'Description': 'A broken {{MavenProject}} can return null values in certain lists, which seems incorrect (at least it is undocumented). Root cause of: http://netbeans.org/bugzilla/show_bug.cgi?id=205867'}"
maven,bugs-dot-jar_MNG-5209_ed651a4d,"{'BugID': 'MNG-5209', 'Summary': 'MavenProject.getTestClasspathElements can return null elements', 'Description': 'A broken {{MavenProject}} can return null values in certain lists, which seems incorrect (at least it is undocumented). Root cause of: http://netbeans.org/bugzilla/show_bug.cgi?id=205867'}"
maven,bugs-dot-jar_MNG-5212_712c4fff,"{'BugID': 'MNG-5212', 'Summary': 'DefaultPluginDescriptorCache does not retain pluginDescriptor dependencies', 'Description': ""PluginDescriptor dependencies is defined in META-INF/maven/plugin.xml, so there is no reason not to retain this field when storing and retrieving PluginDescriptor instances to and from descriptor cache. The attribute can be used in embedding scenarios and for command line builds to quickly determine if plugin has certain dependencies or not, without having to fully resolve plugin dependencies. I'll commit a fix with corresponding unit test shortly.""}"
maven,bugs-dot-jar_MNG-5212_c53d95ce,"{'BugID': 'MNG-5212', 'Summary': 'DefaultPluginDescriptorCache does not retain pluginDescriptor dependencies', 'Description': ""PluginDescriptor dependencies is defined in META-INF/maven/plugin.xml, so there is no reason not to retain this field when storing and retrieving PluginDescriptor instances to and from descriptor cache. The attribute can be used in embedding scenarios and for command line builds to quickly determine if plugin has certain dependencies or not, without having to fully resolve plugin dependencies. I'll commit a fix with corresponding unit test shortly.""}"
maven,bugs-dot-jar_MNG-5459_c225847e,"{'BugID': 'MNG-5459', 'Summary': 'failure to resolve pom artifact from snapshotVersion in maven-metadata.xml', 'Description': ""We're using Artifactory on the server side, and ivy / sbt to publish artifacts  upstream.\n\nAfter publishing several -SNAPSHOT versions of a project, trying to use it from Maven, resulted in a warning and ultimately a build failure because it cannot determine the dependencies:\n\n{code}[WARNING] The POM for com.foo:bar:jar:0.4.0-20130404.093655-3 is missing, no dependency information available{code}\n\nThis is the corresponding maven-metadata-snapshots.xml:\n{code:xml}\n<metadata>\n  <groupId>com.foo</groupId>\n  <artifactId>bar</artifactId>\n  <version>0.4.0-SNAPSHOT</version>\n  <versioning>\n    <snapshot>\n      <timestamp>20130404.090532</timestamp>\n      <buildNumber>2</buildNumber>\n    </snapshot>\n    <lastUpdated>20130404093657</lastUpdated>\n    <snapshotVersions>\n      <snapshotVersion>\n        <extension>pom</extension>\n        <value>0.4.0-20130404.090532-2</value>\n        <updated>20130404090532</updated>\n      </snapshotVersion>\n      <snapshotVersion>\n        <extension>jar</extension>\n        <value>0.4.0-20130404.093655-3</value>\n        <updated>20130404093655</updated>\n      </snapshotVersion>\n    </snapshotVersions>\n  </versioning>\n</metadata>\n{code}\n\nAs you can see, the <value> for the jar artifact and the pom artifact differ:\n\n0.4.0-20130404.093655-3\n0.4.0-20130404.090532-2\n\nApparently, artifactory optimizes the case when an artifact doesn't change; it does not create a new file, but just links to the existing one.\n\nMaven, however, takes a shortcut and makes the erroneous assumption that the values for pom and jar artifact always match up.\n\nThe attached patch fixes this.""}"
maven,bugs-dot-jar_MNG-5613_bef7fac6,"{'BugID': 'MNG-5613', 'Summary': 'NPE error when building a reactor with duplicated artifacts', 'Description': ""Using v3.2.1 when building a malformed project containing a duplicated groupId:artifactId I got this rather unhelpful error:\n\n{code}\n[ERROR] Internal error: java.lang.NullPointerException -> [Help 1]\norg.apache.maven.InternalErrorException: Internal error: java.lang.NullPointerException\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:167)\n\tat org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)\n\tat org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)\n\tat org.apache.maven.cli.MavenCli.main(MavenCli.java:157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\nCaused by: java.lang.NullPointerException\n\tat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:270)\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)\n\t... 11 more\n{code}\n\nThe more helpful error should have been:\n{code}\norg.apache.maven.project.DuplicateProjectException: Project 'com.foo.bar:foo-bar:2.15.0' is duplicated in the reactor\n\tat org.apache.maven.project.ProjectSorter.<init>(ProjectSorter.java:93)\n\tat org.apache.maven.DefaultProjectDependencyGraph.<init>(DefaultProjectDependencyGraph.java:53)\n\tat org.apache.maven.DefaultMaven.createProjectDependencyGraph(DefaultMaven.java:819)\n\tat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:268)\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)\n\tat org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)\n\tat org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)\n\tat org.apache.maven.cli.MavenCli.main(MavenCli.java:157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)\n\tat org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)\n{code}""}"
maven,bugs-dot-jar_MNG-5645_af1ecd5f,"{'BugID': 'MNG-5645', 'Summary': 'version of ""..."" causes InternalErrorException.', 'Description': 'The following dependency causes InternalErrorException.\n\n    <dependency>\n      <groupId>any</groupId>\n      <artifactId>any</artifactId>\n      <version>...</version>\n    </dependency>\n\nThis can be confusing to a new maven user trying to get a dependency to work.\n\nA patch is attached that fixes the problem.'}"
maven,bugs-dot-jar_MNG-5647_cdb8ad6d,"{'BugID': 'MNG-5647', 'Summary': '${maven.build.timestamp} uses incorrect ISO datetime separator', 'Description': 'Separator must be {{T}} and not {{-}} if we predefine ISO 8601 datetime format. Additionally, {{Z}} should be added to denote UTC time zone.'}"
maven,bugs-dot-jar_MNG-5655_96337372,"{'BugID': 'MNG-5655', 'Summary': 'WeakMojoExecutionListener callbacks invoked multiple times in some cases', 'Description': ""When the same WeakMojoExecutionListener instance is injected under multiple component Key's, the instance before/after execution callbacks are invoked multiple times.""}"
maven,bugs-dot-jar_MNG-5687_3d2d8619,"{'BugID': 'MNG-5687', 'Summary': 'Parallel Builds can build in wrong order', 'Description': ""Fixed JDK8 IT failure for MavenITmng3004ReactorFailureBehaviorMultithreadedTest#testitFailFastSingleThread\n\nIt turns out the execution order of the modules in the build can be incorrect, in some cases severely incorrect.\nFor parallel builds this can have all sorts of interesting side effects such as classpath\nappearing to be intermittently incorrect, missing jars/resources and similar.\n\nThe -am options and -amd options may simply fail with the incorrect build order\nbecause expected dependencies have not been built and actual dependencies may not have been built.\n\nThe underlying problem was that ProjectDependencyGraph#getDownstreamProjects and getUpstreamProjects\ndid not actually obey the reactor build order as defined by ProjectDependencyGraph#getSortedProjects,\neven though the javadoc claims they should.\n\nThis has only worked by accident on earlier JDK's and might not have worked at all (basically\ndepends on Set iteration order being equal to insertion order). JDK8 has slightly different\niteration order, which caused the IT failure.\n\nThis problem may be the root cause of MNG-4996 and any other issue where the modules build\nin incorrect order.\n\nThe bug affects:\n\nparallel builds\ncommand line -am (--also-make) option\ncommand line -amd (also-make-dependents) option\n\nOn all java versions, although visibility might be somewhat different on different jdks.\n\nAdded simple unit test that catches the problem.\n""}"
maven,bugs-dot-jar_MNG-5716_2d0ec942,"{'BugID': 'MNG-5716', 'Summary': 'ToolchainManagerPrivate.getToolchainsForType() returns toolchains that are not of expected type', 'Description': ""found while working on maven-toolchains-plugin 1.1:\n{noformat}[INFO] Required toolchain: fake-type [ other-attribute='other-value' attribute='value' ]\n[DEBUG] Toolchain JDK[/home/opt/jdk1.5] is missing required property: other-attribute\n[DEBUG] Toolchain JDK[/home/opt/jdk1.6] is missing required property: other-attribute\n[DEBUG] Toolchain JDK[/home/opt/jdk1.7] is missing required property: other-attribute\n[DEBUG] Toolchain JDK[/home/opt/jdk1.8] is missing required property: other-attribute\n[ERROR] No toolchain matched for type fake-type\n[INFO] Required toolchain: another-fake-type [ any ]\n[INFO] Toolchain matched for type another-fake-type: JDK[/home/opt/jdk1.5]{noformat}\n\nthese jdk toochains should not have been ever tested again non-jdk type requirement""}"
maven,bugs-dot-jar_MNG-5727_ce6f0bfd,"{'BugID': 'MNG-5727', 'Summary': 'unexpected InvalidArtifactRTException from ProjectBuilder#build', 'Description': 'Calling into ProjectBuilder#build(File, ProjectBuildingRequest) results in InvalidArtifactRTException below if project pom.xml has managed dependency without <version>. Although the pom is invalid, I expected to get ProjectBuildingException that includes location of problematic dependency, similar to what I get during command line build.\n\n{code}\norg.apache.maven.artifact.InvalidArtifactRTException: For artifact {org.apache.maven.its:a:null:jar}: The version cannot be empty.\n\tat org.apache.maven.artifact.DefaultArtifact.validateIdentity(DefaultArtifact.java:148)\n\tat org.apache.maven.artifact.DefaultArtifact.<init>(DefaultArtifact.java:123)\n\tat org.apache.maven.bridge.MavenRepositorySystem.XcreateArtifact(MavenRepositorySystem.java:695)\n\tat org.apache.maven.bridge.MavenRepositorySystem.XcreateDependencyArtifact(MavenRepositorySystem.java:613)\n\tat org.apache.maven.bridge.MavenRepositorySystem.createDependencyArtifact(MavenRepositorySystem.java:121)\n\tat org.apache.maven.project.DefaultProjectBuilder.initProject(DefaultProjectBuilder.java:808)\n\tat org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:174)\n\tat org.apache.maven.project.DefaultProjectBuilder.build(DefaultProjectBuilder.java:118)\n...\n{code}\n\n'}"
maven,bugs-dot-jar_MNG-5742_6ab41ee8,"{'BugID': 'MNG-5742', 'Summary': 'inconsistent classloading for extensions=true plugins', 'Description': 'Maven creates two class realms for build extensions plugins. One realm is used to contribute core extensions and the other to execute plugins goals. The two realms have slightly different classpath, with extensions realm not ""seeing"" classes from other extensions and not resolving reactor dependencies. The two realms are mostly independent and have duplicate copies of components, including duplicate copies of singletons. This results in multiple invocation of singleton components in some cases. This also results inconsistent/unexpected component wiring. '}"
wicket,bugs-dot-jar_WICKET-128_7e1000dd,"{'BugID': 'WICKET-128', 'Summary': 'Debug settings / serialize session attributes option not working', 'Description': ""Session attributes are serialized even if this debug setting is turned off. I've noticed that the code that serializes attributes and logs their serialized size in HttpSessionStore#setAttribute is duplicated in Session#setAttribute - but without the debug settings condition. This code was added by the recent patch resolving WICKET-100 and only in the trunk, not in the wicket-1.x branch... why???\n\nRegards,\nBendis""}"
wicket,bugs-dot-jar_WICKET-1619_b154d12f,"{'BugID': 'WICKET-1619', 'Summary': ""PagingNavigator.setEnabled(false) doesn't work "", 'Description': '1. Create paging navigator PagingNavigator \n2. call PagingNavigator.setEnabled(false)\n3. navigator will be rendered as enabled, if click on any link (""1"", ""2"" etc) - content of the data view will be changed.\n\nIn many cases it\'s necessary disable navigator, for example, when user need to edit only single line of DataView other controls need to be disabled. '}"
wicket,bugs-dot-jar_WICKET-1677_01a3dd66,"{'BugID': 'WICKET-1677', 'Summary': 'AjaxFormChoiceComponentUpdatingBehavior affects checkboxes even if component uses radios and vice-versa', 'Description': ""I have a form with two radio buttons.  Depending which radio the user selects, I show one form or another form.  I'm using an AjaxFormChoiceComponentUpdatingBehavior attached to the RadioGroup.  One of the forms has a checkbox.  The checkbox triggers an ajax update--even though the AjaxFormChoiceComponentUpdatingBehavior is attached to a RadioGroup.  AjaxFormChoiceComponentUpdatingBehavior should only affect the appropriate controls based on whether it is attached to a choice component that uses radios or checkboxes.  If a developer really wants both, then he can use two AjaxFormChoiceComponentUpdatingBehavior instances.\n\nI've attached a patch. ""}"
wicket,bugs-dot-jar_WICKET-16_3431e60d,"{'BugID': 'WICKET-16', 'Summary': 'missing base64/ URL encoding', 'Description': 'yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug. \n \nI have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like: \n \n @Override\n        public void onSubmit() \n        {\n            String suchFeld = getSuchfeld();\n            if(suchFeld.length()>0)\n            {\n                PageParameters params = new PageParameters();\n                params.add(""finde"",suchFeld);\n                setResponsePage(Suche.class,params);\n            }\n            else\n            {\n                setResponsePage(getPage().getClass());\n            }  \n      }\n \nthe component is put into a ""BasePage"":\n \n public BasePage() {\n        ....    \n        add(bar);\n        add(new SuchPanel(""SuchPanel""));\n        .....\n}\n \nwich is then extended by the real page:\n \npublic class Foo extends BasePage{\n    \n    /** Creates a new instance of Zigarren */\n    public Foo() {\n        }\n \nwich works all fine, however if the class name contains non ascii letters\n(e.g: ö ä ü etc.) it gives me a bug if nothing is entered into the search and the part\n \npublic class Zubehör extends BasePage{\n    \n    /** Creates a new instance of Zubehör */\n    public Zubehör() {\n    }\n \n""setResponsePage(getPage().getClass());"" comes to action, the trouble is that the page might have the URL:\n?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r\nbut the form tries to go to : \nwicket:bookmarkablePage=:de.pages.Zubeh%F6r\n \nwich results in a CODE 404 in the App Server \n'}"
wicket,bugs-dot-jar_WICKET-16_6c5083b4,"{'BugID': 'WICKET-16', 'Summary': 'missing base64/ URL encoding', 'Description': 'yesterday i showed the concept of omponents to a friend and stumled into something i dont understand and think it might be a bug. \n \nI have a small panelcompoment that holds a searchform (textfield + submit) nothing special here, the code behind looks like: \n \n @Override\n        public void onSubmit() \n        {\n            String suchFeld = getSuchfeld();\n            if(suchFeld.length()>0)\n            {\n                PageParameters params = new PageParameters();\n                params.add(""finde"",suchFeld);\n                setResponsePage(Suche.class,params);\n            }\n            else\n            {\n                setResponsePage(getPage().getClass());\n            }  \n      }\n \nthe component is put into a ""BasePage"":\n \n public BasePage() {\n        ....    \n        add(bar);\n        add(new SuchPanel(""SuchPanel""));\n        .....\n}\n \nwich is then extended by the real page:\n \npublic class Foo extends BasePage{\n    \n    /** Creates a new instance of Zigarren */\n    public Foo() {\n        }\n \nwich works all fine, however if the class name contains non ascii letters\n(e.g: ö ä ü etc.) it gives me a bug if nothing is entered into the search and the part\n \npublic class Zubehör extends BasePage{\n    \n    /** Creates a new instance of Zubehör */\n    public Zubehör() {\n    }\n \n""setResponsePage(getPage().getClass());"" comes to action, the trouble is that the page might have the URL:\n?wicket:bookmarkablePage=:de.pages.Zubeh%C3%B6r\nbut the form tries to go to : \nwicket:bookmarkablePage=:de.pages.Zubeh%F6r\n \nwich results in a CODE 404 in the App Server \n'}"
wicket,bugs-dot-jar_WICKET-1718_bb7f9cf5,"{'BugID': 'WICKET-1718', 'Summary': 'WebPage#onAfterRender erroneously reports missing header', 'Description': ""In WebPage#onAfterRender() there's a check wether a header was missing on a page and header contributions would be lost.\n\nIn the following case this check erroneously barks:\n- page A was requested\n- in A's onBeforeRender() a RestartResponseAtInterceptPageException to page B is thrown\n- page A's onAfterRender() is invoked in a finally block\n- processing continues with page B\n\nPage A's onAfterRender() complains about the missing header, althought his page was never completely rendered.\n\nIMHO there's a check missing in WebPage#onAfterRender():\n\n    \tif (getRequestCycle().getResponsePage() == this) {\n\t\t.....\n\t}\n\nOr is Page A not allowed to throw RestartResponseAtInterceptPageException in onBeforeRender() at all?""}"
wicket,bugs-dot-jar_WICKET-172_99e22ce4,"{'BugID': 'WICKET-172', 'Summary': 'Component reAttach and versioning', 'Description': 'I\'m reAttaching a component doing something like: \n\nMyFooPanel p1 = new MyFooPanel(this, ""panel"";); \nMyBarPanel p2 = new MyBarPanel(this, ""panel""); \np1.reAttach(); \n\nWhen I try to restore to the initial page version I found that the component with id ""panel"" is not a children component of the page. \n\nI have investigated it and I think it is because when the component is reAttached the order in which the changes are added to the ChangesList is: \n- Add p2. \n- Remove p1. \n\nWhen the initial version is restored the undo functionality is done in reverse mode like, \n- Add p1. \n- Remove p2. \n\nThe problem is p1 and p2 have the same id, so when p2 is removed what is removing is p1 that has just added. \n\nOscar.'}"
wicket,bugs-dot-jar_WICKET-1886_5226978a,"{'BugID': 'WICKET-1886', 'Summary': 'WicketTester Cookie handling', 'Description': 'While trying to test my SecureForm implementation (https://issues.apache.org/jira/browse/WICKET-1885) with WicketTester I ran into this issue: A cookie set in the response never shows up in the ""next"" request, because both have their own lists of cookies that aren\'t shared.\n\nAfaik both should share the same List instance to handle cookies. That way its possible to set a cookie in the response and read it from the request.\n\nA simple testcase is attached.'}"
wicket,bugs-dot-jar_WICKET-1897_8ee095bf,"{'BugID': 'WICKET-1897', 'Summary': 'StatelessForm submitted to the wrong page', 'Description': ""I made a small application to reproduce the problem. You can download it from http://aditsu.net/wickettest.zip , I'll try to attach it too.\nDependencies: jetty 6, wicket 1.4-m3, slf4j, log4j\nSteps to reproduce:\n1. Run the test.Start class\n2. Open http://localhost:8080 in a browser\n3. Open http://localhost:8080/page2 in a new tab\n4. Go to the first tab and click submit\n\nResult:\n\nWicketRuntimeException: unable to find component with path form on stateless page [Page class = test.Page2, id = 0, version = 0]\n\nIt looks like the 2 pages are created with the same id in 2 different pagemaps, but when I submit the form, it goes to the second pagemap and finds the second page (with no form on it).""}"
wicket,bugs-dot-jar_WICKET-1931_986848f7,"{'BugID': 'WICKET-1931', 'Summary': ""FormTester doesn't correctly submit a form when a FileUploadField was not set (which is not required)"", 'Description': ""FormTester doesn't correctly submit a form when  a FileUploadField was not set.\nThis file is not required.\n\nSo it is impossible to create a real test because I am forced to always set a File to check to whole form.\n\nThere was discussion about this problem here: http://www.nabble.com/FormTester-and-FileUploadField-td18566869.html\n\n\nI will be very grateful if you can fix it :)\nArtur""}"
wicket,bugs-dot-jar_WICKET-2033_420ac965,"{'BugID': 'WICKET-2033', 'Summary': '&amp; instead of & in javascript', 'Description': 'the non httpsessionstore part of:\nhttps://issues.apache.org/jira/browse/WICKET-1971\n\nis that \n\nin the \nwicket:ignoreIfNotActive actually becomes\n\namp;wicket:ignoreIfNotActive=true\n\nin:\n\n\tprotected CharSequence encode(RequestCycle requestCycle,\n\t\t\tIListenerInterfaceRequestTarget requestTarget)\n\nof WebRequestCodingStrategy on the line:\n\n\t\t\turl.append(url.indexOf(""?"") > -1 ? ""&amp;"" : ""?"").append(\n\t\t\t\t\tIGNORE_IF_NOT_ACTIVE_PARAMETER_NAME).append(""=true"");\n\n\nso when this happens in \n\tpublic final RequestParameters decode(final Request request) {\n\n---\n\t\tif (request.getParameter(IGNORE_IF_NOT_ACTIVE_PARAMETER_NAME) != null)\n\t\t{\n\t\t\tparameters.setOnlyProcessIfPathActive(true);\n\t\t}\n---\n\nthis never actually happens.\n\n\nthen if you have a throttle, ajaxlazyloadpanel etc with onlyprocessifpathactive set to true, and you logout, but go to another wicket page, then the original session is destroyed and a new one is created\n\nif this is worked around in the way the  guys on WICKET-1971 suggest,\nWebRequestCycleProcessor\n\nmethod\n\n\tpublic IRequestTarget resolve(final RequestCycle requestCycle,\n\t\t\tfinal RequestParameters requestParameters)\n\n\n\t\t\t\tif (requestParameters.isOnlyProcessIfPathActive())\nlast branch falls through:\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\t// TODO also this should work..\n\t\t\t\t\t}\n\n\nand it throws PageExpiredException because the request component/page/behavior does not exist in this new session.   even though onlyprocessifpathactive was set to true, and it\'s purpose is precisely to avoid pageexpiredexception.'}"
wicket,bugs-dot-jar_WICKET-2057_e2d88568,"{'BugID': 'WICKET-2057', 'Summary': 'AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator)', 'Description': 'AjaxPreprocessingCallDecorator calls the delegate decorator before itself (same behavior as AjaxPostprocessingCallDecorator), when it should call itself before the delegate.'}"
wicket,bugs-dot-jar_WICKET-2060_0578d6ee,"{'BugID': 'WICKET-2060', 'Summary': 'Invalid javascript when setStripJavascriptCommentsAndWhitespace is enabled', 'Description': ""When setStripJavascriptCommentsAndWhitespace is enabled (for example in deployment mode), some javascript files get corrupted. For example, the following line (notice the 2 spaces after 'return')\nreturn  this.__unbind__(type, fn);\nis compacted to\nreturn\nthis.__unbind__(type, fn);\nwhich does not execute the unbind function.""}"
wicket,bugs-dot-jar_WICKET-2065_9da430fb,"{'BugID': 'WICKET-2065', 'Summary': 'Generated urls for mounted pages contain redundant trailing ""/""', 'Description': 'Is it OK (i.e. ""by design"" as opposed to ""by mistake"") that the urls generated for the mounted pages end up with the ""/""?\n\nProvided that there\'s a page that expects single parameter (here: ""content"")...\npublic class HelpPage extends WebPage {\npublic HelpPage(PageParameters p) {\nsuper(p);\nadd(new DynamicContentPanel(""contentPanel"", new Model<String>(p.getString(""content""))));\n}\n}\n\n...and it is mounted in the Application#init()\nmount(new BookmarkablePageRequestTargetUrlCodingStrategy(""help"", HelpPage.class, null));\n\n...and further referred to somewhere else as:\nadd(new BookmarkablePageLink(""helpPage"", HelpPage.class, new PageParameters(""content=a"")));\n\nthe url in the generated markup is in the following form:\nhttp://localhost:8080/dummy-web/help/content/a/;jsessionid=11624C6125F8DF4867E3218676D79A29\n\nWhile IMHO it should read:\nhttp://localhost:8080/dummy-web/help/content/a;jsessionid=11624C6125F8DF4867E3218676D79A29\n\nIt looks even more awkward when there are more parameters and part of them is encoded as a query string:\nhttp://localhost:8080/dummy-web/help/content/a/?param2=value2/;jsessionid=11624C6125F8DF4867E3218676D79A29\n\nThe page parameter for both cases is resolved correctly by the HelpPage\'s constructor, so it seems that even though there\'s an extra ""/"" at the end of the url it gets omitted.\nThen why bother generating it?\n\nI stumbled upon an issue https://issues.apache.org/jira/browse/WICKET-765. Apart from the compatibility with wicket 1.2 I see no rationale for trailing ""/"". Looking at implementations of IRequestTargetUrlCodingStrategy I come to the conclusion the the ""append(""/"")"" is being overused and redundant especially when it is preceded by the following code which makes sure that the ""/"" is in place before adding another parameter.'}"
wicket,bugs-dot-jar_WICKET-2079_ceac38b1,"{'BugID': 'WICKET-2079', 'Summary': 'Component Use Check always fails for visible components inside an invisible border body', 'Description': None}"
wicket,bugs-dot-jar_WICKET-208_b224bad8,"{'BugID': 'WICKET-208', 'Summary': 'Fixing AjaxTimerBehaviorTest', 'Description': 'This is an attempt to fix failing testcase:\n\ntarget/surefire-reports/wicket.ajax.AjaxTimerBehaviorTest.txt\n-------------------------------------------------------------------------------\nTest set: wicket.ajax.AjaxTimerBehaviorTest\n-------------------------------------------------------------------------------\nTests run: 2, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec <<< FAILURE!\ntestAddToAjaxUpdate(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.063 sec  <<< FAILURE!\njunit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp\nected:<1> but was:<0>\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.failNotEquals(Assert.java:282)\n        at junit.framework.Assert.assertEquals(Assert.java:64)\n        at junit.framework.Assert.assertEquals(Assert.java:201)\n        at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)\n        at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:143)\n        at wicket.ajax.AjaxTimerBehaviorTest.testAddToAjaxUpdate(AjaxTimerBehaviorTest.java:99)\n\ntestAddToWebPage(wicket.ajax.AjaxTimerBehaviorTest)  Time elapsed: 0.026 sec  <<< FAILURE!\njunit.framework.AssertionFailedError: There should be 1 and only 1 script in the markup for this behavior,but 0 were found exp\nected:<1> but was:<0>\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.failNotEquals(Assert.java:282)\n        at junit.framework.Assert.assertEquals(Assert.java:64)\n        at junit.framework.Assert.assertEquals(Assert.java:201)\n        at wicket.ajax.AjaxTimerBehaviorTest.validateTimerScript(AjaxTimerBehaviorTest.java:178)\n        at wicket.ajax.AjaxTimerBehaviorTest.validate(AjaxTimerBehaviorTest.java:155)\n        at wicket.ajax.AjaxTimerBehaviorTest.testAddToWebPage(AjaxTimerBehaviorTest.java:127)\n\nThe attached patch properly handles the case when the callback script is added in body onload.  Also, AbstractAjaxTimerBehavior needs to handle AjaxRequestTarget properly, because adding a body onload has no effect in an ajax request.'}"
wicket,bugs-dot-jar_WICKET-2172_ea4a3f8a,"{'BugID': 'WICKET-2172', 'Summary': 'PageParameters construced with keyValuePairs does not handle array values', 'Description': 'The PageParameters constructor that takes a ""keyValuePairs"" argument does\nnot convert repeated keys into an array of values.  For example:\n\n{code}\n// specify three comma delimited values for the ""a"" parameters\nPageParameters parameters = new PageParameters(""a=1,a=2,a=3"");\nString[] a = parameters.getStringArray(""a"");\nassertEquals(3, a.length); // fails because a.length == 1\n{code}\n\nIssue first described on the user\'s list:\nhttp://www.nabble.com/PageParameters-with-String-array-question-to22540294.html'}"
wicket,bugs-dot-jar_WICKET-2181_d79d0192,"{'BugID': 'WICKET-2181', 'Summary': 'Bounds error in PageableListView#getCurrentPage()', 'Description': 'In the getCurrentPage() method of class PageableListView, the following code:\n\nwhile ((currentPage * rowsPerPage) > getList().size())\n{\n           currentPage--;\n}\n\nchecks if ""first cell if out of range"". However, the index of that first cell is (currentPage * rowsPerPage), and then the comparison with getList().size() should use a "">="" instead a "">"".'}"
wicket,bugs-dot-jar_WICKET-2202_24ac1a35,"{'BugID': 'WICKET-2202', 'Summary': ""Form gets submitted using AjaxSubmitBehavior when sub-form has error's"", 'Description': ""from http://www.nabble.com/Should-a-form-submit-when-sub-form-has-error%27s--tt22803314.html\n\nI have a main-form where I add a panel that contains another form.\nThis sub-form contains a formvalidator that gives the error.\nHowever the main-form is submitted, but the feedbackpanel does show the error message set in the sub-form's validator.\n\nI'll attach 2 patches with testcases displaying the behavior in wicket 1.3 vs 1.4\n\n(As a side note, I had to rename the org.apache.wicket.markup.html.form.validation.TestHomePage to org.apache.wicket.markup.html.form.validation.HomePageTest to get the test to run when building wicket)""}"
wicket,bugs-dot-jar_WICKET-2261_089303f4,"{'BugID': 'WICKET-2261', 'Summary': 'wicketTester.executeAjaxEvent(combo, ""onchange""); works with 1.4-rc1 but not anymore with 1.4-rc2', 'Description': 'Try the attached Unit Test.'}"
wicket,bugs-dot-jar_WICKET-2281_6e0b40bc,"{'BugID': 'WICKET-2281', 'Summary': 'MockHttpServletRequest is broken when used with CryptedUrlWebRequestCodingStrategy', 'Description': ""Upgraded to 1.3.6. One of my test cases started to fail with \norg.apache.wicket.WicketRuntimeException: Internal error parsing wicket:interface = ?x=GR7uTj8e-D8FE0tmM9vvYcwdiASd9OJ5GgveAhSNaig\n     \nI tracked down the issue to MockHttpServletRequest .setRequestToComponent()\nIn line 1253 it check for url starting with 6*. However, in CryptedUrlWebRequestCodingStrategy following encryption is employed:\n\n198:\t\t\t\t\tqueryString = shortenUrl(queryString).toString();\n199:\n200:\t\t\t\t\t// encrypt the query string\n201:\t\t\t\t\tString encryptedQueryString = urlCrypt.encryptUrlSafe(queryString);\n\n\nshortenUrl will replace 'wicket:interface=' with '6*' but then it gets immediately encrypted, consequently MockHttpServletRequest  will never recognize it correctly.\n\n""}"
wicket,bugs-dot-jar_WICKET-2334_96330447,"{'BugID': 'WICKET-2334', 'Summary': 'DebugBar throws an java.lang.ExceptionInInitializerError when Tomcat is restarted', 'Description': 'I have just added the DebugBar to our base page, and since then when Tomcat is restarted and session would be reloaded by this it throws this exception:\n\n1    ERROR org.apache.catalina.session.ManagerBase  - Exception loading sessions from persistent storage\njava.lang.ExceptionInInitializerError\n\tat sun.misc.Unsafe.ensureClassInitialized(Native Method)\n\tat sun.reflect.UnsafeFieldAccessorFactory.newFieldAccessor(UnsafeFieldAccessorFactory.java:25)\n\tat sun.reflect.ReflectionFactory.newFieldAccessor(ReflectionFactory.java:122)\n\tat java.lang.reflect.Field.acquireFieldAccessor(Field.java:918)\n\tat java.lang.reflect.Field.getFieldAccessor(Field.java:899)\n\tat java.lang.reflect.Field.getLong(Field.java:528)\n\tat java.io.ObjectStreamClass.getDeclaredSUID(ObjectStreamClass.java:1614)\n\tat java.io.ObjectStreamClass.access$700(ObjectStreamClass.java:52)\n\tat java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:425)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:413)\n\tat java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:310)\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:547)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1583)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1732)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480)\n\tat org.apache.wicket.Component.readObject(Component.java:4469)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:351)\n\tat java.util.concurrent.CopyOnWriteArrayList.readObject(CopyOnWriteArrayList.java:845)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1871)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1667)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1947)\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:480)\n\tat org.apache.wicket.Page.readPageObject(Page.java:1349)\n\tat org.apache.wicket.Component.readObject(Component.java:4465)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:351)\n\tat org.apache.wicket.protocol.http.SecondLevelCacheSessionStore$SecondLevelCachePageMap.readObject(SecondLevelCacheSessionStore.java:412)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:974)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1849)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1753)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:351)\n\tat org.apache.catalina.session.StandardSession.readObject(StandardSession.java:1407)\n\tat org.apache.catalina.session.StandardSession.readObjectData(StandardSession.java:931)\n\tat org.apache.catalina.session.StandardManager.doLoad(StandardManager.java:394)\n\tat org.apache.catalina.session.StandardManager.load(StandardManager.java:321)\n\tat org.apache.catalina.session.StandardManager.start(StandardManager.java:637)\n\tat org.apache.catalina.core.ContainerBase.setManager(ContainerBase.java:432)\n\tat org.apache.catalina.core.StandardContext.start(StandardContext.java:4160)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014)\n\tat org.apache.catalina.core.StandardHost.start(StandardHost.java:736)\n\tat org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1014)\n\tat org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)\n\tat org.apache.catalina.core.StandardService.start(StandardService.java:448)\n\tat org.apache.catalina.core.StandardServer.start(StandardServer.java:700)\n\tat org.apache.catalina.startup.Catalina.start(Catalina.java:552)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:295)\n\tat org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:433)\nCaused by: org.apache.wicket.WicketRuntimeException: There is no application attached to current thread main\n\tat org.apache.wicket.Application.get(Application.java:178)\n\tat org.apache.wicket.devutils.debugbar.DebugBar.getContributors(DebugBar.java:146)\n\tat org.apache.wicket.devutils.debugbar.DebugBar.registerContributor(DebugBar.java:140)\n\tat org.apache.wicket.devutils.debugbar.DebugBar.registerStandardContributors(DebugBar.java:152)\n\tat org.apache.wicket.devutils.debugbar.DebugBar.<clinit>(DebugBar.java:65)\n\t... 109 more'}"
wicket,bugs-dot-jar_WICKET-2337_36a41358,"{'BugID': 'WICKET-2337', 'Summary': 'IndexOutOfBoundsException when PropertyResolver is using an invalid list index', 'Description': 'When using  PropertyResolver.getValue(""myList[1]"", myBean),  the PropertyResolver$ListGetSet.getValue() (line 762) unconditionally does:\nreturn ((List)object).get(index);\nwhich throws an   java.lang.IndexOutOfBoundsException: Index: 1, Size: 1  if the backing list contains only one element (at index 0).\nShouldn\'t the implementation rather return null like with every other property not found? Like when using ""bla.bli.blo"" as a lookup string and there is no bla field and no getBla() method?\n\nSo this method should rather be:\n\norg.apache.wicket.util.lang.PropertyResolver$ListGetSet.getValue():\n\n\t\t/**\n\t\t * @see org.apache.wicket.util.lang.PropertyResolver.IGetAndSet#getValue(java.lang.Object)\n\t\t */\n\t\tpublic Object getValue(Object object)\n\t\t{\n\t\t\tList list = (List) object;\n\t\t\tif (index >= list.size()) {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\treturn list.get(index);\n\t\t}'}"
wicket,bugs-dot-jar_WICKET-2350_cd281092,"{'BugID': 'WICKET-2350', 'Summary': 'Localization messages stops working with validators since 1.4-rc2', 'Description': 'With the previous 1.3.6 and 1.4-rc1 releases I was capable to restrict a localization message for a validation to only one wicket id e.g. :\n\nin foobar.java\nRequiredTextField nameTF = new RequiredTextField(""name"");\nnameTF.add(StringValidator.lengthBetween(2, 255));\nnameTF.add(new PatternValidator(""[^|:]*""));\n\nand in foobar.properties\nname.Required=some text\nname.StringValidator.range=some other text\nname.PatternValidator=some other text again\n\nSo, like this I could have to create an another RequiredTextField named ""password"", and attach to it a different localization message (for example ""password.Required=blabla"").\n\nBut somehow with the 1.4-rc2-5 it looks like that this function is broken, it only recognizes the localization text, when I remove the ""name."" prefix from my property.'}"
wicket,bugs-dot-jar_WICKET-2368_fae1601b,"{'BugID': 'WICKET-2368', 'Summary': 'Page.checkRendering fails after setting BorderBodyContainer visiblity to false', 'Description': 'After toggling visibility of the BorderBodyContainer to false the Page.checkRendering method fails in line 1157, claiming an iterator IllegalStateException. This happens because iterator.remove() is called twice for a child component in the border component, if the body is not visible.\n\nMy Code:\n\npublic class TogglePanel extends Border {\n\tprivate boolean expanded = true;\n\n\tpublic TogglePanel(String id, IModel<String> titleModel) {\n\t\tsuper(id, titleModel);\n\n\t\tLink link = new Link(""title"") {\n\n\t\t\t@Override\n\t\t\tpublic void onClick() {\n\t\t\t\texpanded = !expanded;\n\t\t\t\tgetBodyContainer().setVisible(expanded);\n\t\t\t}\n\t\t};\n\t\tlink.add(new Label(""titleLabel"", titleModel));\n\n\t\tadd(link);\n\t}\n\n}\n\nMarkup:\n\n<wicket:border>\n\t<h3 class=""collapse"" wicket:id=""title"">\n\t\t<span class=""label"" wicket:id=""titleLabel"">Panel Title</span>\n\t\t<a class=""foldicon"">&nbsp;</a>\n\t</h3>\n\t<wicket:body />\n</wicket:border>\n\n'}"
wicket,bugs-dot-jar_WICKET-2506_0f8a2990,"{'BugID': 'WICKET-2506', 'Summary': 'Regression: ""Could not find child with id: <ID> in the wicket:enclosure"" for non-component tag', 'Description': 'Attached testcase passes with wicket-1.4.1 but fails with 1.4.2 saying:\n\norg.apache.wicket.WicketRuntimeException: Could not find child with id: radio in the wicket:enclosure\n\tat org.apache.wicket.markup.html.internal.Enclosure.checkChildComponent(Enclosure.java:210)\n\tat org.apache.wicket.markup.html.internal.Enclosure.ensureAllChildrenPresent(Enclosure.java:249)\n\tat org.apache.wicket.markup.html.internal.Enclosure.onComponentTagBody(Enclosure.java:169)\n\tat org.apache.wicket.Component.renderComponent(Component.java:2626)\n\tat org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512)\n\tat org.apache.wicket.Component.render(Component.java:2457)\n\tat org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:229)\n\tat org.apache.wicket.markup.resolver.EnclosureResolver.resolve(EnclosureResolver.java:61)\n\tat org.apache.wicket.markup.resolver.ComponentResolvers.resolve(ComponentResolvers.java:81)\n\tat org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1418)\n\tat org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1577)\n\tat org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1501)\n\tat org.apache.wicket.Component.renderComponent(Component.java:2626)\n\tat org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1512)\n\tat org.apache.wicket.Component.render(Component.java:2457)\n\tat org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1414)\n\tat org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1528)\n\tat org.apache.wicket.Page.onRender(Page.java:1545)\n\tat org.apache.wicket.Component.render(Component.java:2457)\n\tat org.apache.wicket.Page.renderPage(Page.java:914)\n\tat org.apache.wicket.request.target.component.BookmarkablePageRequestTarget.respond(BookmarkablePageRequestTarget.java:262)\n\tat org.apache.wicket.request.AbstractRequestCycleProcessor.respond(AbstractRequestCycleProcessor.java:105)\n\tat org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1258)\n\tat org.apache.wicket.RequestCycle.step(RequestCycle.java:1329)\n\tat org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428)\n\tat org.apache.wicket.RequestCycle.request(RequestCycle.java:594)\n\tat org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:478)\n\tat org.apache.wicket.protocol.http.MockWebApplication.processRequestCycle(MockWebApplication.java:390)\n\tat org.apache.wicket.util.tester.BaseWicketTester.startPage(BaseWicketTester.java:300)\n\tat org.apache.wicket.EnclosurePageTest.testRender(EnclosurePageTest.java:23)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:154)\n\tat junit.framework.TestCase.runBare(TestCase.java:127)\n\tat junit.framework.TestResult$1.protect(TestResult.java:106)\n\tat junit.framework.TestResult.runProtected(TestResult.java:124)\n\tat junit.framework.TestResult.run(TestResult.java:109)\n\tat junit.framework.TestCase.run(TestCase.java:118)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:208)\n\tat junit.framework.TestSuite.run(TestSuite.java:203)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.maven.surefire.junit.JUnitTestSet.execute(JUnitTestSet.java:213)\n\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140)\n\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:127)\n\tat org.apache.maven.surefire.Surefire.run(Surefire.java:177)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:345)\n\tat org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1009)'}"
wicket,bugs-dot-jar_WICKET-2552_12e1f39b,"{'BugID': 'WICKET-2552', 'Summary': 'CreditCardValidator accepts invalid inputs', 'Description': '(1) The onValidate() method of the CreditCardValidator class returns true for invalid inputs with null or unicode character such as 4\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0. \n(2) Also there is no length check on the input, therefore even invalid length inputs such as 9845 are accepted. \n(3) There is no check for invalid issuer identifier, i.e.,  840898920205250 is accepted, where 84XXXX is not a valid issuer identifier'}"
wicket,bugs-dot-jar_WICKET-2569_9ced53a5,"{'BugID': 'WICKET-2569', 'Summary': 'Inheritance layout excludes XML header from output', 'Description': 'When using inheritance layout, if the superclass (Layout class) has an ?xml header at the top, it\'s excluded from the rendering of subclasses, if they have an associated html file. If the subclass has no .html file associated with it, the ?xml header is preserved in the rendering output.\n\nTo reproduce: Create a SuperPage class extending WebPage. At the top of SuperPage.html, put ""<?xml version=""1.0"" encoding=""utf-8""?>"" . Create two subclasses of SuperPage, one with an HTML file and one without. View the sub pages. Notice when the one with an HTML file is rendered, the xml header is excluded.\n\nExpected: The ?xml header should always be preserved in the rendered output as it\'s vital to the layout.'}"
wicket,bugs-dot-jar_WICKET-2609_7da4ad17,"{'BugID': 'WICKET-2609', 'Summary': 'EnumChoiceRenderer misbehaves with anonymous enum classes', 'Description': 'Please find attached testcase reproducing the problem.\n\nProper fix is to do\nreturn object.getDeclaringClass().getSimpleName() + ""."" + object.name()\n\ninstead of\nreturn object.getClass().getSimpleName() + ""."" + object.name()\n\nin EnumChoiceRenderer.resourceKey'}"
wicket,bugs-dot-jar_WICKET-2621_c849f986,"{'BugID': 'WICKET-2621', 'Summary': ""Ajax buttons inside ModalWindows don't submit properly"", 'Description': 'I have a ModalWindow that contains an IndicatingAjaxButton. When I click the button, I get a big Java error complaining that the form submit wasn\'t multipart.\n\nDigging into the javascript in wicket-ajax.js, I found this from line 1102 in the method handleMultipart\n\n{code}\nmultipart=multipart||form.enctype==""multipart/form-data"";\n\nif (multipart==false) {\n     // nothing to handle\n    return false;\n }\n{code}\n\nWhen this executed, multipart was false, and enctype was """" and therefore the submit aborted. This may be the cause.\n\nHere\'s the Java stacktrace\n\n{noformat}\njava.lang.IllegalStateException: ServletRequest does not contain multipart content\n\tat org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:113)\n\tat org.apache.wicket.protocol.http.servlet.MultipartServletWebRequest.<init>(MultipartServletWebRequest.java:83)\n\tat org.apache.wicket.extensions.ajax.markup.html.form.upload.MultipartRequest.<init>(MultipartRequest.java:41)\n\tat org.apache.wicket.extensions.ajax.markup.html.form.upload.UploadWebRequest.newMultipartWebRequest(UploadWebRequest.java:66)\n\tat org.apache.wicket.markup.html.form.Form.handleMultiPart(Form.java:1651)\n\tat org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:850)\n\tat org.apache.wicket.ajax.form.AjaxFormSubmitBehavior.onEvent(AjaxFormSubmitBehavior.java:135)\n\tat org.apache.wicket.ajax.AjaxEventBehavior.respond(AjaxEventBehavior.java:177)\n\tat org.apache.wicket.ajax.AbstractDefaultAjaxBehavior.onRequest(AbstractDefaultAjaxBehavior.java:299)\n\tat org.apache.wicket.request.target.component.listener.BehaviorRequestTarget.processEvents(BehaviorRequestTarget.java:119)\n\tat org.apache.wicket.request.AbstractRequestCycleProcessor.processEvents(AbstractRequestCycleProcessor.java:92)\n\tat org.apache.wicket.RequestCycle.processEventsAndRespond(RequestCycle.java:1250)\n\tat org.apache.wicket.RequestCycle.step(RequestCycle.java:1329)\n\tat org.apache.wicket.RequestCycle.steps(RequestCycle.java:1428)\n\tat org.apache.wicket.RequestCycle.request(RequestCycle.java:545)\n\tat org.apache.wicket.protocol.http.WicketFilter.doGet(WicketFilter.java:479)\n\tat org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:312)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n{noformat}'}"
wicket,bugs-dot-jar_WICKET-2624_ef880545,"{'BugID': 'WICKET-2624', 'Summary': ""MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter"", 'Description': ""MethodGetAndSet.setValue uses wrong source to determine which type to convert to when there's no setter, resulting in exceptions like this:\norg.apache.wicket.WicketRuntimeException: Error setting field: private int PropertyResolverTest$DirectFieldSetWithDifferentTypeThanGetter.value on object: PropertyResolverTest$DirectFieldSetWithDifferentTypeThanGetter@396477d9\n\tat org.apache.wicket.util.lang.PropertyResolver$MethodGetAndSet.setValue(PropertyResolver.java:1150)\n\tat org.apache.wicket.util.lang.PropertyResolver$ObjectAndGetSetter.setValue(PropertyResolver.java:588)\n\tat org.apache.wicket.util.lang.PropertyResolver.setValue(PropertyResolver.java:136)\n\tat PropertyResolverTest.testDirectFieldSetWithDifferentTypeThanGetter(PropertyResolverTest.java:12)\n\nBug is located in:\nconverted = converter.convert(value, getMethod.getReturnType());\n\nInstead, it should read:\nconverted = converter.convert(value, type);\n\nTestcase attached.\n\nAdditional thoughts:\nif (setMethod != null)\n{\n  type = getMethod.getReturnType();\n}\nThis is really confusing (we check setMethod presence but get type from getMethod). Luckily, this works as expected because in MethodGetAndSet.findSetter only methods with same (or superclass) type as getter are returned.\n""}"
wicket,bugs-dot-jar_WICKET-2839_15477252,"{'BugID': 'WICKET-2839', 'Summary': 'ajax not working due to bugs in resource handling', 'Description': 'A couple of bugs were found that were preventing .js resources to be returned to the client correctly. One bug was returning the jar file size as the content length of the resource if it is in a jar file. The other was copying past a source buffer into the response.\n\nAfter fixing these bugs, the ajax functions in the trunk seems to be working.\n\nA patch is provided. Test cases included.'}"
wicket,bugs-dot-jar_WICKET-2882_ebe56869,"{'BugID': 'WICKET-2882', 'Summary': 'Dynamically adding component via an IComponentResolver fails within an enclosure for versions after 1.4.1', 'Description': ""We have been using an IComponentResolver implementation for a long time to allow the inclusion of certain panels to be determined by the markup. Some panels are included inside enclosures and some are not. Both cases worked fine in wicket 1.4.1 but in versions 1.4.2 and later a 'Tag expected' error occurs if the component is wrapped inside a wicket enclosure.\n\nA quickstart example has been included to demonstrate the problem.""}"
wicket,bugs-dot-jar_WICKET-2900_0e70ce39,"{'BugID': 'WICKET-2900', 'Summary': '""isPrimary"" check is not applied to beans in parent contexts', 'Description': 'see this comment in WICKET-2771: https://issues.apache.org/jira/browse/WICKET-2771?focusedCommentId=12872246&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12872246'}"
wicket,bugs-dot-jar_WICKET-294_5c592d85,"{'BugID': 'WICKET-294', 'Summary': 'WebRequestCodingStrategy: path mounting and matching', 'Description': 'Assuming a mount path to ""/p"", it will match /pxyz\n\nAssuming this is the desired behavior of matching (warning), then to avoid this match it should be declared ""/p/"" but it will create urls such as \'/app/p//SomePage\'. which is wrong.\n\nIn the servlet specs,  the mapping syntax \'/p\' is an exact match, this is not what you want in your case since you\'re doing path mapping, so the syntax if you want to stick close to the servlet specs should be \'/p/*\' or if you wan to get close to mod_proxy syntax it would be \'/p/\'\n\nNote that the examples are also using this wrong mapping declaration. In the example below: both should throw a 404:\nhttp://www.wicket-library.com/wicket-examples/niceurl/my/mounted/packageXXX\nhttp://www.wicket-library.com/wicket-examples/niceurl/my/mounted/Xpackage\n\n'}"
wicket,bugs-dot-jar_WICKET-2961_3d8c9d75,"{'BugID': 'WICKET-2961', 'Summary': 'Adding a component in Component#onInitialize() leads to StackOverflowError', 'Description': 'Adding a component in Page#onInitialize() leads to StackOverflowError:\n\n at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)\n     at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)\n     at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)\n     at org.apache.wicket.Component.initialize(Component.java:970)\n     at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)\n     at org.apache.wicket.Page.componentAdded(Page.java:1130)\n     at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)\n     at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)\n     at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)\n     at org.apache.wicket.Component.initialize(Component.java:970)\n     at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)\n     at org.apache.wicket.Page.componentAdded(Page.java:1130)\n     at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)\n     at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)\n     at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)\n     at org.apache.wicket.Component.initialize(Component.java:970)\n     at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)\n     at org.apache.wicket.Page.componentAdded(Page.java:1130)\n     at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)\n     at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)\n     at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)\n     at org.apache.wicket.Component.initialize(Component.java:970)\n     at org.apache.wicket.MarkupContainer.initialize(MarkupContainer.java:992)\n     at org.apache.wicket.Page.componentAdded(Page.java:1130)\n     at org.apache.wicket.MarkupContainer.addedComponent(MarkupContainer.java:978)\n     at org.apache.wicket.MarkupContainer.add(MarkupContainer.java:168)\n     at org.apache.wicket.examples.WicketExamplePage.onInitialize(WicketExamplePage.java:67)\n     at org.apache.wicket.Component.initialize(Component.java:970)\n...'}"
wicket,bugs-dot-jar_WICKET-2993_0b4f78cc,"{'BugID': 'WICKET-2993', 'Summary': 'ClassCastException when requesting for non-page class ', 'Description': 'org.apache.wicket.request.mapper.BookmarkableMapper tries to instantiate Page even for classes which are not Page.\nRequesting http://localhost:8080/wicket/bookmarkable/com.mycompany.Pojo fails with:\n\nERROR - DefaultExceptionMapper     - Unexpected error occurred\njava.lang.ClassCastException: com.mycompany.Pojo\n\tat org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:155)\n\tat org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:59)\n\tat org.apache.wicket.session.DefaultPageFactory.newPage(DefaultPageFactory.java:43)\n\tat org.apache.wicket.Application$2.newPageInstance(Application.java:1425)\n\tat org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:259)\n\tat org.apache.wicket.request.handler.PageProvider.getPageInstance(PageProvider.java:160)\n\tat org.apache.wicket.request.handler.render.WebPageRenderer.getPage(WebPageRenderer.java:59)\n\tat org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:131)\n\tat org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:232)\n\tat org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:147)\n\tat org.apache.wicket.request.RequestHandlerStack.executeRequestHandler(RequestHandlerStack.java:84)\n\tat org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:217)\n\tat org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253)\n\tat org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:135)\n\tat org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:188)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157) \n        .....'}"
wicket,bugs-dot-jar_WICKET-3065_b293b75c,"{'BugID': 'WICKET-3065', 'Summary': ""HomePageMapper ignores request to '/' with query string parameters"", 'Description': 'Issue a request to http://host:port/contextpath/?something\nWicket will log an error message like:\nERROR - RequestCycle               - Unable to execute request. No suitable RequestHandler found. URL=?something\n\nI think the reason is in HomePageMapper which maps to the configured home page only if there are no query parameters.\n\nHomePageMapper.java:\n{code}\npublic IRequestHandler mapRequest(Request request)\n\t{\n\t\tif (request.getUrl().getSegments().size() == 0 &&\n\t\t\trequest.getUrl().getQueryParameters().size() == 0)\n\t\t{\n\t\t\treturn new RenderPageRequestHandler(new PageProvider(getContext().getHomePageClass()));\n\t\t}\n\t\telse\n\t\t{\n\t\t\treturn null;\n\t\t}\n\t}\n{code}'}"
wicket,bugs-dot-jar_WICKET-3076_d3dc9a50,"{'BugID': 'WICKET-3076', 'Summary': 'UrlUtils.isRelative returns false if URL parameter contains an absolute URL', 'Description': 'I have a page that gets a return path for a back link as a parameter. A link to this page looks like this:\n\n./mypage?return=http://example.com\n\nIn WebRequestCodingStrategy.encode, this URL is returned by pathForTarget.\nThen it is checked whether this URL is relative using UrlUtils.isRelative. The URL is apparently relative, but UrlUtils.isRelative returns false, since the check contains:\n\n(url.indexOf(""://"") < 0\n\nthis is false for the above example. Thus, an incorrect path is returned by WebRequestCodingStrategy.encode (relative path resolution does not take place).\n\nA fix for the problem would be to check for \n\n!(url.startsWith(""http://"") || url.startsWith(""https://""))\n\nOr, if other protocols should also be supported, a regular expression like ""^[^/?]*://"" should work. \n'}"
wicket,bugs-dot-jar_WICKET-3098_1b7afefc,"{'BugID': 'WICKET-3098', 'Summary': 'AjaxEventBehavior#onEvent is invoked on disabled behavior', 'Description': 'Security bug  AjaxEventBehavior#onEvent is invoked on disabled behavior. It should not be - it is really dangerous, can you fix it.\n\nI think it is security bug.'}"
wicket,bugs-dot-jar_WICKET-3166_4d7f7359,"{'BugID': 'WICKET-3166', 'Summary': 'isVisibleInHierarchy() possibly unnecessarily checks children whose parents are invisible?', 'Description': 'Hi!\n\nSee attached quickstart with junit test reproducing the bug. See also patch proposal.\n\nI have a page with two panels:\n\npage.form.add(panel1);\npage.form.add(panel2);\n\nin some situations panel1 is not visible.\n\nHowever, a form submit event will visit all formcomponents of panel1 via:\n\n       at org.apache.wicket.markup.html.form.FormComponent.visitFormComponentsPostOrder(FormComponent.java:400)\n       at org.apache.wicket.markup.html.form.Form.visitFormComponentsPostOrder(Form.java:1209)\n       at org.apache.wicket.markup.html.form.Form.inputChanged(Form.java:1403)\n       at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:865)\n\nThis results in a crash because panel1 components are not prepared to\nbe invoked via isvisible when the panel itself is not visible.\n\nI wonder if the component.isVisibleInHierarchy could be changed as\nfollows, to first check parent visibility:\n\n public final boolean isVisibleInHierarchy()\n {\n   Component component = this;\n   while (component != null)\n   {\n     Component componentParent = component.getParent();\n\n     if (((componentParent == null) ||\ncomponentParent.isVisibleInHierarchy()) &&\ncomponent.determineVisibility())\n     {\n       component = componentParent;\n     }\n     else\n     {\n       return false;\n     }\n   }\n   return true;\n }\n\nSimilar change could/should maybe be possible also for isEnabledInHierarchy ?'}"
wicket,bugs-dot-jar_WICKET-3174_0cf14725,"{'BugID': 'WICKET-3174', 'Summary': 'SmartLinkLabel failing to process email with +', 'Description': 'Using SmartLinkLabel with an email address that includes a ""+"" generates a link only on the right-most part of the address.\n\nExample:\n- my+test@example.com\nWill generate a link like:\n- my+<a href=""mailto:test@example.com"">test@example.com@pappin.ca</a>\n\nTHe addition of the ""+"" char is a valid email address format.\n\n'}"
wicket,bugs-dot-jar_WICKET-3196_f1c0f263,"{'BugID': 'WICKET-3196', 'Summary': 'UrlValidator failes to validate urls that containt multiple dots in path', 'Description': 'refer to UrlValidator.java:466 (isValidPath).\nif we have an url, that contains more than two consequent dots, for example ""http://www.somedomain.com/this_one_is_tricky...but...still.....valid"", validator will fail.\nbtw, the other side effect is that countTokens actually counts \'...\' a two 2dots.\nOne possible workaround is not just count \'..\' tokens, but count them along with slash, like \'../\'.\n\n'}"
wicket,bugs-dot-jar_WICKET-3197_be70e608,"{'BugID': 'WICKET-3197', 'Summary': ""getMarkupId() can be used only if the component's markup is attached"", 'Description': ""With change r1037139 Component#getMarkupImpl() first tries to get the markup id from the component's markup.\nIf the markup is not available/attached yet for this component the call ends with :\norg.apache.wicket.markup.MarkupException: Can not determine Markup. Component is not yet connected to a parent. [Component id = label]\n""}"
wicket,bugs-dot-jar_WICKET-3222_5729ed90,"{'BugID': 'WICKET-3222', 'Summary': ""AbstractMarkupParser doesn't remove Comments correctly"", 'Description': ""AbstractMarkupParser removeComment(...) doesn't remove Comments correctly\n\nif two html comments stand to close together <!-- foo --> <!-- bar -->\nfoo will be removed but not bar.\n\nsee:\n\nhttps://github.com/mafulafunk/wicketComments\n\ngit@github.com:mafulafunk/wicketComments.git""}"
wicket,bugs-dot-jar_WICKET-3253_71b6e905,"{'BugID': 'WICKET-3253', 'Summary': 'NPE with nested property models', 'Description': 'After updated from 1.4.8 to 1.4.14 I got this bug.\n\nThe problem is with nested property models where the ""top"" model has a null model object that is bound to a TextField. You get a NPE when the page is rendered. There is a quick workaround by overriding getOjbectClass() on the property model.\n\nI provide a running example of the problem.'}"
wicket,bugs-dot-jar_WICKET-3272_c86b972a,"{'BugID': 'WICKET-3272', 'Summary': 'Set an request parameter on Wicket tester do not add it in the request URL', 'Description': 'When submitting an form, the parameters set in request do not get appended to the URL query string.\nInitial impression is that UrlRender should append query parameters in the base URL on relatives URL.'}"
wicket,bugs-dot-jar_WICKET-3278_60d07288,"{'BugID': 'WICKET-3278', 'Summary': 'DropDownChoice no selection value', 'Description': 'This problem came from this topic:\nhttp://apache-wicket.1842946.n4.nabble.com/DropDownChoice-no-selection-value-td3160661.html\n\nI\'ve noticed that the method AbstractSingleSelectChoice.getNoSelectionValue() returns the value for no selection. In AbstractSingleSelectChoice.getDefaultChoice(final Object selected) on line 314: \n      return ""\\n<option selected=\\""selected\\"" value=\\""\\"">"" + option + ""</option>""; \n\nand on line 296: \n      buffer.append("" value=\\""\\"">"").append(option).append(""</option>""); \n\nIn those cases the null value option has empty value attribute. Wouldn\'t it be more consistent for this option to have the ""value"" attribute with the result provided from getNoSelectionValue()?'}"
wicket,bugs-dot-jar_WICKET-3280_295e73bd,"{'BugID': 'WICKET-3280', 'Summary': ""IResponseFilter doesn't work in 1.5"", 'Description': 'In current 1.5-SNAPSHOT there are no callers of org.apache.wicket.settings.IRequestCycleSettings.getResponseFilters() and thus filters are never executed.'}"
wicket,bugs-dot-jar_WICKET-3297_71499e17,"{'BugID': 'WICKET-3297', 'Summary': 'UrlAttributes are encoded incorrectly when style is null but variation is not', 'Description': 'AbstractResourceReferenceMapper.encodeResourceReferenceAttributes() method generates the same ""-foo"" output for these two different inputs: {locale = null, style = ""foo"", variation = null} and {locale = null, style = null, variation = ""foo""}.\nFor the second input it should generate ""--foo"" (double dash prefix).'}"
wicket,bugs-dot-jar_WICKET-3304_7e7ab76c,"{'BugID': 'WICKET-3304', 'Summary': 'TextField ingnores convertEmptyInputStringToNull = true property when the String type is set', 'Description': 'I posted this patch on WICKET-3269, but the discussion on this ticket is about an improvement request, not a bug. I opened this one for the bug.'}"
wicket,bugs-dot-jar_WICKET-3309_debca73b,"{'BugID': 'WICKET-3309', 'Summary': 'unable to add nodes to an empty rootless Tree (e.g. LinkTree)', 'Description': '2 scenarios which adding new nodes (via ajax) to a rootless Tree is not working as expected.\nthe node is getting added to the treemodel but non is displayed.\n\n1) adding a node to the rootnode. the newly added node is not displayed.\n2) the rootless tree already has a node. if you add additional nodes to the root node, they will be displayed (compare to 1), if you add an additional node to one of the added nodes, the complete tree will disappear.\n\nsee attached quickstart'}"
wicket,bugs-dot-jar_WICKET-3333_ddf7e8a2,"{'BugID': 'WICKET-3333', 'Summary': 'Links with multiple parameters are wrongly generated', 'Description': 'If you have a PageParameters, with multiple params, then the resulting link will be something like this /url?id=123&amp;sid=456, so for some reason the & sign is encoded to &amp; which will result in the following parameters on the receiving page:\nid=[123], amp;sid=[456]\nSee the attached quickstart for example.'}"
wicket,bugs-dot-jar_WICKET-3413_499a9c6b,"{'BugID': 'WICKET-3413', 'Summary': 'FLAG_INHERITABLE_MODEL and default model change', 'Description': 'The issue is about correctness of Component#setDefaultModel (Component#setModelImpl) method behavior. I expect that the flag FLAG_INHERITABLE_MODEL should be checked there and turned off in case if new model is not a IComponentInheritedModel. \n\nLet check the next code:\npublic MyPanel(String id) {\n super(id);\n  ...\n  form.setModel(new CompoundPropertyModel(this));\n  DropDownChoice ddc = new DropDownChoice(""variant"", Arrays.ofList(...)) {    // p1\n    @Override\n    protected void onInitialize() {\n       super.onInitialize();\n       setModel(new DefaultingWrapModel(getModel(), Model.of(""default value""));            // p2\n    }\n  };\n  ddc.setNullValid(false);\n  ddc.setRequired(true);\n  form.add(ddc);\n  ...\n}\n\nIn the (p1) the DDC will initialize with CompoundPropertyModel and the FLAG_INHERITABLE_MODEL will be turned on soon by the first invocation of FormComponent#getModel().\n\n In the (p2) we wrap the DDC model with the model which provide the default value (DefaultingWrapModel implements IWrapModel). So we change the model, but the FLAG_INHERITABLE_MODEL is still turned on. On the Component#detach() event, the method Component#setModelImpl(null) will be invoked for the ddc and the DefaultingWrapModel instance will be lost:\n\n\t\t// reset the model to null when the current model is a IWrapModel and\n\t\t// the model that created it/wrapped in it is a IComponentInheritedModel\n\t\t// The model will be created next time.\n\t\tif (getFlag(FLAG_INHERITABLE_MODEL))\n\t\t{\n\t\t\tsetModelImpl(null);\n\t\t\tsetFlag(FLAG_INHERITABLE_MODEL, false);\n\t\t}\n\nI think that such behavior is unexpected.\n\nhttp://apache-wicket.1842946.n4.nabble.com/1-4-15-FLAG-INHERITABLE-MODEL-and-default-model-change-td3252093.html'}"
wicket,bugs-dot-jar_WICKET-3420_be97d017,"{'BugID': 'WICKET-3420', 'Summary': 'javascript with a less than character (""<"") fails to execute when added through a header contribution in ajax response', 'Description': 'This is adapted from a wicket users post I made (links are to the same thread in two archive systems):\n\nhttp://markmail.org/search/?q=wicket%20users%20wicket-ajax.js#query:wicket%20users%20wicket-ajax.js+page:1+mid:rfts3ar3upffhbbt+state:results\n\nhttp://mail-archives.apache.org/mod_mbox/wicket-users/201102.mbox/%3CAANLkTi=EkmTA0RnA+GyJE-CQWmkCxRLsjp+z8jwv-Aw9@mail.gmail.com%3E\n\nThe problem:  I have a panel with this:\n\n    <wicket:head>\n\t<script>\n\t\tif (someVariable < 0) {\n\t\t\tsomeVariable = 0;\t\t\n\t\t}\n\t</script>\n    </wicket:head>\n\nThis script fails to execute when the panel is loaded by ajax.  If I replace the less than character ""<"" with equals ""=="", then it executes (but of course, this is not what I need).\n\nI tested this in Firefox 4.0b10 and Chrome 8.\n\nAfter some debugging, it seems to me that this needs to be corrected in wicket-ajax.js. The header contribution is sent to the browser inside of a CDATA section so the ""<"" character arrives to javascript intact. However, in parsing the script tag, the ""<"" seems to signal the beginning of an HTML tag that then is considered malformed.\n\n\nPossible workarounds for apps:\n\n - Invert the logic so a greater-than is used. In my example, this would be: ""if (0 > someVariable) {""\n - Put the code into a separate JS file (the downside is it requires another network hop from the browser)\n - Embed the script in <wicket:panel> rather than <wicket:head> (the disadvantage is the script will be re-sent with the panel content when the panel is re-used on the same page)\n\n'}"
wicket,bugs-dot-jar_WICKET-3428_ffc0cae9,"{'BugID': 'WICKET-3428', 'Summary': 'IRequestCycleListener: RequestCycle.get() is null inside onBeginRequest', 'Description': 'I expect the request cycle that is supplied as an argument to onBeginRequest to be the same as RequestCycle.get.\n\n== == == CODE == == ==\n\n    @Override\n    public void onBeginRequest(RequestCycle cycle) {\n        Session session = Session.get(); // throws IllegalArgumentException\n        if (session.getMetaData(REDIRECTED_JSESSIONID) == null) {\n            logger.debug(""first application request - redirecting to loading page"");\n            session.setMetaData(REDIRECTED_JSESSIONID, Boolean.TRUE);\n            String url = getServletRequestContextPath() + ""/"" + cycle.getRequest().getUrl();\n            throw new RestartResponseException(newLoadingPage(url));\n        }\n    }\n== == == == == == == ==\n\n\n== == == STACK TRACE == == ==\n\njava.lang.IllegalArgumentException: Argument \'requestCycle\' may not be null.\n    at org.apache.wicket.util.lang.Args.notNull(Args.java:37)\n    at org.apache.wicket.Application.fetchCreateAndSetSession(Application.java:1436)\n    at org.apache.wicket.Session.get(Session.java:154)\n    at com.joynit.tuv.common.view.request.SessionIdRemoveListener.onBeginRequest(SessionIdRemoveListener.java:30)\n\n... [snipped -other part is not relevant]\n\n== == == == == == == == == == =='}"
wicket,bugs-dot-jar_WICKET-3454_f1e854b3,"{'BugID': 'WICKET-3454', 'Summary': 'Value exchange in a wicket:message throws an exception', 'Description': ""i tried to exchange values in a <wicket:message> like described in wiki <https://cwiki.apache.org/WICKET/wickets-xhtml-tags.html#Wicket%27sXHTMLtags-Elementwicket:message>.\nBut i get an exception:\nERROR - RequestCycle               - No get method defined for class:\nclass org.apache.wicket.markup.resolver.MarkupInheritanceResolver$TransparentWebMarkupContainer expression: vat1value\norg.apache.wicket.WicketRuntimeException: No get method defined for class:\nclass org.apache.wicket.markup.resolver.MarkupInheritanceResolver$TransparentWebMarkupContainer expression: vat1value\nat org.apache.wicket.util.lang.PropertyResolver.getGetAndSetter(PropertyResolver.java:488)\nat org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:330)\nat org.apache.wicket.util.lang.PropertyResolver.getObjectAndGetSetter(PropertyResolver.java:237)\n...\n\nMaybe it's caused by usage of border. I've debugged a bit, but could get a real glue.\n\nI added a quick start with test case.""}"
wicket,bugs-dot-jar_WICKET-3455_f30bd1cb,"{'BugID': 'WICKET-3455', 'Summary': 'onremove() in RefreshingView.onPopulate', 'Description': ""file a bug with a quickstart. onremove() should be called on all\nremoved components.\n\n-igor\n\nOn Fri, Feb 18, 2011 at 5:38 AM, Benedikt Rothe <benedikt.rothe@qleo.de> wrote:\n> > Hi\n> >\n> > Are the existing children of a RepeatingView/RefreshingView being informed,\n> > when\n> > the View is newly populated (RefreshingView.onPopulate).\n> >\n> > I'd like to clean some internal references in this case.\n> > I tried:\n> > - aChild.onRemove is not called in this situation\n> > - aChild.setParent(null) is called. I treid to override setParent it. But\n> > setParent is private.\n> >\n> > Any suggestions?\n> > Benedikt\n""}"
wicket,bugs-dot-jar_WICKET-3510_292a2582,"{'BugID': 'WICKET-3510', 'Summary': ""DateTimeField improperly converts time causing wrong dates when the server's current date is different from the client's date."", 'Description': 'The bug is in DateTimeField#convertInput().\n<code>\n// Get year, month and day ignoring any timezone of the Date object\nCalendar cal = Calendar.getInstance();\ncal.setTime(dateFieldInput);\nint year = cal.get(Calendar.YEAR);\nint month = cal.get(Calendar.MONTH) + 1;\nint day = cal.get(Calendar.DAY_OF_MONTH);\nint hours = (hoursInput == null ? 0 : hoursInput % 24);\nint minutes = (minutesInput == null ? 0 : minutesInput);\n\n// Use the input to create a date object with proper timezone\nMutableDateTime date = new MutableDateTime(year, month, day, hours, minutes, 0, 0,\n\tDateTimeZone.forTimeZone(getClientTimeZone()));\n</code>\nIf the server\'s current date is different from the client\'s, this produces wrong output. I attached a patch with a test case that simulates this condition.\n\nI don\'t know why this ""casting"" of day, month, year is done.\n'}"
wicket,bugs-dot-jar_WICKET-3511_4a875f46,"{'BugID': 'WICKET-3511', 'Summary': 'Mapping ResourceReferences to Urls is slow', 'Description': 'PackageResourceReference is often used for stylesheets and JavaScript resources, many of which can appear on a typical page (WicketAjaxReference is one common example). Every time the page is rendered, these resources are mapped to urls in order to build the appropriate <link href=""...""> or <script src=""...""> tags.\n\nThe trouble is that this mapping process is extremely inefficient. To map a ResourceReference to a url, ResourceReference#getLastModified() must be consulted for FilenameWithTimestampResourceCachingStrategy, and ResourceReference#getUrlAttributes() is called to append appropriate query parameters.\n\nIn PackageResourceReference, both of these methods delegate to the very expensive PackageResourceReference#lookupStream(), which makes several attempts to locate the underlying file or classpath item using various permutations of locale, style, and variation. Each of these attempts involves I/O. The default ResourceStreamLocator, which does the actual file and classpath queries, does no caching whatsoever.\n\nOn a trivial Wicket page containing 7 total PackageResourceReferences for images, stylesheets and JavaScript files, the average response time in my tests was 211 ms. The vast majority of that time was spent in ResourceStreamLocator, due to the expensive steps described above.\n\nIt seems that putting caching at the ResourceStreamLocator would be extremely beneficial. I am attaching a simple implementation. With caching enabled in ResourceStreamLocator, the response time of my test page dropped from 211 ms to 49 ms.\n'}"
wicket,bugs-dot-jar_WICKET-3514_2b6da516,"{'BugID': 'WICKET-3514', 'Summary': 'SimpleTree example not working with CryptoMapper', 'Description': 'Adding the following lines to WicketExampleApplication.java causes the SimpleTree example to break. There are no expand icons anymore and there is no way to expand the tree. Even the expand link will not work.\n\nAdd to WicketExampleApplication.java \n\nIRequestMapper cryptoMapper = new CryptoMapper(getRootRequestMapper(), this);\nsetRootRequestMapper(cryptoMapper);\n\n\nComment out in WicketExampleApplication.java \n\n//getSecuritySettings().setCryptFactory(new ClassCryptFactory(NoCrypt.class, ISecuritySettings.DEFAULT_ENCRYPTION_KEY));\n\nWithout the CryptoMapper everythings works fine.\n\n\n'}"
wicket,bugs-dot-jar_WICKET-3520_d1b62639,"{'BugID': 'WICKET-3520', 'Summary': 'SHOW_NO_EXCEPTION_PAGE responding with HTTP status 500 is overwritten by redirect', 'Description': 'If the application is configured with SHOW_NO_EXCEPTION_PAGE as unexpectedExceptionDisplay, an exception thrown while submitting a form should result in an HTTP 500 status.\nSince the request is already marked as a redirect in AbstractListenerInterfaceRequestTarget#onProcessEvents(), the 500 status is overwritten with status 200 when the redirect is handled afterwards.'}"
wicket,bugs-dot-jar_WICKET-3539_a4459ef4,"{'BugID': 'WICKET-3539', 'Summary': 'Event broadcast type ""Depth"" does not work when the sink is a Component but not a MarkupContainer', 'Description': 'Event broadcast type ""Depth"" does not work when the sink is a Component but not a MarkupContainer. In this case, no sinks receive the event.'}"
wicket,bugs-dot-jar_WICKET-3563_c62b66c1,"{'BugID': 'WICKET-3563', 'Summary': 'Interaction betwen IAjaxRegionMarkupIdProvider, renderPlaceholderTag and visibility', 'Description': ""I've just discovered what I think is a bug with\nIAjaxRegionMarkupIdProvider. We are using it on a Behavior that provides\na border to form components (label, mandatory marker, etc), which for\nthe most part works great.\n\nWe have encountered a problem when toggling the visibility of a form\ncomponent with this behavior via ajax. \n\nThe component is first sent out visible and the markup is all correct.\n\nA change elsewhere on the page causes the component to be set to not\nvisible and redrawn via ajax. The ajax response contains a tag with a\nmarkupid generated via renderPlaceholderTag. This does not take into\naccount the  IAjaxRegionMarkupIdProvider behaviour.\n\nAnother change happens on the page causing the component to become\nvisible, and the ajax replace can't happen because the component with\nthe correct markupId is not present.\n""}"
wicket,bugs-dot-jar_WICKET-3597_5e2c6702,"{'BugID': 'WICKET-3597', 'Summary': 'Wicket 1.5 RC-3 Bug with conditional comments', 'Description': 'IE Conditional Comments with script block causes malformed HTML on Chrome and Firefox.'}"
wicket,bugs-dot-jar_WICKET-3598_7c364566,"{'BugID': 'WICKET-3598', 'Summary': 'DatePicker issues with locale medium date format', 'Description': 'DateTextField as follows: DateTextField d = new DateTextField(id, model, new StyleDateConverter(""M-"",false));\n\nCase 1:\n- en-US locale\n- for example DatePicker insert 04 rather than Apr in DateTextField, even though pattern clearly says MMM\n\nCase 2:\n- pl-PL locale\n- 2010-10-25 is in the DateTextField\n- DatePicker opens on April 2031 rather than October 2010\n\nI believe the problem lies in wicket-date.js, in functions substituteDate and parseDate.\n\nI know this might be duplicate of WICKET-2427 and WICKET-2375, but apparently this hasn\'t been properly fixed yet.'}"
wicket,bugs-dot-jar_WICKET-3603_aa1d177a,"{'BugID': 'WICKET-3603', 'Summary': 'DataTable row groups are present in markup even when they contain no rows.', 'Description': 'As per the HTML spec :\n""When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element.""\n\nThere is no check in place to remove the row group tags from the output if they don\'t contain any row.'}"
wicket,bugs-dot-jar_WICKET-3617_7ae109a6,"{'BugID': 'WICKET-3617', 'Summary': 'Using render strategy ONE_PASS_RENDER fails for Ajax requests', 'Description': 'I have an application which has two pages. Page A has an AjaxLink which makes some checks and either sets some error feedback and stays on the same page (e.g. login page with ""Invalid user"" error) or if everything is OK then redirects to page B (via setResponsePage(B.class)).\nThe problem comes when the current render strategy is ONE_PASS_RENDER. In this case no matter that fromUrl and toUrl are different and the request is Ajax the current code directly writes the page markup to the response.\nI think it should trigger a redirect instead.\nI am not sure whether it should be redirect to render or to buffer ...'}"
wicket,bugs-dot-jar_WICKET-3618_fbfd17e6,"{'BugID': 'WICKET-3618', 'Summary': 'IllegalStateException: Header was already written to response!', 'Description': ""Getting this error for no apparent reason, the code works fine on wicket 1.4.17. Code example with error is attached.\n\nClick on the 'click here' link to see the error occur in the console, below is part of the stack trace.\n\n\nERROR - DefaultExceptionMapper     - Unexpected error occurred\njava.lang.IllegalStateException: Header was already written to response!\n        at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.checkHeader(HeaderBufferingWebResponse.java:62)\n        at org.apache.wicket.protocol.http.HeaderBufferingWebResponse.setDateHeader(HeaderBufferingWebResponse.java:131)\n\n        at org.apache.wicket.protocol.http.BufferedWebResponse$SetDateHeaderAction.invoke(BufferedWebResponse.java:241)\n        at org.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:487)\n        at org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:225)\n        at org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:139)\n        at org.apache.wicket.request.cycle.RequestCycle$HandlerExecutor.respond(RequestCycle.java:715)\n        at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:274)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)\n        at org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:283)\n        at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227)\n        at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:253)\n        at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:138)\n        at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:194)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1323)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:474)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:517)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:935)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:404)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:870)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:247)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:151)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n        at org.eclipse.jetty.server.Server.handle(Server.java:346)\n        at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:596)\n        at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1051)\n        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:592)\n        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:214)\n        at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:426)\n        at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:241)\n        at org.eclipse.jetty.server.ssl.SslSocketConnector$SslConnectorEndPoint.run(SslSocketConnector.java:646)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:528)\n        at java.lang.Thread.run(Thread.java:619)""}"
wicket,bugs-dot-jar_WICKET-3620_1a2bc1bc,"{'BugID': 'WICKET-3620', 'Summary': 'IResponseFilter cannot change buffer contents', 'Description': 'Changes to the responseBuffer, passed to an IResponseFilter, are not picked up, nor are newly created AppendingStringBuffer (return value of the method). Both callers of the method invoke it with a copy of the buffer and ignore return values (BufferedWebResponse line 145 and AjaxRequestTarget line 687).'}"
wicket,bugs-dot-jar_WICKET-3644_ab1856db,"{'BugID': 'WICKET-3644', 'Summary': 'RequestCycleListenerCollection.onException should not throw an exception when multiple listeners handle the exception', 'Description': ""When multiple listeners handle the exception, RequestCycleListenerCollection should simple take the first handler. The current approach makes it impossible to add a listener that handles all exceptions and later add listeners for specific exceptions. Simply removing the 'if (handlers.size() > 1)' should suffice.""}"
wicket,bugs-dot-jar_WICKET-3646_12124902,"{'BugID': 'WICKET-3646', 'Summary': 'RequestHandler listeners are called with null handler', 'Description': 'When running RequestCycleListeners with WicketTester, they get called with null for the requesthandler in onScheduleHandler(). This is an artifact of the wicket tester requestcycle, which I think should normally not occur in Wicket processing. The parameters are not marked as optional, so it is IMO safe to assume that handler should never be null.\n\nI propose to modify scheduleRequestHandlerAfterCurrent(IRequestHandler handler) to not invoke the listeners when the handler is null, but only to clear the current scheduled handler.'}"
wicket,bugs-dot-jar_WICKET-3647_1b57b51c,"{'BugID': 'WICKET-3647', 'Summary': 'Component#getMarkupId() throws exceptions when not added to page yet', 'Description': 'When retrieving the markup ID for a component that has not yet been added to a page, Wicket currently throws an exception telling that the markup could not be found, or that the markup type (in case the component was added to a Panel) could not be determined. In 1.4, Wicket would generate a markup ID in these cases.\n\nProposed solution: to first see if a markup ID has been either generated or set (using setOutputMarkupId), and then returning that, or if no ID was yet available *and* the component has been added to a Page: use the ID from the markup, or if the component has not been added to a Page nor a markup ID: generate the ID.\n'}"
wicket,bugs-dot-jar_WICKET-3702_a08562a7,"{'BugID': 'WICKET-3702', 'Summary': 'wicket:border: inconsistency between add() and remove()', 'Description': ""Assuming c1 is a Border and c2 is some component, the following sequence crashes with duplicate addition:\n\nc1.add(c2);\nc1.remove(c2);\nc1.add(c2);\n\nThe reason for this is that remove() doesn't remove the object from the bodycontainer. The sequence can be made to work by changing the middle line to:\n\nc1.getBodyContainer().remove(c2);\n\nThat remove() doesn't look the component from the same container as add() adds it to, seems to violate the principle of least astonishment. Unfortunately the Component structure manipulation API has more methods, such as swap(), size(), get(), etc. which are final, and can't be overridden by Border as they are. It could be best to force all users to use c1.getBodyContainer().add() instead of c1.add(), because consistent operation is probably easier to deal with in the long run than behavior that conforms to initial assumptions but has flaws elsewhere.\n\nThis ticket suggests removing the overload of add() and documenting the difference in migration guide.""}"
wicket,bugs-dot-jar_WICKET-3713_e1168a57,"{'BugID': 'WICKET-3713', 'Summary': 'g/apache/wicket/protocol/http/request/UserAgent matches method is not correct', 'Description': 'In the UserAgent Enum matches method, the loop over detectionStrings is at most executed once:\n\n    for (List<String> detectionGroup : detectionStrings)\n    {\n      for (String detectionString : detectionGroup)\n      {\n        if (!userAgent.contains(detectionString))\n        {\n          return false;\n        }\n      }\n\n      return true;\n    }\n\nIt returns true after only processing the first element in the detectionStrings list.\nIt never looks at any of the other elements of the list.'}"
wicket,bugs-dot-jar_WICKET-3715_557de7bc,"{'BugID': 'WICKET-3715', 'Summary': 'FileUpload writeToTempFile() method throws NPE for sessionless requests', 'Description': 'I have created stateless page with stateless form containing FileUploadField, however when I tried to post file to it, NPE was thrown.\n\nThe issue is caused by method FileUpload#writeToTempFile() method trying to use session id as temp file prefix. \n\nWorkaround: create temp file manually and use method FileUpload#writeToFile( myTempFile)'}"
wicket,bugs-dot-jar_WICKET-3719_5ad32df9,"{'BugID': 'WICKET-3719', 'Summary': ""Component's markup cannot be found in Ajax requests if the parent is transparent"", 'Description': ""When TransparentWebMarkupContainer is used an inner markup container cannot find its markup on Ajax updates.\nThe problem seems to be caused by the fact that ComponentResolvers#resolve() is not executed and since there is transparent container involved Markup.find(String) cannot find the markup for non-transparent markup containers.\nI'll commit a disabled test case that shows the problem. ""}"
wicket,bugs-dot-jar_WICKET-3721_1858bc18,"{'BugID': 'WICKET-3721', 'Summary': ""The Url's query parameters are not properly URL encoded"", 'Description': 'If page parameter has a value with special characters like \' then it is rendered as it is in the produced markup and is only XML encoded but never URL encoded.\nThis causes broken html for example in the case when a Link is attached to a non- a|area|link tag:\n\n\n<html><body><span wicket:id=""link"" onclick=""var win = this.ownerDocument.defaultView || this.ownerDocument.parentWindow; if (win == window) { window.location.href=&#039;bookmarkable/org.apache.wicket.MockPageWithLink?urlEscapeNeeded=someone&#039;s+ba+parameter&#039;; } ;return false""></span></body></html>\n\nNotice that &#039; after \'someone\' closes the location.href string too early and breaks the app.'}"
wicket,bugs-dot-jar_WICKET-3764_48454f4d,"{'BugID': 'WICKET-3764', 'Summary': 'Ajax behaviors are failing in stateless pages', 'Description': 'Stateless ajax behaviors are not working in stateless pages in 1.5-RC4.2. I verified it with the stateless demo project of Martin Grigorov (https://github.com/martin-g/wicket-stateless), when changing the dropdown on the start page an exception is thrown (clicking the increment link causes a similar exception): \n\norg.apache.wicket.behavior.InvalidBehaviorIdException: Cannot find behavior with id: 0 on component: [DropDownChoice [Component id = c]]\n\nAt first glance the reason may be located in org.apache.wicket.Behaviors.getBehaviorById() which does not create the ID list if missing (getBehaviorsIdList(false) in line 286 instead of getBehaviorsIdList(true)), because this error does not occur when getBehaviorId() was manually called in the page constructor to force creation of the list.'}"
wicket,bugs-dot-jar_WICKET-3767_84c3baac,"{'BugID': 'WICKET-3767', 'Summary': 'INullAcceptingValidator behavior seems broken in 1.5-RC4.2', 'Description': 'As discussed in this forum thread:\nhttp://apache-wicket.1842946.n4.nabble.com/INullAcceptingValidator-behavior-tp3570352p3570352.html\n\nIt appears that Wicket no longer calls INullAcceptingValidator intances when the validatable value is null.\n\nWicket wraps validators  as behaviors, using the adapter pattern. The adapter class (org.apache.wicket.validation.ValidatorAdapter) implements  the interface IValidator<T>. This ""hides"" the case where the actual validator is an INullAcceptingValidator. Therefore, when going through a component\'s attached validators, the code of org.apache.wicket.markup.html.form.FormComponent will never call INullAcceptingValidators when the value is null.\n'}"
wicket,bugs-dot-jar_WICKET-3769_b4e9d426,"{'BugID': 'WICKET-3769', 'Summary': 'WicketSessionFilter and HttpSessionStore use different attribute name for Wicket Session', 'Description': 'from this topic \nhttp://apache-wicket.1842946.n4.nabble.com/WicketSessionFilter-and-ignorePaths-in-WicketFilter-td3570291.html\nPlease, look at the second post (the ignorePaths param is not linked with this issue as the title suggests).\n\nHow to reproduce with the quickstart:\n1. open localhost:8080 - a wicket test page is displayed.\n2. open localhost:8080/external - this is the external servlet that tries to access the wicket session. An exception is thrown.'}"
wicket,bugs-dot-jar_WICKET-3834_30255f11,"{'BugID': 'WICKET-3834', 'Summary': 'WicketTester does not follow absolute redirects', 'Description': ""Wicket tester does not follow absolute redirects:\n\nThis is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.""}"
wicket,bugs-dot-jar_WICKET-3834_747bccb5,"{'BugID': 'WICKET-3834', 'Summary': 'WicketTester does not follow absolute redirects', 'Description': ""Wicket tester does not follow absolute redirects:\n\nThis is a problem when using HttpsMapper. For example when requesting a page over http:// with an forced redirect to https:// for secure access will make wicket tester return 'null' for the last renderer page instead of the rendered page instance. In general all kinds of absolute redirects to another page will not be tracked by wicket tester. So this potentially a problem for all kinds of tests that rely on absolute redirects.""}"
wicket,bugs-dot-jar_WICKET-3836_843b76b1,"{'BugID': 'WICKET-3836', 'Summary': 'regression on strategy to integrate cas authentication', 'Description': ""yes, It happens in org.apache.wicket.request.handler.PageProvider.getPageInstance() ,\nbut not for the WelcomePage, but the redirection page (RedirectPage).\nthe CASPageAuthorizationStrategy as we are not authentified a org.apache.wicket.RestartResponseAtInterceptPageException with in parameter an instance of RedirectPage.\nOn the second call of PageProvider.getPageInstance, the pageId is of 0, an all other parameters are nulls.\n\nThe run seems quite different on 1.5-RC4.2 version - There is only one call of the method PageProvider.getPageInstance() and it come after the CASPageAuthorizationStrategy.isPageAuthorized\n\nLe 27/06/2011 11:24, Martin Grigorov a écrit :\n> Put a breakpoint in\n> org.apache.wicket.request.handler.PageProvider.getPageInstance() and\n> see what happens.\n> It seems the test tries to retrieve a page from the page store by id\n> but there is no such.\n>\n> On Mon, Jun 27, 2011 at 12:20 PM, Thomas Franconville\n> <tfranconville@tetraedge.com>  wrote:\n>> Hi,\n>>\n>> Upgrading wicket from 1.5-RC4.2 to 1.5-RC5.1  make my Junit Test down with\n>> the error 'Page expired'\n>>\n>> /**\n>>   * Simple test using the WicketTester\n>>   */\n>> public class TestHomePage\n>> {\n>>     private WicketTester tester;\n>>\n>>     @Before\n>>     public void setUp()\n>>     {\n>>         tester = new WicketTester(new MyApplication());\n>>     }\n>>\n>>     @Test\n>>     public void homepageRendersSuccessfully()\n>>     {\n>>         //start and render the test page\n>>         tester.startPage(WelcomePage.class);\n>>\n>>         //assert rendered page class\n>>         tester.assertRenderedPage(RedirectPage.class);\n>>     }\n>> }\n>>\n>> My application use a CASPageAuthorizationStrategy inspired of\n>> http://www.lunikon.net/2009/11/24/integrating-cas-and-wicket/\n>>\n>>\n>> Kind Regards\n>>\n>> Thomas\n""}"
wicket,bugs-dot-jar_WICKET-3838_97514205,"{'BugID': 'WICKET-3838', 'Summary': 'In wicket 1.5 urlFor returns incorrect string for package mounted pages', 'Description': 'Attached two quickstart projects for 1.4 and 1.5.\n\nThen access http://localhost:8080/app/Page1 and see 1.5 returns wrong address.'}"
wicket,bugs-dot-jar_WICKET-3845_afc7034d,"{'BugID': 'WICKET-3845', 'Summary': 'support custom response headers in AbstractResource.ResourceResponse', 'Description': 'I\'m converting an application to Wicket 1.5 and I see some problems with resources.\n\nThere is a case I need to add headers (not present in ResourceResponse properties) and it looks ugly.\n\nThis is what I need to do:\n\n    @Override\n    protected void configureCache(ResourceResponse data, Attributes attributes)\n    {\n        super.configureCache(data, attributes);\n        ((WebResponse) attributes.getResponse()).setHeader(""Accept-Ranges"", ""bytes"");\n    }\n\nIt\'s a hack to use configureCache here, but this can\'t be added to setResponseHeaders, which seams a better apparent method name for it.\n'}"
wicket,bugs-dot-jar_WICKET-3861_d1e0e411,"{'BugID': 'WICKET-3861', 'Summary': 'AbstractTransformerBehavior sets wrong namespace', 'Description': 'AbstractTransformerBehaviour adds a wicket namespace (http://wicket.apache.org) to its tag which is different from that of the whole page (http://wicket.apache.org/dtds.data/wicket-xhtml1.4-strict.dtd).\n\nThis causes (at least) XPath queries for Wicket nodes to fail when matching the contents of components with an AbstractTransformerBehavior. '}"
wicket,bugs-dot-jar_WICKET-3872_3feb0e3a,"{'BugID': 'WICKET-3872', 'Summary': 'MarkupContainer.removeAll() does not detach models recursively', 'Description': 'ML thread at: http://markmail.org/message/ybdfd2ts4i3j2b72'}"
wicket,bugs-dot-jar_WICKET-3884_b772ff87,"{'BugID': 'WICKET-3884', 'Summary': 'Null model for AttributeAppender should not render empty attribute', 'Description': 'I can\'t think of a reason this would be valid, but passing in null model renders <span class="""">Test</span>.  If previous and new attribute are both null, the component should render cleanly like <span>Test</span>.'}"
wicket,bugs-dot-jar_WICKET-3885_beb9086d,"{'BugID': 'WICKET-3885', 'Summary': 'setResponsePage in AjaxLink goes always to localhost:8080 instead to the right host and port', 'Description': 'setResponsePage in an AjaxLink in Wicket 1.4 redirects with a relative path to the response page.\nWicket 1.5 takes the absolute path ""localhost:8080/path to the response page"" even when the host and port are different.\n(e.g. with Apache2 a virtual host is created with server name www.mycompany.com, setResponce wil go to ""localhost:8080/path to page"" instead of  ""www.mycompany.com/path to page"")'}"
wicket,bugs-dot-jar_WICKET-3906_aadaa4e9,"{'BugID': 'WICKET-3906', 'Summary': 'PageParameters#set not follow INamedParameters#set behavior', 'Description': 'Couple of problems to work with page parameters:\nMajor - The PageParameters#set(final String name, final Object value, final int index) used remove/add pattern instead of set parameter value by specified index.\nMinor - Inposible to get the index of key in elegant way to use obtained index in #set operation'}"
wicket,bugs-dot-jar_WICKET-3931_8fbdc68f,"{'BugID': 'WICKET-3931', 'Summary': 'Component markup caching inconsistencies', 'Description': ""In WICKET-3891 we found that Component#markup field is not being reset between requests. The problem is that this field is transient and it is null-ified only when the page is read from the second level page cache (see https://cwiki.apache.org/confluence/x/qIaoAQ). If the page instance is read from first level cache (http session) then its non-serialized version is used and the markup field value is still non-null.\n\nIn WICKET-3891 this looked like a minor issue with the markup caching in development mode but actually this problem is valid even in production mode.\nSee the attached application. When the panel's variation is changed every MarkupContainer inside still uses its old markup. ""}"
wicket,bugs-dot-jar_WICKET-3965_6051019b,"{'BugID': 'WICKET-3965', 'Summary': 'A (stateless) page immediately disappears after the first render', 'Description': 'Using setResponsePage(new SomeStatelessNonBookmarkablePage(aParameter)) renders the page but trying to reload the page in the browser fails with PageExpiredException.\n\nThe reason is that the page is stateless and thus it is not saved in the page stores. Since it was scheduled for render with setResponsePage(Page) method its Url is created by PageInstanceMapper (i.e. something like: wicket/page?1). An attempt to refresh such page fails with ""Page with id \'1\' is not found => PageExpiredException"".\n\nIgor suggested to call \'page.setStatelessHint(false)\' for all pages passed to PageProvider(IRequestablePage) constructor, i.e. such pages must be stored.\nThis solved the problem but exposed few more problems:\n- MockPageManager (used in WicketTester) until now always touched/stored pages, no matter their statelessness\n- org.apache.wicket.markup.html.internal.EnclosureTest.testRender10() was wrong for some unknown reason. All expectations against EnclosurePageExpectedResult_10-2.html should not have the enclosure rendered because ""input"" component is invisible'}"
wicket,bugs-dot-jar_WICKET-3989_6a8fc1cc,"{'BugID': 'WICKET-3989', 'Summary': 'MarkupNotFoundException when refreshing a component with AJAX inside a TransparentWebMarkupContainer', 'Description': 'A component placed inside a TransparentWebMarkupContainer, added to its parent cannot be refreshed with AJAX. See quickstart.'}"
wicket,bugs-dot-jar_WICKET-3998_b76f9c44,"{'BugID': 'WICKET-3998', 'Summary': 'CreditCardValidator returns incorrect cardId for VISA', 'Description': 'When the validation for a VISA is correct, it returns a SWITCH cardId instead of a VISA.\nThis error occurs in both 1.4.x and 1.5.X\n\n'}"
wicket,bugs-dot-jar_WICKET-4000_38e928c1,"{'BugID': 'WICKET-4000', 'Summary': 'Header contributions order is not stable', 'Description': ""In the last RCs, I started to experience problems with the contributions order.\nFor example, I add jQuery, and until 1.5RC5, it worked well, but now the call to the jQuery script has been moved to the bottom of the page head, and this disables all my other scripts that are expecting jQuery's $ to be defined.\n\nI attach a quickstart to demonstrate the problem.\nMaybe the order in the quickstart is not the expected one, but what it shows is that the order does not make real sense (at least to me) :\nIn the quickstart, the wicket:head tag contributions are in the order 3 - 8 - 9 - 5, and the renderHead methods contributions are in the order 4 - 1 - 2 - 6 - 7.""}"
wicket,bugs-dot-jar_WICKET-4012_d35d2d85,"{'BugID': 'WICKET-4012', 'Summary': ""Component's onAfterRender() is called so many times as it is depth in the component tree + 1"", 'Description': 'org.apache.wicket.Component.afterRender() calls org.apache.wicket.Component.onAfterRenderChildren() which for MarkupContainers calls afterRender() for its children.\n\nSo for code like:\n\n WebMarkupContainer comp1 = new WebMarkupContainer(""c1"");\n        add(comp1);\n        \n        WebMarkupContainer comp2 = new WebMarkupContainer(""c2"");\n        comp1.add(comp2);\n        \n        WebMarkupContainer comp3 = new WebMarkupContainer(""c3"") {\n\n            @Override\n            protected void onAfterRender() {\n                super.onAfterRender();\n                System.err.println(""called"");\n            }\n            \n        };\n        comp2.add(comp3);\n\nyou\'ll see ""called"" printed 4 times in a single request.\n\nAdditionally I think onAfterRenderChildren() should be called before onAfterRender() in Component.afterRender(). The flow should be first-in last-out: onBeforeRender > onBeforeRenderChildren > onAfterRenderChildren > onAfterRender,'}"
wicket,bugs-dot-jar_WICKET-4014_e60bac5f,"{'BugID': 'WICKET-4014', 'Summary': 'Wicket 1.5 Form Post Action and Link Get discard Page Class Information', 'Description': 'Page expiry is a very annoying and perplexing event especially if users stay logged in via remember-me cookie.\n\nIt is therefore not a fancy enhancement but an essential business requirement to not drop the user out of context after session expiry.\nOnly stateless pages can fully achieve this, but it is not always desirable to go fully stateless, especially while a recovery solution already exists.\nIn 1.4, this appears to be automatic with BookmarkablePageRequestTargetUrlCodingStrategy - without any additional coding.\n\nThe solution is well known - keep as much state in the client as required to recover the page class, and possibly even its page parameters, and to not destroy this information.\n\nThe two attached testcases show two possible methods of page fallback recovery (one with AJAX, one without) that already work behind the scenes.\nOf course it is easy with AJAX, to just force a page reload, but this is not discussed here. AJAX just serves to demonstrate how easy the principle actually is.\nIn most cases the user could successfully reload the page but Wicket 1.5 can\'t create a response because it has forgotten the class of the expired page.\n\nIn 1.4, it is possible to recover the class of an expired page via its mount path.\nThis feature is lost in 1.5.\n\nTo get this functionality back in a more streamlined fashion, I am additionaly proposing in a separate jira issue 4013 to store page class and page parameters in PageExpiredException.\n\nMeanwhile, the focus of this issue is to request whatever means to not overwrite the path of a page in a form post action or get request, and to get the page class back as in 1.4 by whatever means.\n\nThe two attached testcases may be helpful for expermintation. The 1.4 tescase demonstrates how the scheme works, unfortunately I could not fill the blanks in the 1.5 testcase.\n\nIn 1.4,\n\na form tag is rendered as:\n<form wicket:id=""form"" action=""?wicket:interface=:0:form::IFormSubmitListener::""\nThis is requested as:\n/testForm.0?wicket:interface=:0:form::IFormSubmitListener::\nand the page class can be recovered from the mount path ""testForm"" as in\n    mount(new HybridUrlCodingStrategy(""testForm"", TestPageForm.class));\n\nan anchor tag is rendered as:\n<a href=""?wicket:interface=:0:linkSwitch::ILinkListener::""\nThis is requested as:\n/testLink.0?wicket:interface=:0:linkSwitch::ILinkListener::\nand the page class can be recovered from the mount path ""test"" as in\n    mount(new HybridUrlCodingStrategy(""testLink"", TestPageLink.class));\n\n\n\nIn 1.5,\n\na form tag is rendered as:\n<form wicket:id=""form"" action=""wicket/page?0-2.IFormSubmitListener-form""\nThis is requested requested as:\n/wicket/page?0-1.IFormSubmitListener-form\n\nThis overwrites the mount path ""testForm"" as in\n    mountPage(""testForm"", TestPageForm.class);\nConsequently the server cannot discover the page class\n\n\nan anchor tag is rendered as:\n<a href=""wicket/page?0-1.ILinkListener-linkSwitch""\nThis is requested requested as:\n/wicket/page?0-1.ILinkListener-linkSwitch\n\nThis overwrites the mount path ""testLink"" as in\n    mountPage(""testLink"", TestPageLink.class);\nConsequently the server cannot discover the page class\n'}"
wicket,bugs-dot-jar_WICKET-4016_f1c9cef2,"{'BugID': 'WICKET-4016', 'Summary': 'MarkupContainer.toString(true) fails with MarkupNotFoundException if the call is made in the component constructor', 'Description': 'org.apache.wicket.MarkupContainer.toString(boolean) uses ""if (getMarkup() != null)"" to decide whether to write something for the markup but since recently Component#getMarkup() throws MarkupNotFoundException when there is no markup and doesn\'t return null.'}"
wicket,bugs-dot-jar_WICKET-4020_081cdeb2,"{'BugID': 'WICKET-4020', 'Summary': 'ResourceMapper throws IllegalStateException when attempting to map a request to a URL ending in a empty segment (directory)', 'Description': ""ResourceMapper.mapRequest() calls ResourceMapper.removeCachingDecoration() which, throws IllegalStateException if the URL's last segment is an empty string.\n\nURLs like: path/to/my/non/wicket/directory/ end in a empty segment. \n\nWe must change the behaviour to not attempt to undecorate a URL ending in an empty segment.""}"
wicket,bugs-dot-jar_WICKET-4030_5f69685d,"{'BugID': 'WICKET-4030', 'Summary': 'HeaderResponse.renderCSSReference does not render context path relative url, but wicket filter url-pattern relative url', 'Description': 'In an application with a wicket filter url-pattern different than /*, if you use HeaderResponse.renderCSSReference(String url), where url is a context-path-relative url (css/main.css, for example), the generated css link is not context relative, but wicket url-pattern relative.'}"
wicket,bugs-dot-jar_WICKET-4038_f3d7565c,"{'BugID': 'WICKET-4038', 'Summary': 'MountedMapper.mapHandler ruins Links inside mounted pages appending parameters wicket-ajax and wicket-ajax-baseurl', 'Description': 'With the last commit n° 1166194 method mapHandler has been added to MountedMapper class in order to solve WICKET-4014. Unfortunately this method seems to ruin Link url inside mounted page (for example home page) if this page uses AJAX.\nmapHandler modifies Link url appending parameters \'wicket-ajax\' and \'wicket-ajax-baseur\'l to it. In this way when we click Link we get an error from browser like this:\n\n       "" This XML file does not appear to have any style information associated with it. The document tree is shown below.\n      <ajax-response><redirect>wicket/page?41</redirect></ajax-response> ""\n\nThe error message is the same for Firefox and Chromium.\nSee attached quickstart.\n\nWarning: as I\'m writing this issue, Wicket snapshot is not affected yet by this bug, so you have to run quickstart with the last source from repository.'}"
wicket,bugs-dot-jar_WICKET-4066_4d3d1f85,"{'BugID': 'WICKET-4066', 'Summary': 'RestartResponseAtInterceptPageException.InterceptData is never cleared', 'Description': 'RestartResponseAtInterceptPageException.InterceptData is supposed to be cleared after continueToOriginalDestination() is called. This is accomplished via RestartResponseAtInterceptPageException.MAPPER, which is registered in the SystemMapper.\n\nHowever there seems to be a serious bug here. The MAPPER always returns a compatibilityScore of 0, and thus is never actually invoked. The InterceptData is thus never cleared. Furthermore, even if the MAPPER did return a Integer.MAX_VALUE score, it would still not be invoked in many scenarios, since other mappers in the SystemMapper are registered later and therefore have higher priority.\n\nIn practice, this can lead to very odd login behavior in Wicket applications (which is where RestartResponseAtInterceptPageException is typically used). For example, if the user clicks a ""login"" link they may end up on a totally unexpected page, due to stale InterceptData that is hanging around in the session.\n\nI am attaching a quick start that demonstrates the problem, as well as a patch the fixes the compatibilityScore and moves the MAPPER to a higher priority in the SystemMapper.'}"
wicket,bugs-dot-jar_WICKET-4070_d450acb0,"{'BugID': 'WICKET-4070', 'Summary': 'Errors reported from Form#onValidateModelObjects() are ignored', 'Description': None}"
wicket,bugs-dot-jar_WICKET-4072_7d5b8645,"{'BugID': 'WICKET-4072', 'Summary': 'Form Input example fails when changing the language', 'Description': 'Trying to change the language of http://localhost:8080/forminput example fails with:\n\n\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -1\n\tat java.util.ArrayList.remove(ArrayList.java:390)\n\tat org.apache.wicket.request.Url.resolveRelative(Url.java:884)\n\tat org.apache.wicket.markup.html.form.Form.dispatchEvent(Form.java:1028)\n\tat org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:699)\n\tat org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670)\n\t... 37 more\n'}"
wicket,bugs-dot-jar_WICKET-4099_1dcaec98,"{'BugID': 'WICKET-4099', 'Summary': ""SmartLinkLabel doesn't recognize already tagged links"", 'Description': 'The SmartLinkLabel works as expected for the texts without <a>..</a> tag. \nfor text like\nextensions @ http://www.wicketframework.org/wicket-extensions/index.html are cool!!\nSmartLinkLabel generates the html - \nextensions @ <a href=""http://www.wicketframework.org/wicket-extensions/index.html"">http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!!\n\nbut for the text like\nextensions @ <a href=\'http://www.wicketframework.org/wicket-extensions/index.html\'>http://www.wicketframework.org/wicket-extensions/index.html</a> are cool!!\nSmartLinkLabel generates the html - \nextensions @ <a href=\'<a href=""http://www.wicketframework.org/wicket-extensions/index.html"">http://www.wicketframework.org/wicket-extensions/index.html</a>\'><a href=""http://www.wicketframework.org/wicket-extensions/index.html"">http://www.wicketframework.org/wicket-extensions/index.html</a></a> are cool!!\n\nI think this is a bug & needs a fix.\n'}"
wicket,bugs-dot-jar_WICKET-4102_e743fd7e,"{'BugID': 'WICKET-4102', 'Summary': 'AutoLabelTextResolver fails to pick up locale changes in the session', 'Description': 'When using <wicket:label key=""...""> AutoLabelTextResolver correctly picks up the localized message identified by the key. However, if the Session locale is changed, neither the printed label nor the FormComponent\'s label model get updated - both will still contain the initial message. This is inconsistent with the behavior of <wicket:message> and StringResourceModel. The principle of least surprise (and in my opinion, also that of highest usefulness ;-) ) suggests that AutoLabelTextResolver should dynamically get the localized string whenever it deals with something that can be localized. That includes the <wicket:label key=""...""> case mentioned above, as well as when using FormComponent#getDefaultLabel.\n\nI have only tested this in trunk 1.5 (since it recently came up during a training I gave on Wicket 1.5). I suspect it also affects 1.4.x.'}"
wicket,bugs-dot-jar_WICKET-4105_64656c98,"{'BugID': 'WICKET-4105', 'Summary': 'Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException', 'Description': 'Using ajax to update a component that has an AbstractTransformerBehavior attached throws a ClassCastException:\n\njava.lang.ClassCastException:\norg.apache.wicket.ajax.AjaxRequestTarget$AjaxResponse cannot be cast\nto org.apache.wicket.request.http.WebResponse\n      at org.apache.wicket.markup.transformer.AbstractTransformerBehavior.beforeRender(AbstractTransformerBehavior.java:68)\n\n      at org.apache.wicket.Component.notifyBehaviorsComponentBeforeRender(Component.java:3421)\n      at org.apache.wicket.Component.internalRender(Component.java:2344)\n      at org.apache.wicket.Component.render(Component.java:2273)\n\n      at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)\n      at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)\n      at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)\n\n      at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1567)\n      at org.apache.wicket.markup.html.form.Form.onComponentTagBody(Form.java:1570)\n      at org.apache.wicket.markup.html.panel.DefaultMarkupSourcingStrategy.onComponentTagBody(DefaultMarkupSourcingStrategy.java:72)\n\n      at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)\n      at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)\n      at org.apache.wicket.Component.internalRender(Component.java:2345)\n\n      at org.apache.wicket.Component.render(Component.java:2273)\n      at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1474)\n      at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1638)\n\n      at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1613)\n      at org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:735)\n      at org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76)\n\n      at org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112)\n      at org.apache.wicket.Component.internalRenderComponent(Component.java:2515)\n      at org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1576)\n\n      at org.apache.wicket.Component.internalRender(Component.java:2345)\n      at org.apache.wicket.Component.render(Component.java:2273)\n      at org.apache.wicket.ajax.AjaxRequestTarget.respondComponent(AjaxRequestTarget.java:982)\n\n      at org.apache.wicket.ajax.AjaxRequestTarget.respondComponents(AjaxRequestTarget.java:796)\n      at org.apache.wicket.ajax.AjaxRequestTarget.constructResponseBody(AjaxRequestTarget.java:676)\n      at org.apache.wicket.ajax.AjaxRequestTarget.respond(AjaxRequestTarget.java:637)\n\n      at org.apache.wicket.request.cycle.RequestCycle$HandlerExecutor.respond(RequestCycle.java:712)\n      at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:63)\n      at org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:96)\n\n      at org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:208)\n      at org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:251)\n      at org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:162)\n\n      at org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:218)\n      at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n      at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n\n      at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n      at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n      at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n\n      at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n      at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n      at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n\n      at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n      at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n      at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n\n      at java.lang.Thread.run(Thread.java:619)\n'}"
wicket,bugs-dot-jar_WICKET-4109_8f7805f8,"{'BugID': 'WICKET-4109', 'Summary': 'AutocompleteTextField after Submit does not work', 'Description': 'I use an AutocompleteTextfield together with a submit-Button. After once submitting the content oft the AutocompleteTextField the parameter q is added to the URL. After that the autocompletion will only complete the parameter q in the url and not the parameter given by ajax.\n\nI tracked the problem down to the callbackURL. \nIt contains a pattern looking as follows: ....&q=<paramproducedbysubmit>&q=<paramproducedbyajaxautocomplete> \nThe callbackurl is build of the parameter q and the extraction of parameters only accepts the first parameter\n'}"
wicket,bugs-dot-jar_WICKET-4116_4624ab3d,"{'BugID': 'WICKET-4116', 'Summary': 'Ajax link reports weird error when session is expired', 'Description': 'Reproducing steps:\n\n1. Put below simple page into a Wicket application and get it mounted:\n\nTestPage.java:\n\nimport org.apache.wicket.ajax.AjaxRequestTarget;\nimport org.apache.wicket.ajax.markup.html.AjaxLink;\nimport org.apache.wicket.markup.html.WebPage;\n\n@SuppressWarnings(""serial"")\npublic class TestPage extends WebPage {\n\t\n\tpublic TestPage() {\n\t\t\n\t\tadd(new AjaxLink<Void>(""test"") {\n\n\t\t\t@Override\n\t\t\tpublic void onClick(AjaxRequestTarget target) {\n\t\t\t}\n\t\t\t\n\t\t});\n\t\t\n\t}\n\t\n}\n\nTestPage.html:\n\n<!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"">\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<html xmlns=""http://www.w3.org/1999/xhtml"">\n\t<head>\n\t\t<title>Test Page</title>\n\t</head>\n\t<body>\n\t\t<a wicket:id=""test"">test</a>\n\t</body>\n</html>\n\n2. Access the page in browser via mounted url, the page will display a link. \n\n3. Wait until current session is expired (do not refresh the page or click the link while waiting). \n\n4. Hit the link and below exception will be thrown:\nMessage: Cannot find behavior with id: 0 on component: [ [Component id = test]]. Perhaps the behavior did not properly implement getStatelessHint() and returned \'true\' to indicate that it is stateless instead of returning \'false\' to indicate that it is stateful.\n\n5. In wicket 1.5.0, this results in a PageExpiredException which is more comprehensive. \n\n'}"
wicket,bugs-dot-jar_WICKET-4119_bb7a6995,"{'BugID': 'WICKET-4119', 'Summary': 'FileResourceStream returns unknown content type', 'Description': 'See http://apache-wicket.1842946.n4.nabble.com/PackageResourceReference-and-Doctype-in-Markup-file-tp3889467p3889587.html\n\nThe response for FileResourceStreams returns an unknown content type for css- and image-files. Correct content types should be ""text/css"" and ""image/png"" (see also attached quickstart).'}"
wicket,bugs-dot-jar_WICKET-4121_8967eb2b,"{'BugID': 'WICKET-4121', 'Summary': 'WizardStep$FormValidatorWrapper.isActiveStep(WizardStep.java) causes NullPointerException', 'Description': 'Using the Wizard with a nested WizardModel (see RecursiveWizardModel implementation at the attached quickstart application) causes a NullPointerException. I was able to run the same test application with wicket 1.4.18 without any problems.\n\nCaused by: java.lang.NullPointerException\n        at org.apache.wicket.extensions.wizard.WizardStep$FormValidatorWrapper.isActiveStep(WizardStep.java:145)\n        at org.apache.wicket.extensions.wizard.WizardStep$FormValidatorWrapper.getDependentFormComponents(WizardStep.java:109)\n        at org.apache.wicket.markup.html.form.validation.FormValidatorAdapter.getDependentFormComponents(FormValidatorAdapter.java:47)\n        at org.apache.wicket.markup.html.form.Form.validateFormValidator(Form.java:1782)\n        at org.apache.wicket.markup.html.form.Form.validateFormValidators(Form.java:1828)\n        at org.apache.wicket.markup.html.form.Form.validate(Form.java:1706)\n        at org.apache.wicket.markup.html.form.Form.process(Form.java:773)\n        at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:728)\n        at org.apache.wicket.markup.html.form.Form.onFormSubmitted(Form.java:670)'}"
wicket,bugs-dot-jar_WICKET-4138_7c89598a,"{'BugID': 'WICKET-4138', 'Summary': 'BookmarkablePageLinks not working on a forwarded page', 'Description': ""While migrating our app from 1.4 to 1.5 we have discovered a problem with how BookmarkablePageLinks are rendered.\n\nThe attached quickstart demonstrates the problem:\n\nTwo pages: HomePage and OtherPage mounted at: /content/home and /content/other respectively.\n\nThese are mounted using the encoder UrlPathPageParametersEncoder for backwards compatibility with existing 1.4 style URLs.\n\nA filter has been established in web.xml to forward requests to root (eg., localhost) to localhost/content/home\n[Note: I have left out the port :8080 part from all URL references so please insert when testing]\n\nPoint browser to http://localhost and the page is forwarded to http://localhost/content/home and displays correctly (browser URL still shows http://localhost as desired) but the links do not work because they remove the 'content' segment of the URL:\n\neg., Home link -> http://localhost/home - fails - should have been http://localhost/content/home\n\nIf you type in the full URL: http://localhost/content/home\n\nthen the home page displays and the links work correctly.\n\nA similar page set  up works fine in 1.4.""}"
wicket,bugs-dot-jar_WICKET-4153_2737d7c7,"{'BugID': 'WICKET-4153', 'Summary': 'The tbody section of a DataTable is empty when no records are returned by the provider.', 'Description': 'When a DataTable is rendered without records, the tbody section is empty. This violates the html spec.\n\nFrom the spec:\n""When present, each THEAD, TFOOT, and TBODY contains a row group. Each row group must contain at least one row, defined by the TR element.""\nand\n""The THEAD, TFOOT, and TBODY sections must contain the same number of columns.""'}"
wicket,bugs-dot-jar_WICKET-4173_84bbbf68,"{'BugID': 'WICKET-4173', 'Summary': 'Incorrect URL for setResponsePage() within a Form#onSubmit( )', 'Description': 'If the WebApplication uses IRequestCycleSettings.RenderStrategy.ONE_PASS_RENDER, the issue described and exemplified in the attached quickstart at\n\nhttps://issues.apache.org/jira/browse/WICKET-3442\n\nprevails. \n\nClicking the link on /pageone results in this URL: /pageone?0-1.IFormSubmitListener-form'}"
wicket,bugs-dot-jar_WICKET-4184_a0150366,"{'BugID': 'WICKET-4184', 'Summary': 'AppendingStringBuffer.insert  infinite loop', 'Description': 'When trying to insert a StringBuffer into an AppendingStringBuffer, the method \n\npublic AppendingStringBuffer insert(final int offset, final Object obj)\n\nwill call itself repeatedly generating an infinite loop.\n\nThe fix would be to call toString() method if the object is a StringBuffer\n\n\npublic AppendingStringBuffer insert(final int offset, final Object obj)\n\t{\n\t\tif (obj instanceof AppendingStringBuffer)\n\t\t{\n\t\t\tAppendingStringBuffer asb = (AppendingStringBuffer)obj;\n\t\t\treturn insert(offset, asb.value, 0, asb.count);\n\t\t}\n\t\telse if (obj instanceof StringBuffer)\n\t\t{\n\t\t\t//return insert(offset, obj);\n                       return insert(offset, obj.toString());\n\n\t\t}\n\t\treturn insert(offset, String.valueOf(obj));\n\t}\n'}"
wicket,bugs-dot-jar_WICKET-4185_5fd03973,"{'BugID': 'WICKET-4185', 'Summary': 'ListenerInterfaceRequestHandler should not assume existence of a page', 'Description': 'ListenerInterfaceRequestHandler should not assume a page instance is always available in isPageInstanceCreated. This handler can also be used for links on bookmarkable pages. The attached patch fixes this.'}"
wicket,bugs-dot-jar_WICKET-4251_53bcb78d,"{'BugID': 'WICKET-4251', 'Summary': 'Multipart Form and AjaxSubmitLink will result in invalid redirect after user session expires', 'Description': 'Hi,\n\nI have hit an issue similar to this one:\n\nhttps://issues.apache.org/jira/browse/WICKET-3141\n\nI do not receive any errors from Wicket itself to help clarify, so I will try to explain using an example.\n\nThe example below with which I could recreate the issue uses the default SignInPanel (in my LoginPage.clas) and AuthenticatedWebSession to authenticate the user and store the session:\n\n\tprotected Class<? extends WebPage> getSignInPageClass()\n\t{\n\t\treturn LoginPage.class;\n\t}\n\nIf the authentiation is succesfull then the user is redirect back to the test page:\n\n\t\t\tprotected void onSignInSucceeded() {\n\t\t\t\tsetResponsePage(Test.class);\n\t\t\t}\n\nSo far so good. However if I use a form with setMultiPart(true) in combination with an AjaxSubmitLink as shown in the following piece of code:\n\nimport org.apache.wicket.ajax.AjaxRequestTarget;\nimport org.apache.wicket.ajax.markup.html.form.AjaxSubmitLink;\nimport org.apache.wicket.authroles.authorization.strategies.role.annotations.AuthorizeInstantiation;\nimport org.apache.wicket.markup.html.WebPage;\nimport org.apache.wicket.markup.html.form.Form;\n\n@AuthorizeInstantiation(""USER"")\npublic class Test extends WebPage {\n\n\tpublic Test()\n\t{\n\t\tsuper();\n\t\t\n\t\tfinal Form testForm = \n\t\t\t\tnew Form(""testForm"");\n\t\n\t\ttestForm.setMultiPart(true);\n\t\t\n\t\ttestForm.add(new AjaxSubmitLink(""testButton"", testForm) {\n\t\t\t\n\t\t\t@Override\n\t\t\tprotected void onSubmit(AjaxRequestTarget target, Form form) {\n\t\t\t\tsuper.onSubmit();\n\t\t\t};\n\t\t\t\n\t\t\t@Override\n\t\t\tprotected void onError(AjaxRequestTarget target, Form form) {\n\t\t\t\t\n\t\t\t};\n\t\t});\n\t\t\t\t\n\t\tadd(testForm);\n\t}\n}\n\nAnd have selected the option ""Remember credentials"" in the SignInPanel, clicking on the testButton AFTER the session has expired will result in:\n\nhttp://localhost:8080/PaladinWicket/?3-1.IBehaviorListener.0-testForm-testButton&wicket-ajax=true&wicket-ajax-baseurl=.\n\nwhich displays this in the browser:\n\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\n<ajax-response>\n<redirect>\n<![CDATA[ .?1 ]]>\n</redirect>\n</ajax-response>'}"
wicket,bugs-dot-jar_WICKET-4255_c250db9c,"{'BugID': 'WICKET-4255', 'Summary': 'bug in org.apache.wicket.validation.validator.UrlValidator', 'Description': 'Looks like there is a bug in UrlValidator. It validates URLs like ""http://testhost.local/pages/index.php"" as invalid.\nBut URL is valid! Try to execute ""new java.net.URL(""http://testhost.local/pages/index.php"");"" for example. It does not throws ""MalformedURLException"" because URL is valid.\n\nIn method: UrlValidator.isValidAuthority() there is code: ""if (topLevel.length() < 2 || topLevel.length() > 4){return false;}"" Looks like this ""> 4"" is a wrong constraint.'}"
wicket,bugs-dot-jar_WICKET-4256_09166ea8,"{'BugID': 'WICKET-4256', 'Summary': 'onBeforeRender() is called on components that are not allowed to render', 'Description': 'pasted from the list:\n\nHi,\n\nI ran into an odd problem this week. A model fed to a ListView was\ncalling service methods the current user wasn\'t allowed to use, and I\nwas wondering how that could happen. A panel far above this ListView in\nthe hierarchy had been secured (using Shiro annotations, but that turns\nout to not matter at all) and was not supposed to be rendered for this\nuser. From this I had expected the ListView not to be rendered either,\nbut here it was trying to assemble itself in onBeforeRender and thus\ncalling the ""forbidden"" service methods.\n\nI investigated Component and friends for a bit, and have found a\npotential problem.\n\ninternalBeforeRender() checks determineVisibility() before doing\nanything. So far so good. determineVisibility() then checks\nisRenderAllowed() so the application\'s IAuthorizationStrategy can block\ncertain components. This is where it goes wrong though:\nisRenderAllowed() only looks at FLAG_IS_RENDER_ALLOWED for performance\nreasons, and that flag hasn\'t been set yet! internalPrepareForRender()\nonly calls setRenderAllowed() *after* beforeRender().\n\nDue to this, the supposedly secure panel was going through its own\nbeforeRender and thus calling that ListView\'s beforeRender.\n\nI think this can be a serious problem, such as in my case described\nabove. I\'d expect that if isActionAuthorized(RENDER) is false, the\nsecured component should basically never get to the beforeRender phase.\nMy questions are now:\n\n- Is this intentional? If yes, please explain the reasoning behind it,\n because it isn\'t obvious to me.\n\n- If not, can we fix it? My intuitive suggestion would be to simply\n move the call to setRenderAllowed() from the end of\n internalBeforeRender() (prepareForRender in 1.4) to the beginning of\n that method, so beforeRender() can reliably look at that flag.\n\n- If we can fix it, when and where do we fix it? This hit me in 1.4,\n and looking at the code it\'s still there in 1.5. I\'d *really* like it\n fixed in the last 1.4 release, and certainly in 1.5, given that this\n has the potential to impact security.\n\n It\'s not an API break, but I\'m not sure whether the implications for\n application behavior are tolerable for all existing applications. On\n the other hand, it seems to be a real security problem, so maybe the\n change is justified. I\'d like some core dev opinions please :-)\n\nIf this is in fact a bug, I\'m of course willing to provide a ticket and\na patch :-)\n\nThanks!\n\nCarl-Eric\nwww.wicketbuch.de'}"
wicket,bugs-dot-jar_WICKET-4259_1f128536,"{'BugID': 'WICKET-4259', 'Summary': 'Using an IValidator on an AjaxEditableLabel causes ClassCastException', 'Description': 'AjaxEditableLabel<Integer> label = new AjaxEditableLabel<Integer>(""label"", new PropertyModel<Integer>(this, ""value""));\nform.add(label);\nlabel.setRequired(true);\nlabel.add(new RangeValidator<Integer>(1, 10));\n\nUsing a RangeValidator<Integer> on an AjaxEditableLabel<Integer>  causes an ClassCastException after editing the label. \n\njava.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String\n\nThis can be avoided by setting the type explicit on the AjaxEditableLabel.\n\nlabel.setType(Integer.class);\n\nBut this wasn\'t necessary in Wicket 1.4.19. In this version all works fine without setting the type explicit.\n\nI found out, that AbstractTextComponent.resolveType() is not able to get the type of the DefaultModel of the AjaxEditableLabel in Wicket 1.5.3.\n\nI will attach two QuickStarts to demonstrate the bug. One with wicket 1.4.19 and the other with Wicket 1.5.3\n\n'}"
wicket,bugs-dot-jar_WICKET-4260_925cae5c,"{'BugID': 'WICKET-4260', 'Summary': 'UrlRenderer renders invalid relative URLs if first segment contains colon', 'Description': 'Seen on Wicket 1.5.3.\n\nIf a relative url of a link starts with a path segment containing a colon then the whole uri will be regarded as absolute uri, so typically browsers will complain that there is no handle for the protocol foo in foo:bar/dee/per.\n\nSee also the attached quickstart. The start page contains three links, one relative with colon, one absolute and one to a mounted page without colon for comparison.\nThe application also has a static switch to add an extended urlrenderer, prepending ""./"" if needed. This fix is merely a quick shot and there might be better alternatives.\n'}"
wicket,bugs-dot-jar_WICKET-4276_32c76c4a,"{'BugID': 'WICKET-4276', 'Summary': ""Select component loses it's value"", 'Description': ""Select component loses selected option and shows the first option in some situations (one example is when you try to submit a form, but there are validation errors).\n\nIt was working fine in 1.4.18, but it's broken in 1.4.19.This must be caused by the solution from this issue https://issues.apache.org/jira/browse/WICKET-3962\nI think the problem is likely in Select.isSelected method, where String[] paths = getInputAsArray() is actually an array of uuid-s, so uuid-s are compared to paths.\n\nI haven't tested wicket 1.5, but this problem may also affect 1.5 versions.""}"
wicket,bugs-dot-jar_WICKET-428_4a6a573b,"{'BugID': 'WICKET-428', 'Summary': 'MiniMap.iterator().next() should throw NoSuchElementException', 'Description': 'The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add:\nif(i >= size)\n    throw new NoSuchElementException();\n'}"
wicket,bugs-dot-jar_WICKET-428_d906576c,"{'BugID': 'WICKET-428', 'Summary': 'MiniMap.iterator().next() should throw NoSuchElementException', 'Description': 'The wicket.util.collections.MiniMap.iterator().next() should throw NoSuchElementException when there are no more elements to return (line 235), please add:\nif(i >= size)\n    throw new NoSuchElementException();\n'}"
wicket,bugs-dot-jar_WICKET-4290_e1953357,"{'BugID': 'WICKET-4290', 'Summary': ""Confusion between a form component's wicket:id and a PageParameter in Wicket 1.5.x"", 'Description': 'A Form has a strange behavior when a component has the same wicket:id than a page parameter.\n\nTo create a Bookmarkable link after a form is submited, setResponsePage is called, and a PageParameter object is given as a parameter : \n\t\t\tPageParameters params = new PageParameters();\n\t\t\tparams.add(""searchString"", searchField.getValue());\n\t\t\tsetResponsePage(SomePage.class, params);\n\nIn Wicket 1.5, if ""searchString"" is also a form-component\'s wicket:id, the form will only be submitted once : \nsearchField.getValue() will always return the first value entered by the user.\n\n\nHere\'s an example : \n\npublic class SearchPanel extends Panel {\n\n\tpublic SearchPanel(String id) {\n\t\tsuper(id);\n\t\tadd(new SearchForm(""searchForm""));\n\t}\n\n\tprivate class SearchForm extends Form<String> {\n\n\t\tprivate static final long serialVersionUID = 1L;\n\t\tprivate TextField<String> searchField;\n\n\t\tpublic SearchForm(String id) {\n\t\t\tsuper(id);\n\t\t\tsearchField = new TextField<String>(""searchString"", new Model<String>(""""));\n\t\t\tadd(searchField);\n\t\t}\n\n\t\t@Override\n\t\tpublic void onSubmit() {\n\t\t\tPageParameters params = new PageParameters();\n\t\t\tparams.add(""searchString"", searchField.getValue());\n\t\t\tsetResponsePage(ListContactsPage.class, params);\n\t\t}\n\t}\n}\n\n\nI tested the same application with Wicket 1.4.17 and it was fine. I only had this problem in Wicket 1.5.2 and 1.5.3.'}"
wicket,bugs-dot-jar_WICKET-4292_9cb617ae,"{'BugID': 'WICKET-4292', 'Summary': 'MockHttpServletResponse.addCookie(Cookie) adds duplicate cookies', 'Description': 'org.apache.wicket.protocol.http.mock.MockHttpServletResponse.addCookie(Cookie) makes a bad check whether the cookie to be added is already in the list of cookies.\nSince javax.servlet.http.Cookie doesn\'t implement #equals() ""cookies.remove(cookie)"" wont remove the previous cookie because the identity is different.\n\nAccording to http://www.ietf.org/rfc/rfc2109.txt, p.4.3.3 :\n\n If a user agent receives a Set-Cookie response header whose NAME is\n   the same as a pre-existing cookie, and whose Domain and Path\n   attribute values exactly (string) match those of a pre-existing\n   cookie, the new cookie supersedes the old.  However, if the Set-\n   Cookie has a value for Max-Age of zero, the (old and new) cookie is\n   discarded.  Otherwise cookies accumulate until they expire (resources\n   permitting), at which time they are discarded.\n\nI.e. the equality is on the name, path and domain.'}"
wicket,bugs-dot-jar_WICKET-4301_50b52742,"{'BugID': 'WICKET-4301', 'Summary': 'ByteArrayResource throws error if data is null', 'Description': 'When ByteArrayResource#getData(org.apache.wicket.request.resource.IResource.Attributes) returns null, the class throws a WicketRuntimeException.\n\nThis behavior differs from DynamicImageResource and ResourceStreamResource which instead issue the following call:\nresponse.setError(HttpServletResponse.SC_NOT_FOUND);\n\nByteArrayResource should follow the same behavior. This would allow for instance to use it for resources which depend on the contents of attributes.getParameters(). When the parameters are invalid, a 404 should be issued instead of an exception.'}"
wicket,bugs-dot-jar_WICKET-4309_b4274415,"{'BugID': 'WICKET-4309', 'Summary': 'StringValueConversionException for correct situation', 'Description': 'StringValue.toOptionalLong() produces org.apache.wicket.util.string.StringValueConversionException if empty string was passed.\nLet me suggest, that this behavior should be changes for all toOptionalXXX methods except getOptionalString method.\n\nThe problem in inner code:\n\nThe problem in following code:\n\npublic final Long toOptionalLong() throws StringValueConversionException\n    {\n        return (text == null) ? null : toLongObject();\n    }\n\nShould be something like this:\n\nThe problem in following code:\n\npublic final Long toOptionalLong() throws StringValueConversionException\n    {\n        return Strings.isEmpty() ? null : toLongObject();\n    }\n\nBut there is another problem: what to do if incorrect param was passed - for example ""abc"" for parameter of Long type?'}"
wicket,bugs-dot-jar_WICKET-4323_e24874da,"{'BugID': 'WICKET-4323', 'Summary': ""StringResourceModels doesn't seem to detach properly"", 'Description': 'If a StringResourceModel contains a model for property substitutions, and there has not been assigned a component it is relative to on construction time, it will not detach the property substitution model.\n\nSee this thread for a full explanation\nhttp://apache-wicket.1842946.n4.nabble.com/StringResourceModels-doesn-t-seem-to-detach-properly-td4257267.html'}"
wicket,bugs-dot-jar_WICKET-4338_9decad35,"{'BugID': 'WICKET-4338', 'Summary': 'POST params ignored by IPageParametersEncoder#decodePageParameters()', 'Description': ""As per this conversation: http://apache-wicket.1842946.n4.nabble.com/how-to-get-https-port-number-in-Wicket-1-5-td4295139.html\n\nit seems that POST params are not properly processed and made available as PageParameters. Can anyone say whether this is intended behavior or not? I will attach a Quickstart to demonstrate.\n\nMartin's proposed fix is straightforward, but I am not comfortable enough with Wicket internals to say whether or not this would break something.\n\nThanks\n""}"
wicket,bugs-dot-jar_WICKET-4345_4f08e6f2,"{'BugID': 'WICKET-4345', 'Summary': 'CryptoMapper does not work for applications having a home page that needs query parameters', 'Description': 'CryptoMapper.decryptUrl() should not return null for requests like http://myhost/MyApplication/app/?param=xx \n\nAs a possible fix one can replace\n\nif (encryptedUrl.getSegments().isEmpty() && encryptedUrl.getQueryParameters().isEmpty()) {\n           return encryptedUrl;\n}\n\nwith \n\nif (encryptedUrl.getSegments().isEmpty()) {\n           return encryptedUrl;\n}\n\nbut I suspect that the original test is intended to answer to another use case... '}"
wicket,bugs-dot-jar_WICKET-4358_02ebc8ae,"{'BugID': 'WICKET-4358', 'Summary': 'BufferedWebResponse fails to add/clear cookie in redirect', 'Description': 'bufferedWebResponse.addCookie( cookie );\n\nThat fails under certain conditions: (1) when called on the last of three 302 redirects during OpenID login; and (2) on single redirect immediately after container startup, though it later recovers.  Failure confirmed in Firebug; no cookies sent in any of the response headers.  My workaround is to bypass the buffered response.  This works:\n\n((HttpServletResponse)bufferedWebResponse.getContainerResponse()).addCookie( cookie );\n'}"
wicket,bugs-dot-jar_WICKET-4365_1485a856,"{'BugID': 'WICKET-4365', 'Summary': ""Form components' name/value are encoded in stateless form's action url"", 'Description': ""Stateless forms aren't working well as you can see on wicket examples: http://www.wicket-library.com/wicket-examples/stateless/foo\n\nThe first time you submit, (for example, the value 10), everything works as supposed to. If you now change the value (to 11 for example) and submit the form, the value wicket shows is 10.\n\nI think the problem is stateless forms are generating an action URL with submitted values on query string, and when you resubmit the form, this values on query string replace the POST(or GET) values.""}"
wicket,bugs-dot-jar_WICKET-4370_7ca927c1,"{'BugID': 'WICKET-4370', 'Summary': 'HttpSession getSession() in MockHttpServletRequest is not compliant with the j2ee servlet spec', 'Description': 'The implementation of\nhttpRequest.getSession();\nfor MockHttpServletRequest seems not correct since it can return null when\nthe servler api specs\n(http://docs.oracle.com/javaee/1.4/api/) says:\n\npublic HttpSession getSession()\nReturns the current session associated with this request, or if the request\ndoes not have a session, creates one.\n\nSo as far as I understand\nhttpRequest.getSession(); and httpRequest.getSession(true); are equivalent\n\nThe MockHttpServletRequest implementation is\n\n   public HttpSession getSession()\n   {\n       if (session instanceof MockHttpSession &&\n((MockHttpSession)session).isTemporary())\n       {\n           return null;\n       }\n       return session;\n   }\n\nI think it should be \n   public HttpSession getSession()\n   {\n       return getSession(true);\n   }\n\n'}"
wicket,bugs-dot-jar_WICKET-4379_7a162f77,"{'BugID': 'WICKET-4379', 'Summary': 'org.apache.wicket.validation.ValidatorAdapter class causes problem with validator properties to be loaded', 'Description': '\nPROBLEM:\n<e1nPL> hi I am having such problem: \n<e1nPL> I have implemented validator by implementing IValidator<T> interface\n<e1nPL> and I have impelemnted the same validator by extending AbstractValidator<T> class\n\nCODE:\n    ===================== VALIDATOR EXTENDED FROM AbstractValidator =====================\n    package com.mycompany;\n     \n    import java.util.regex.Pattern;\n    import org.apache.wicket.IClusterable;\n    import org.apache.wicket.util.lang.Classes;\n    import org.apache.wicket.validation.IValidatable;\n    import org.apache.wicket.validation.IValidator;\n    import org.apache.wicket.validation.ValidationError;\n    import org.apache.wicket.validation.validator.AbstractValidator;\n     \n    /**\n     *\n     * @author e1n\n     */\n    public class PasswordPolicyValidator<T> extends AbstractValidator<T> {\n     \n        private static final Pattern UPPER = Pattern.compile(""[A-Z]"");\n        private static final Pattern LOWER = Pattern.compile(""[a-z]"");\n        private static final Pattern NUMBER = Pattern.compile(""[0-9]"");\n       \n        @Override\n        public void onValidate(IValidatable<T> validatable) {\n            final String password = (String)validatable.getValue();\n           \n            if (!NUMBER.matcher(password).find()) {\n                error(validatable, ""no-digit"");\n            }\n            if (!LOWER.matcher(password).find()) {\n                error(validatable, ""no-lower"");\n            }\n            if (!UPPER.matcher(password).find()) {\n                error(validatable, ""no-upper"");\n            }\n     \n        }\n       \n        @Override\n        public void error(IValidatable<T> validatable, String errorKey) {\n            ValidationError err = new ValidationError();\n            err.addMessageKey(Classes.simpleName(getClass()) + ""."" + errorKey);\n            validatable.error(err);\n        }\n       \n    }\n     \n     \n    =============== VALIDATOR directly implementing IValidator interfce ====================\n    package com.mycompany;\n     \n    import java.util.regex.Pattern;\n    import org.apache.wicket.IClusterable;\n    import org.apache.wicket.util.lang.Classes;\n    import org.apache.wicket.validation.IValidatable;\n    import org.apache.wicket.validation.IValidator;\n    import org.apache.wicket.validation.ValidationError;\n    import org.apache.wicket.validation.validator.AbstractValidator;\n     \n    /**\n     *\n     * @author e1n\n     */\n    public class PasswordPolicyValidator<T> implements IValidator<T> {\n     \n        private static final Pattern UPPER = Pattern.compile(""[A-Z]"");\n        private static final Pattern LOWER = Pattern.compile(""[a-z]"");\n        private static final Pattern NUMBER = Pattern.compile(""[0-9]"");\n     \n        public void validate(IValidatable<T> validatable) {\n            final String password = (String)validatable.getValue();\n           \n            if (!NUMBER.matcher(password).find()) {\n                error(validatable, ""no-digit"");\n            }\n            if (!LOWER.matcher(password).find()) {\n                error(validatable, ""no-lower"");\n            }\n            if (!UPPER.matcher(password).find()) {\n                error(validatable, ""no-upper"");\n            }\n     \n        }\n       \n        public void error(IValidatable<T> validatable, String errorKey) {\n            ValidationError err = new ValidationError();\n            err.addMessageKey(Classes.simpleName(getClass()) + ""."" + errorKey);\n            validatable.error(err);\n        }\n       \n    }\n\n\n\n<e1nPL> I also have properties file which is named after validator class\n<e1nPL> and placed in the same package\n<e1nPL> my problem is that when i use to validate my form field validator which implements IValidator interface it is not capable of loading error messages from properties file\n<e1nPL> but when i am using validator which is extending AbstractValidator class\n<e1nPL> properties file with error msgs gets loaded\nPOSSIBLE FIX:\n<e1nPL> ok i have found class which is responsible for my problem and it is probably a bug\n<e1nPL> org.apache.wicket.validation.ValidatorAdapter\n<e1nPL> which wraps classes that directly implements IValidator interface\n<e1nPL> then when resources are loaded, and properties file are searched in class path etc., loaders search in wrong path that is build against org.apache.wicket.validation.ValidatorAdapter \nPLACE WHER FIX SHOULD OCCOUR\norg.apache.wicket.resource.loader.ValidatorStringResourceLoader::loadStringResource(java.lang.Class,java.lang.String,java.util.Locale,java.lang.String,java.lang.String)\n\n'}"
wicket,bugs-dot-jar_WICKET-4384_614e3b50,"{'BugID': 'WICKET-4384', 'Summary': ""improve wicket's handling of empty / null page parameters"", 'Description': ""- DefaultPageFactory#newPage() should be sure to not pass 'null' to a page constructor with page parameters""}"
wicket,bugs-dot-jar_WICKET-4391_5d64196a,"{'BugID': 'WICKET-4391', 'Summary': 'XsltOutputTransformerContainer incorrectly claims markup type ""xsl""', 'Description': 'XsltOutputTransformerContainer return ""xsl"" from getMarkupType(), forcing is on all contained components.\n\nIf the components in org.apache.wicket.markup.outputTransformer.Page_1 are reordered (XsltOutputTransformerContainer coming first) the test fails because no markup for SimpleBorder can be found.'}"
wicket,bugs-dot-jar_WICKET-4398_f88721fd,"{'BugID': 'WICKET-4398', 'Summary': 'Any empty url-parameter will make wicket 1.5 crash', 'Description': 'Adding an empty parameter to the query string will make wicket crash.\n\nhttp://www.example.com/?oneParam&\n\n\nHow to reproduce in test:\n\nPageParameters params = new PageParameters();\nparams.set("""","""");\nparams.getAllNamed();\n\n\nCause:\nWicket accepts empty parameters, but when encoding the url for a rendered page it will call params.getAllNamed().\n\nparams.getAllNamed() instantiates new NamedPairs, which calls Args.notEmpty() on the key during instantiation, causing the application to crash. \n\nThe NamedPair constructor should probably allow empty string as a key, and call Args.notNull() on the key in stead.\n'}"
wicket,bugs-dot-jar_WICKET-442_246d53c5,"{'BugID': 'WICKET-442', 'Summary': 'adding (and querying) feedback messages at construction time fails.', 'Description': 'See http://www.nabble.com/error%28...%29-No-page-found-for-component-tf3497125.html\n\nCurrently, adding (and querying) feedback messages fails whenever it is done on components that are not yet added to a page (or were removed from them due to component replacement).\n\nThere are two ways to fix this. The first fix is attached as a patch, and basically uses a thread local to temporarily store the messages and distribute them to the relevant page instances just in time or when rendering starts. The advantage of this method is that it is completely back wards compatible.\n\nThe other way to fix this is to store all messages, whether component specific or not, in the session, and pull them from there. We need to be careful about how/ when to clean these error messages up though. We can use this issue to think about it a little bit more.'}"
wicket,bugs-dot-jar_WICKET-4441_54c86ebb,"{'BugID': 'WICKET-4441', 'Summary': 'PageProvider should create a new Page instance if PageParameters are changed, even if a stored page exists.', 'Description': ""The 'getStoredPage(int)' method returns a stored page instance even if user changes parameter values encoded into URL, and the PageParameters object of the stored page instance is never changed. So same page is displayed always though user changes url on browser manually.\n\n** HOW TO REPRODUCT **\n\n1. unpack the attached sample project 'pagebug.tar.gz'.\n2. mvn jetty:run\n3. access to http://localhost:8080/user/user1\n\nYou will see a form filled with information about user 1. The user's name is 'user 1', age is 30 and country is 'Japan'.\nThe mount path of this page is '/user/${userId}'. so 'user1' in the accessed url is a parameter value.\n\nafter accessing to the url, the url will be changed to http://localhost:8080/user/user1?0 .  it contains the page id of the currently displayed page.\n\n4. change some values and submit the form. page id will be changed on every submit.\n\n5. change only parameter value in url to 'user2'. Never change page-id.\n\nfor example, if you now access to http://localhost:8080/user/user1?5, change the url to http://localhost:8080/user/user2?5 .\n\n6. This program must display information about user2, because the parameter value of url is changed. But you will see the information of user 1. Wicket always display the page of page-id = 5 (even though user changed url manually).\n\nIn this sample program, I use LoadableDetachableModel for retrieving current parameter-value. But I don't get the new parameter-value because pageParameters object in a page instance is never changed after the construction. pageParameters is fixed in the constructor of Page class.\n\nI think that there are no easy way to retrieve parameter-values encoded into mount-path. Request.getRequestParameters() does not contain parameters encoded into mount-path. So there are no work-around for this issue.\n\n\n** HOW TO FIX THIS ISSUE **\n\nWe must return null from getStoredPage(int) method of PageProvider class, if current PageParameters is not same with the PageParameters of a stored page. In current code, getStoredPage(int) checks only if the class of both pages are same. We must check the PageParameters of both pages.\n\n\n** PATCH **\n\nI attached a pache for PageProvider class. try it.\n\n\n""}"
wicket,bugs-dot-jar_WICKET-4477_2624d2db,"{'BugID': 'WICKET-4477', 'Summary': 'SmartLinkLabel failing to process email with -', 'Description': 'In a similar vein to WICKET-3174 - using SmartLinkLabel with an email address that includes a ""-"" generates a link only on the right-most part of the address. \n\nExample: \n- my-test@example.com \nWill generate a link like: \n- my-<a href=""mailto:test@example.com"">test@example.com</a> \n\nThe addition of the ""-"" char is a valid email address format. '}"
wicket,bugs-dot-jar_WICKET-4483_53442bb4,"{'BugID': 'WICKET-4483', 'Summary': 'Component#setDefaultModel() should call #modelChanging()', 'Description': 'Component#setDefaultModel() should call #modelChanging() as #setDefaultModelObject() does.\nIt worked by chance so far because addStateChange() is called.\n\nhttp://markmail.org/thread/uxl6uufusggqbb6s'}"
wicket,bugs-dot-jar_WICKET-4488_e6582c52,"{'BugID': 'WICKET-4488', 'Summary': 'URL with a previous page version ignores requested page based on mount path', 'Description': 'See discussion on http://mail-archives.apache.org/mod_mbox/wicket-users/201203.mbox/browser\n\nWith 2 mounts /page1 and /page2 to stateful pages and the following sequence:\n1-With a new session, user visits ""/page1"". Displayed URL is ""/page1?0""\n2-Whatever, without expiring session\n3-User requests URL ""/page2?0"" because it was bookmarked, received via email, etc.\n4-Rendered page is ""/page1?0"" which was stored in the page map. The actual URL displayed is ""/wicket/bookmarkable/com.mycompany.Page1?0""\n\nIf a requested page id exists but does not match the page class mounted on the actual requested url, Wicket should not use the old page version. This is very counter-intuitive for users having bookmarks to stateful pages or exchanging links.'}"
wicket,bugs-dot-jar_WICKET-4494_35843c19,"{'BugID': 'WICKET-4494', 'Summary': 'HtmlHandler wrongly handles tags not requiring closed tags if the markup does not have ""top"" level tag', 'Description': 'Hi, \n\nI have custom component (extends MarkupContainer implements IMarkupCacheKeyProvider, IMarkupResourceStreamProvider) which fetches its HTML markup from database. \nFollowing HTML markup: \n\n<img alt="""" src=""logo.png""> \n<br>Some text \n<br>Some more text \n\ncauses following error: \n\n2012-04-12 10:52:53,012 [http-8080-6] ERROR: Unexpected error occurred \nUnable to find close tag for: \'<img alt=""logo"" src=""logo.png"">\' in org.apache.wicket.util.resource.StringResourceStream@3d7e16fc \n MarkupStream: [unknown] \n        at org.apache.wicket.markup.MarkupFragment.<init>(MarkupFragment.java:127) \n        at org.apache.wicket.markup.MarkupStream.getMarkupFragment(MarkupStream.java:485) \n        at org.apache.wicket.MarkupContainer.autoAdd(MarkupContainer.java:244) \n        at org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1421) \n        at org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1596) \n        at org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1571) \n        at org.apache.wicket.MarkupContainer.onComponentTagBody(MarkupContainer.java:1525) \n\nI think the problem is that org.apache.wicket.markup.parser.filter.HtmlHandler does not handle such markup correctly. It does not call ComponentTag.setHasNoCloseTag(true) for the img tag. Such call is missing in postProcess() method. I think that this problem can be fixed by inserting: \n\ntop.setHasNoCloseTag(true); \n\nafter line 80 in HtmlHandler.java file. \n\n\nMichal'}"
wicket,bugs-dot-jar_WICKET-4505_a4caaa57,"{'BugID': 'WICKET-4505', 'Summary': 'AbstractTextComponent not escaping html data by default therefore user text is not redisplayed correctly', 'Description': 'User input is not escaped in all text fields by default (and the default is not configurable).\n\nThis leads to user entered text not being redisplayed correctly.\n\n* You can replicate using the project from WICKET-3330.\n* Just enter the text my&frac12;companyname and press enter\n* The field will not redisplay the text entered properly'}"
wicket,bugs-dot-jar_WICKET-4509_b672cb2d,"{'BugID': 'WICKET-4509', 'Summary': 'Spaces in path cause ModifcationWatcher to fail', 'Description': ""The ModificationWatcher isn't able to reload resource files if there's a space in the path.\n\nThe problem is that Files#getLocalFileFromUrl(String) receives an URL encoded String in which spaces are encoded to %20. They are never decoded and passed to File(). The fix is not to use the external representation of an URL but the file representation.""}"
wicket,bugs-dot-jar_WICKET-4511_4ee5ad1f,"{'BugID': 'WICKET-4511', 'Summary': 'Stack overflow when render malformed html.', 'Description': 'Stack overflow when render malformed html.\n\nPlease, note that </HEAD> element is inserted after </body>.\n\nHTML:\n<html>\n<head>\n<body>\nMalformed HTML\n</body>\n</head>\n</html>\n\nJava:\npackage com.mycompany;\n\nimport org.apache.wicket.markup.html.WebPage;\npublic class Test1 extends WebPage {\n\tprivate static final long serialVersionUID = -4267477971499123852L;\n\n}\n\n\nThanks.'}"
wicket,bugs-dot-jar_WICKET-4518_a88882f7,"{'BugID': 'WICKET-4518', 'Summary': ""Wicket example 'forminput' is broken due to bad url for IOnChangeListener"", 'Description': ""http://localhost:8080/forminput (wicket-examples) doesn't change the locale of the labels when the locale select is changed.\nThe reason seems to be the produced url: './.?5-1.IOnChangeListener-inputForm-localeSelect' \nThis is parsed to a Url with one empty segment and thus HomePageMapper doesn't match it and doesn't handle it.""}"
wicket,bugs-dot-jar_WICKET-4519_e62ded51,"{'BugID': 'WICKET-4519', 'Summary': 'discrepancy between JavaDoc and code in MarkupContainer#visitChildren()', 'Description': 'The JavaDoc for  MarkupContainer#visitChildren() states that\n ""@param clazz The class of child to visit, or null to visit all children""\n\nThe parameter clazz is used to create a new ClassVisitFilter which in\nits visitObject() does not check for clazz == null, leading to a NPE.'}"
wicket,bugs-dot-jar_WICKET-4520_b91154ea,"{'BugID': 'WICKET-4520', 'Summary': ""Inline enclosure doesn't work if wicket:message attribute is used on the same tag"", 'Description': 'Markup like:\n\n        <div wicket:enclosure=""child"" wicket:message=""title:something"">\n\t        <div>Inner div\n\t\t        <span wicket:id=""child"">Blah</span>\n\t        </div>\n        </div>\n\ndoesn\'t work (Inner div is visible, no matter whether \'child\' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.\n'}"
wicket,bugs-dot-jar_WICKET-4520_ccb8fc9e,"{'BugID': 'WICKET-4520', 'Summary': ""Inline enclosure doesn't work if wicket:message attribute is used on the same tag"", 'Description': 'Markup like:\n\n        <div wicket:enclosure=""child"" wicket:message=""title:something"">\n\t        <div>Inner div\n\t\t        <span wicket:id=""child"">Blah</span>\n\t        </div>\n        </div>\n\ndoesn\'t work (Inner div is visible, no matter whether \'child\' is visible or not) because the auto component created for wicket:message breaks somehow wicket:enclosure.\n'}"
wicket,bugs-dot-jar_WICKET-4548_9a6a06be,"{'BugID': 'WICKET-4548', 'Summary': 'NullPointerException in org.apache.wicket.markup.html.form.ValidationErrorFeedback', 'Description': 'org.apache.wicket.markup.html.form.ValidationErrorFeedback throws a NPE in the following situation:\n\n- Form with a TextField<Integer> that has a RangeValidator\n- value outside range is entered\n- form is submitted\n\nSee attached quickstart.'}"
wicket,bugs-dot-jar_WICKET-4572_dfc56674,"{'BugID': 'WICKET-4572', 'Summary': 'DiskDataStore returns the wrong page when the page disk space is full ', 'Description': 'If the configured file size for the session data is overflowed (see org.apache.wicket.settings.IStoreSettings#setMaxSizePerSession(Bytes)) then Wicket may return wrong page data (bytes) for a expired page.\n\nThe problem is in org.apache.wicket.pageStore.PageWindowManager#idToWindowIndex which may have several page ids (the keys) pointing to the same window index (values).'}"
wicket,bugs-dot-jar_WICKET-4578_c66cf607,"{'BugID': 'WICKET-4578', 'Summary': 'Link always causes Page to become stateful, regardless of visibility', 'Description': 'Despite the changes made in WICKET-4468 , an invisible Link still causes a Page to become stateful. \n\nThe problem seems to be that Component#isStateless does this before even checking the visibility: \n\n\t\tif (!getStatelessHint())\n\t\t{\n\t\t\treturn false;\n\t\t}\n\n\n... and Link#getStatelessHint() just contains just ""return false"" . '}"
wicket,bugs-dot-jar_WICKET-4594_556a2236,"{'BugID': 'WICKET-4594', 'Summary': 'Do not use the parsed PageParameters when re-creating an expired page', 'Description': ""WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url.\nThere is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters.\nSince the execution of the behavior is ignored for the recreated page these parameters should be ignored too.""}"
wicket,bugs-dot-jar_WICKET-4594_5e1bf8d8,"{'BugID': 'WICKET-4594', 'Summary': 'Do not use the parsed PageParameters when re-creating an expired page', 'Description': ""WICKET-4014 and WICKET-4290 provided functionality to re-create an expired page if there is a mount path in the current request's url.\nThere is a minor problem with that because the page parameters are passed to the freshly created page. I.e. parameters for a callback behavior are now set as page construction parameters.\nSince the execution of the behavior is ignored for the recreated page these parameters should be ignored too.""}"
wicket,bugs-dot-jar_WICKET-4597_9dab1bb5,"{'BugID': 'WICKET-4597', 'Summary': 'bug in Duration.toString(Locale locale)', 'Description': 'Duration.toString(Locale locale) misses milliseconds in line 529'}"
wicket,bugs-dot-jar_WICKET-4610_b19a3d69,"{'BugID': 'WICKET-4610', 'Summary': 'WicketTester.assertRedirectUrl always fails because it always thinks the redirect was null', 'Description': 'I have a page which always redirects.\n\nWhen I write a test for this page, tester.assertRedirectUrl(...) always fails with the assertion failure showing that the redirect URL was null.\n\nThe page does redirect when running the application for real and I have stepped through in the debugger when running the test and it goes all the way to the HttpServletResponse.sendRedirect call.\n\nHowever, in the same debugging session, tester.getLastResponse().getRedirectLocation() == null\n\nCut-down example follows.\n\n\npublic class AlwaysRedirectPage extends WebPage\n{\n    public AlwaysRedirectPage()\n    {\n        // redirects to another web server on the same computer\n        throw new RedirectToUrlException(""http://localhost:4333/"");\n    }\n}\n\npublic class TestAlwaysRedirectPage\n{\n    @Test\n    public void test()\n    {\n        WicketTester tester = new WicketTester();\n        tester.startPage(AlwaysRedirectPage.class);\n        tester.assertRedirectUrl(""http://localhost:4333/"");\n    }\n}\n'}"
wicket,bugs-dot-jar_WICKET-4616_dd1df04b,"{'BugID': 'WICKET-4616', 'Summary': ""onError call order doesn't match onSubmit"", 'Description': 'onError in Forms and Buttons should be called in the same order as onSubmit (i.e. post-order).'}"
wicket,bugs-dot-jar_WICKET-4658_ef3adb12,"{'BugID': 'WICKET-4658', 'Summary': 'TabbedPanel CSS ""last"" is wrong if last step is not visible', 'Description': 'TabbedPanel renders a ""last"" CSS class for the last tab, this fails however if the last tab is not visible.'}"
wicket,bugs-dot-jar_WICKET-4659_ccd74641,"{'BugID': 'WICKET-4659', 'Summary': 'The default exception mapper is replying cacheable exceptional responses', 'Description': 'The problem is that some common URLs in the application like to a page instance are responding the cached exception page rather than hitting the server for the page instance being requested. It happens because at some moment in the past a exception page were replied and cached for a request in this URL.'}"
wicket,bugs-dot-jar_WICKET-4664_2fcb3417,"{'BugID': 'WICKET-4664', 'Summary': 'Url#getQueryString(charset) method returns quesrystring with ""?"" prefixed to it', 'Description': 'i have just pointed out 6.0.0-beta3/6.x but it must be same in 1.5.x too ,afaik ""?"" is not considered part of querystring ,""?"" is considered separator see http://tools.ietf.org/html/rfc3986#section-3\nthis method is used in Url#toString() too which can be easily fixed but it may be used at other places too so i don\'t know if removing ""?"" will break things now.\n\nso how things break currently\nRequestUtils.decodeParameters(url.getQueryString(),parameters);\ndecodeparameters will considered first key to be ""?key"" \nso may be requestutils#decodeparameters method should strip away ""?"" if it\'s present in the query string before populating pageparameters\n\nthanks!\n\n\n'}"
wicket,bugs-dot-jar_WICKET-4679_f3ec1503,"{'BugID': 'WICKET-4679', 'Summary': ""XmlPullParser doesn't parse correctly attributes with complex namespace"", 'Description': 'Having a markup like:\n<a class=""addthis_button_google_plusone_badge"" g:plusone:size=""smallbadge""  g:plusone:href=""https://plus.google.com/25252/""></a> causes XmlPullParser to throw the following exception:\n\njava.text.ParseException: Same attribute found twice: g:plusone (line 19, column 100)\n     at org.apache.wicket.markup.parser.XmlPullParser.parseTagText(XmlPullParser.java:673)\n     at org.apache.wicket.markup.parser.XmlPullParser.next(XmlPullParser.java:294)\n     at org.apache.wicket.markup.parser.filter.RootMarkupFilter.nextElement(RootMarkupFilter.java:58)\n.....'}"
wicket,bugs-dot-jar_WICKET-4686_89184b79,"{'BugID': 'WICKET-4686', 'Summary': 'MountMapper does not support correctly parameter placeholders', 'Description': ""Package mounting doesn't support parameter placeholders. The problem seems to be inside MountMapper which should wrap PackageMapper and take care of substituting placeholders with their actual value. \nMore precisely this class doesn't read parameter values from PageParameters and it's not very clear to me how it tries to read these values.\nDoes anybody have some hints about this class?""}"
wicket,bugs-dot-jar_WICKET-4696_f5f802c5,"{'BugID': 'WICKET-4696', 'Summary': ""NumberTextField doesn't accept values <=0 for Double and Float"", 'Description': ""The org.apache.wicket.util.lang.Numbers class defines the method :\npublic static Number getMinValue(Class<? extends Number> numberType)\n\nThis method return the MatchingNumberTypeClass.MIN_VALUE.\nBut for Double.MIN_VALUE and Float.MIN_VALUE return the smallest positive number, not the smallest negative number like for the other number classes.\n\nOne side effect is that by default you can't enter a negative value, or a 0 in a NumberTextField<Double> or NumberTextField<Float>.""}"
wicket,bugs-dot-jar_WICKET-4715_4fc82e35,"{'BugID': 'WICKET-4715', 'Summary': ""WebApplication doesn't recognize if an incoming request is multipart."", 'Description': ""Thanks to the mail at http://apache-wicket.1842946.n4.nabble.com/Read-POST-based-request-from-external-site-td4651269.html we have spotted a problem with method  newWebRequest of class WebApplication. \nIt seems that this method doesn't test if the original request is multipart and doing so post parameters go lost. \nWe should create a  MultipartServletWebRequestImpl when such a type of request is being served. I attach a possible patch but I'm not 100% about two things:\n- which is the best way to determinate if a HttpServletRequest is multipart?\n- in order to build a MultipartServletWebRequestImpl we need to provide a string identifier for the upload.   How can we generate it (in my patch it's a constant value)?\n""}"
wicket,bugs-dot-jar_WICKET-4717_6a1b2f61,"{'BugID': 'WICKET-4717', 'Summary': 'StringValidator.exactLength has wrong variable in ErrorMessage', 'Description': ""In error message for StringValidator.exactLength is variable ${exact} , but in StringValidator.decorate is added variable length to map and not exact. \n\nException when is error message interpolate for show in feedback.\n\nCaused by: java.lang.IllegalArgumentException: Value of variable [[exact]] could not be resolved while interpolating [['${label}' is not exactly ${exact} characters long.]]\n\nproperty from application.\nStringValidator.exact='${label}' is not exactly ${exact} characters long.\n\nWhen I added same property in my own properties and change exact to length, it works.""}"
wicket,bugs-dot-jar_WICKET-4738_a7ce7f91,"{'BugID': 'WICKET-4738', 'Summary': ""DownloadLink doesn't wrap the String model used for file name nor does it detach"", 'Description': ""Component DownloadLink doesn't call method wrap of class Component on parameter fileNameModel. This causes models like StringResourceModel to not resolve resource bundles correctly.\nSee the discussion here: http://stackoverflow.com/questions/12196533/how-to-use-wicket-stringresourcemodel-in-downloadlink\n\nThe patch seems quite trivial. \n\nDetachment is also missing.""}"
wicket,bugs-dot-jar_WICKET-4753_21a47387,"{'BugID': 'WICKET-4753', 'Summary': 'Resource bundles are not resolved on PriorityHeaderItems', 'Description': 'If a bundle X provides resource A, and resource A is rendered as priority header item, the resource A is rendered, not bundle X.'}"
wicket,bugs-dot-jar_WICKET-4755_87ae870f,"{'BugID': 'WICKET-4755', 'Summary': '""\'NEW VALUE\' is not a valid Serializable"" error during ajax form submission', 'Description': 'I attached a quickstart with a test in TestHomePage#formSubmitsSuccessfully.\n\nThe test throws ""\'NEW VALUE\' is not a valid Serializable"" error when ""NEW VALUE"" string in ""value"" textField is submitted as a part of myForm ajax submission.\n\nThe problem is that a call to Objects#convertValue(nonNullNonArrayValue, Object.class) will always return null if nonNullNonArrayValue is a value that is not null and not an array! Shouldn\'t it always return the first parameter when the second parameter is Object.class?\n\nSven on Wicket forum suggested to fix this as by adding another if-statement in Objects#convertValue() if (toType.isInstance(value)) {\n  result = toType.cast(value);\n}\n\nSee the following forum thread for more information http://apache-wicket.1842946.n4.nabble.com/Issues-with-default-type-conversion-in-1-5-td4651857.html'}"
wicket,bugs-dot-jar_WICKET-4757_fd910746,"{'BugID': 'WICKET-4757', 'Summary': 'FormComponents remain invalid forever if there is no feedback panel', 'Description': 'if there is no feedback panel the error messages are not removed in ondetach and form component re-validation is skipped so the form component, once marked as invalid, will remain invalid forever or at least until its error messages are rendered.\n\nthe error messages should be dropped and the form component should be re-validated on every form submit.'}"
wicket,bugs-dot-jar_WICKET-4760_2f1ece4b,"{'BugID': 'WICKET-4760', 'Summary': 'JavaScriptStripper fails with single line comments', 'Description': ""The valid input\nx++ //\nx++\n\ngets transformed to\nx++ x++\n\nwhich is syntactically invalid. This breaks the unminified version of bootstrap 2.1.1.\n\nThe problem doesn't occur with multiline comments because the linebreaks are preserved there.""}"
wicket,bugs-dot-jar_WICKET-4766_cda34428,"{'BugID': 'WICKET-4766', 'Summary': 'multiple <style> tags in header are rendered incorrectly', 'Description': 'I created a small quickstart. \nThe BasePage has some multiple <style> tags. Only he first one is rendered correctly, all following render the tag body only, the surrounding <style></style> is missing.\n'}"
wicket,bugs-dot-jar_WICKET-4775_1ac05533,"{'BugID': 'WICKET-4775', 'Summary': ""PageParameters#mergeWith may loose values of the 'other' PP"", 'Description': ""The code at org.apache.wicket.request.mapper.parameter.PageParameters#mergeWith() looks like:\n\nfor (NamedPair curNamed : other.getAllNamed())\n\t\tset(curNamed.getKey(), curNamed.getValue());\n\nmay loose some values if 'other' has a named parameter with several values.With the current code only the last name/value pair is preserved.""}"
wicket,bugs-dot-jar_WICKET-4777_eccb3b11,"{'BugID': 'WICKET-4777', 'Summary': 'JavaScriptReference escapes given URL', 'Description': '\nwhile trying to integrate gmaps3 in our webapp i had issues with the wicketstuff-gmap3 stuff ( - we need a client-id for our request) ...\n\nso i have:\n{noformat}\npublic static final String GMAP_API_URL = ""%s://maps.google.com/maps/api/js?v=3&sensor=%s&client-id=%s"";\n\nresponse.render(JavaScriptHeaderItem.forUrl(String.format(GMAP_API_URL, schema, sensor, clientid)));\n{noformat}\n\nthe rendered result of this is:\n{noformat}\n<script type=""text/javascript"" src=""http://maps.google.com/maps/api/js?v=3&amp;sensor=false&amp;client-id=....""></script>\n{noformat}\n\nso the requestparameters are encoded\n\nwhich is happening in the JavaScriptUtils Helper:\n{noformat}\npublic static void writeJavaScriptUrl(final Response response, final CharSequence url, final String id, boolean defer, String charset)\n{\n        response.write(""<script type=\\""text/javascript\\"" "");\n        if (id != null)\n        {\n            response.write(""id=\\"""" + Strings.escapeMarkup(id) + ""\\"" "");\n        }\n        if (defer)\n        {\n            response.write(""defer=\\""defer\\"" "");\n        }\n        if (charset != null)\n        {\n            response.write(""charset=\\"""" + Strings.escapeMarkup(charset) + ""\\"" "");\n        }\n        response.write(""src=\\"""");\n        response.write(Strings.escapeMarkup(url));\n        response.write(""\\""></script>"");\n        response.write(""\\n"");\n}\n{noformat}\nbut ... is this right to escape the url?\n\nwhen i open the above mentioned script, google tells me i have no parameter ""sensor"" ... which i can understand as ther is only a parameter amp ... '}"
wicket,bugs-dot-jar_WICKET-4789_6f0863f4,"{'BugID': 'WICKET-4789', 'Summary': 'URL rendering regression', 'Description': 'The way URLs are encoded was changed (WICKET-4645) and now the first request (with ;jsessionid in path) generates invalid internal links:\nMy page is mounted to ""/Home/"" and I get redirected to ""/Home/;jsessionid=1234?0"" (fine). There\'s a Link  on the page and the generated URL for it is ""../Home;jsessionid=1234?0-1.ILinkListener-link"". Note the missing ""/"". This results in a 404 and breaks basically all of my system tests.\n\nI\'ll attach a simple quickstart which demonstrates the problem. It\'s important to delete the jsessionid cookie before accessing the page.'}"
wicket,bugs-dot-jar_WICKET-4816_66bfc885,"{'BugID': 'WICKET-4816', 'Summary': 'Handling of semicolons in form action URLs', 'Description': 'What I expect to happen when there is no semicolon support in Wicket is that a\nURL in a form like below stays intact and will not be cut off at the position of\nthe first semicolon:\n\n<form action=""http://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html""\nmethod=""post""><input type=""submit"" value=""Submit"" /></form>\n\nIn my application the part abc_1234:56;023:456_def_78;90.html is ""named1"" in the\nmapping below:\n\nmount(new MountedMapper(""dor/#{named1}"", TestPage.class, new\nMyPageParametersEncoder()));\n\nand parsed in MyPageParametersEncoder.\n\nThe officially intended use of semicolons in URLs seems to be specified in ""RFC\n1808 - Relative Uniform Resource Locators, 2.4.5.""\n(http://www.faqs.org/rfcs/rfc1808.html). But that´s not what I´m looking for.\n\nIf I had not some pages running on this syntax, I could easily swap the\nsemicolon with another symbol. Nevertheless and if I´m correctly informed, I\nthink those URLs should not be cut off.\n\n(Quotation from the mailing list)\n\nThe quickstart can be tested with the following URLs:\n\nhttp://localhost:8080/dor/abc_1234:56;023:456_def_78;90.html\nhttp://localhost:8080/dor/abc_1234:56%3B023:456_def_78%3B90.html\nhttp://localhost:8080/dor/?abc=1234:56%3B023:456&def=78%3B90\n\nThe crucial part is the action attribute in the form in the page´s source code, which contains i.e. ""./abc_1234:56?-1.IFormSubmitListener-form"".'}"
wicket,bugs-dot-jar_WICKET-4824_ad849602,"{'BugID': 'WICKET-4824', 'Summary': 'Redirect to HTTPS is using wrong port 80 if HttpsConfig with default ports 80/443 is used', 'Description': ""HttpsMapper#mapHandler() doesn't set the Url's port, if the desired protocol uses the standard port.\n\nThis leads to UrlRenderer choosing to the request's port as fallback (which is 80 before switching to https).""}"
wicket,bugs-dot-jar_WICKET-4839_8b294488,"{'BugID': 'WICKET-4839', 'Summary': 'Date converters should use a new instance of DateFormat to be thread safe', 'Description': ""Please consider the linked issue WICKET-4833.\n\nI had to open a new issue because I cannot attach the quickstart project I've prepared to a closed issue.""}"
wicket,bugs-dot-jar_WICKET-4841_ce172da8,"{'BugID': 'WICKET-4841', 'Summary': 'Return error code 400 when an Ajax request has no base url set in header/request parameters.', 'Description': ""Hello,\n\ncurrently we've got a problem with faked ajax requests. these ajax \nrequests misses some parameters, but the wicket-ajax header flag is set. \nSo ServletWebRequest throws an exception:\n\njava.lang.IllegalStateException: Current ajax request is missing the base url header or parameter\n         at org.apache.wicket.util.lang.Checks.notNull(Checks.java:38)\n         at org.apache.wicket.protocol.http.servlet.ServletWebRequest.getClientUrl(ServletWebRequest.java:171)\n         at org.apache.wicket.request.UrlRenderer.<init>(UrlRenderer.java:59)\n\n\nThese faked requests are so massive, that our application is no longer \nmonitorable. Our workaround rejects these requests via apache config. \n\nInstead of logging an exception, in deployment mode wicket should log a warning and reject the request""}"
wicket,bugs-dot-jar_WICKET-4877_6470c3f7,"{'BugID': 'WICKET-4877', 'Summary': 'encodeUrl fails parsing jsessionid when using root context', 'Description': 'We are using Selenium 2.26.0 to test our Wicket application, using Jetty 6.1.25 (also tried 7.0.0.pre5) and Firefox 12 as client browser.\n\nWith Wicket 1.5.8 everything worked fine but updating to 1.5.9 the following error occurs on first request:\n\njava.lang.NumberFormatException: For input string: ""56704;jsessionid=t3j8z4tsuazh1jfbcnjr8ryg""\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Integer.parseInt(Integer.java:458)\n\tat java.lang.Integer.parseInt(Integer.java:499)\n\tat org.apache.wicket.request.Url.parse(Url.java:195)\n\tat org.apache.wicket.request.Url.parse(Url.java:121)\n\tat org.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeURL(ServletWebResponse.java:194)\n\tat org.apache.wicket.protocol.http.HeaderBufferingWebResponse.encodeURL(HeaderBufferingWebResponse.java:161)\n\tat org.apache.wicket.request.cycle.RequestCycle.renderUrl(RequestCycle.java:524)\n\tat org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:492)\n\tat org.apache.wicket.request.cycle.RequestCycle.urlFor(RequestCycle.java:477)\n\tat org.apache.wicket.Component.urlFor(Component.java:3319)\n\tat org.apache.wicket.markup.html.link.BookmarkablePageLink.getURL(BookmarkablePageLink.java:209)\n\tat org.apache.wicket.markup.html.link.Link.onComponentTag(Link.java:361)\n\tat org.apache.wicket.Component.internalRenderComponent(Component.java:2530)\n\tat org.apache.wicket.MarkupContainer.onRender(MarkupContainer.java:1530)\n\tat org.apache.wicket.Component.internalRender(Component.java:2389)\n\tat org.apache.wicket.Component.render(Component.java:2317)\n\tat org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1428)\n\tat org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1592)\n\tat org.apache.wicket.Page.onRender(Page.java:907)\n\tat org.apache.wicket.markup.html.WebPage.onRender(WebPage.java:140)\n\tat org.apache.wicket.Component.internalRender(Component.java:2389)\n\tat org.apache.wicket.Component.render(Component.java:2317)\n\tat org.apache.wicket.Page.renderPage(Page.java:1035)\n\tat org.apache.wicket.request.handler.render.WebPageRenderer.renderPage(WebPageRenderer.java:118)\n\tat org.apache.wicket.request.handler.render.WebPageRenderer.respond(WebPageRenderer.java:246)\n\tat org.apache.wicket.request.handler.RenderPageRequestHandler.respond(RenderPageRequestHandler.java:167)\n\tat org.apache.wicket.request.cycle.RequestCycle$HandlerExecutor.respond(RequestCycle.java:784)\n\tat org.apache.wicket.request.RequestHandlerStack.execute(RequestHandlerStack.java:64)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:304)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.executeExceptionRequestHandler(RequestCycle.java:313)\n\tat org.apache.wicket.request.cycle.RequestCycle.processRequest(RequestCycle.java:227)\n\tat org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:283)\n\tat org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:188)\n\tat org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:244)\n\nUsing debugger, the encodeUrl method has variables \n\nfullUrl = http://localhost:56704\nencodedFullUrl = http://localhost:56704;jsessionid=8kxeo3reannw1qjtxgkju8yiu\n\nbefore the exception occurs. I believe this is related to https://issues.apache.org/jira/browse/WICKET-4645.\n'}"
wicket,bugs-dot-jar_WICKET-4923_d78132be,"{'BugID': 'WICKET-4923', 'Summary': 'CryptoMapper ignores original queryString parameters', 'Description': 'When an AjaxRequest with parameters (e.g.: Autocomplete.getChoices()) arrives and CryptoMapper decrypts it, original queryString parameters dissapears.\n\nDebugging CryptoMapper, I\'ve checked that this method:\nprivate Url decryptUrl(final Request request, final Url encryptedUrl) {\n        ...\n}\n\nreceives querystrings parameters (on field url.parameter from ""request"" parameter) and the new Url returned by the method never adds them to its own list. '}"
wicket,bugs-dot-jar_WICKET-4927_8c827e33,"{'BugID': 'WICKET-4927', 'Summary': 'Header can not be set from IRequestCycleListener#onEndRequest()', 'Description': 'Due to HeaderBufferingWebResponse a header can no longer be set from IRequestCycleListener#onEndRequest().\n\nIn 1.4.x this was possible because BufferedWebResponse just passed through all headers to HttpServletResponse.'}"
wicket,bugs-dot-jar_WICKET-4932_f20b2d70,"{'BugID': 'WICKET-4932', 'Summary': 'Mounted page is not throwing ExpireException with setting setRecreateMountedPagesAfterExpiry(false)', 'Description': ""We have a page that is both bookmarkable (and accessible with certain page parameters) and has a second constructor taking an object. \nWhen ever the session time-out we want to show a session expired page. But we get a exception because Wicket is trying to rebuild the page with no page parameters. \nWe have set the setting getPageSettings().setRecreateMountedPagesAfterExpiry(false); This works when clicking on (ajax)links, but it's not working when using the back/forward button in the browser (or javascript:history.go(-1)).\n\nI'll attache a quickstart.""}"
wicket,bugs-dot-jar_WICKET-4988_a4a3a9a6,"{'BugID': 'WICKET-4988', 'Summary': 'AbstractNumberConverter issue when used with NumberFormat#getCurrencyInstance', 'Description': 'Summary of the discussion on users@:\n\nThere is an issue when using AbstractNumberConverter when #getNumberFormat returns NumberFormat#getCurrencyInstance()\nI think the problem is due to AbstractNumberConverter#parse(Object, double, double, Locale):\n\nif (value instanceof String)\n{\n        // Convert spaces to no-break space (U+00A0) to fix problems with\n        // browser conversions.\n        // Space is not valid thousands-separator, but no-br space is.\n        value = ((String)value).replace(\' \', \'\\u00A0\');\n}\n\nWhich replace spaces, so a string like ""1,5 €"" is invalid while being parsed.\n\npublic class CurrencyConverter extends AbstractNumberConverter<Double>\n{\n    private static final long serialVersionUID = 1L;\n\n    public CurrencyConverter()\n    {\n    }\n\n    @Override\n    protected Class<Double> getTargetType()\n    {\n        return Double.class;\n    }\n\n    @Override\n    public NumberFormat getNumberFormat(Locale locale)\n    {\n        return NumberFormat.getCurrencyInstance(locale);\n    }\n\n    @Override\n    public Double convertToObject(String value, Locale locale)\n    {\n        locale = Locale.FRANCE;\n\n        return this.parse(value, Double.MIN_VALUE, Double.MAX_VALUE, locale);\n\n//        This does work:\n//        final NumberFormat format = this.getNumberFormat(locale);\n//        return this.parse(format, value, locale);\n    }\n}\n\nAs Sven indicates, there is (yet another) issue in Java currency formating (space as thousand separator)\nhttp://matthiaswessendorf.wordpress.com/2007/12/03/javas-numberformat-bug/\nhttp://bugs.sun.com/view_bug.do?bug_id=4510618\n\nSo will I let you decide whether or not you wish to fix it (the space before the currency symbol).\n\nThanks & best regards,\nSebastien.'}"
wicket,bugs-dot-jar_WICKET-4997_ee02c883,"{'BugID': 'WICKET-4997', 'Summary': 'Mounted bookmarkable Page not recreated on Session Expiry', 'Description': ""With the default true of org.apache.wicket.settings.IPageSettings#getRecreateMountedPagesAfterExpiry() PageExpiryException is thrown when a Link on a page is clicked.\n\nI find it very useful and in fact indispensible to rely on RecreateMountedPagesAfterExpiry especially on logout links.\n\nBut it doesn't seem to work for me in 6.4.0. I think this is because Link#getUrl() calls Component#utlFor() which requires a stateless page for this to work:\n\n\t\tif (page.isPageStateless())\n\t\t{\n\t\t\thandler = new BookmarkableListenerInterfaceRequestHandler(provider, listener);\n\t\t}\n\t\telse\n\t\t{\n\t\t\thandler = new ListenerInterfaceRequestHandler(provider, listener);\n\t\t}\n\nWith a stateless page a url is:\n\nhttp://localhost:8080/wicket/HomePage?-1.ILinkListener-toolBar-signout\n\nWith a non stateless but bookmarkable page a url is:\n\nhttp://localhost:8080/wicket/page?1-1.ILinkListener-toolBar-signout\n\nSo I guess that a stateful page cannot be recovered because after session expiry Wicket cannot find out what the page is. It is not coded in the URL.\n\nLooking at the semantics of UrlFor(), I thought this might be a bug and I copied and changed the code in my Link subclass from\n\n//\t\tif (page.isPageStateless()) {\nto:\n        if (page.isBookmarkable()) {\n\t\t\nIt works as expected but I don't know whether it would break other things if implemented in Wicket.\n\nI guess I am not the only one who needs recovery for bookmarkable pages in this way. Would it be possible to change Wicket to fix this so it becomes possible?""}"
wicket,bugs-dot-jar_WICKET-5019_917dd2b5,"{'BugID': 'WICKET-5019', 'Summary': 'Handling of NO_MINIFIED_NAME in PackageResourceReference#internalGetMinifiedName()   ', 'Description': 'The Value NO_MINIFIED_NAME is not handled correctly as entry in the MINIFIED_NAMES_CACHE in PackageResourceReference#internalGetMinifiedName()   \n\n\tprivate String internalGetMinifiedName()\n\t{\n\t\tString minifiedName = MINIFIED_NAMES_CACHE.get(this);\n\t\tif (minifiedName != null && minifiedName != NO_MINIFIED_NAME)\n\t\t{                                                        ^^^^^^^\n\t\t\treturn minifiedName;\n                }\n                ...\n\nYou should remove the condition ""minifiedName != NO_MINIFIED_NAME"" here to leverage the \nMINIFIED_NAMES_CACHE for NO_MINIFIED_NAME cache entries. Otherwise you always run into the resource resolving code if there is no minified resource.\n '}"
wicket,bugs-dot-jar_WICKET-5043_2b1ce91d,"{'BugID': 'WICKET-5043', 'Summary': 'Page not mounted with WebApplication#mountPackage', 'Description': 'A bookmarkable page FormPage is mounted via WebApplication#mountPackage().\n\nIf this page is opened via IModel model; setResponsePage(new FormPage(IModel model)); then the URL is /wicket/page?0 which is not mounted.\n\nIf the page is mounted via WebApplication#mountPage() then the URL is mounted as expected.\n\nIf the page is not mounted then the users get PageExpiredException which in this case is unrecoverable.'}"
wicket,bugs-dot-jar_WICKET-5056_56169634,"{'BugID': 'WICKET-5056', 'Summary': 'Page mount with an optional named parameter overtakes a mount with more specific path', 'Description': 'See the discussion in http://markmail.org/thread/sgpiku27ah2tmcim\n\nHaving:\n  mountPage(""/all/sindex"",Page1.class);\n  mountPage(""/all/#{exp}"", Page2.class);\n\nRequest to /all/sindex will be handled by Page2.\n\nCompatibility score for optional parameters should be lower than mandatory parameters which should be lower than exact value.'}"
wicket,bugs-dot-jar_WICKET-5060_8e6a6ec5,"{'BugID': 'WICKET-5060', 'Summary': 'Fragment and Component with same id fail with misleading exception', 'Description': ""A page having a component from inherited markup *and* fragment with the *same* id fails with misleading exception message.\n\nException message:\nThe component(s) below failed to render. Possible reasons could be that: 1) you have added a component in code but forgot to reference it in the markup (thus the component will never be rendered), 2) if your components were added in a parent container then make sure the markup for the child container includes them in <wicket:extend> ... and list of component id's from fragment multiplied by amount of rows in DataTable\n\nCause: The markup of the component is used by the fragment.""}"
wicket,bugs-dot-jar_WICKET-5071_6e794ad0,"{'BugID': 'WICKET-5071', 'Summary': '404 Error on Nested ModalWindows in IE7 and IE8', 'Description': 'When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.\n\nTo replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.\n\nThe problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.'}"
wicket,bugs-dot-jar_WICKET-5071_a2f848f2,"{'BugID': 'WICKET-5071', 'Summary': '404 Error on Nested ModalWindows in IE7 and IE8', 'Description': 'When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.\n\nTo replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.\n\nThe problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.'}"
wicket,bugs-dot-jar_WICKET-5071_d3d42d42,"{'BugID': 'WICKET-5071', 'Summary': '404 Error on Nested ModalWindows in IE7 and IE8', 'Description': 'When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.\n\nTo replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.\n\nThe problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.'}"
wicket,bugs-dot-jar_WICKET-5071_faaae8d3,"{'BugID': 'WICKET-5071', 'Summary': '404 Error on Nested ModalWindows in IE7 and IE8', 'Description': 'When opening a ModalWindow inside a ModalWindow, the inner ModalWindow generates a 404 error.  Both windows use a PageCreator for content.\n\nTo replicate, you must use an actual IE 7 or IE 8 browser, as this does not replicate using developer tools and setting the document and brower to IE 7.\n\nThe problem can be seen at http://www.wicket-library.com/wicket-examples/ajax/modal-window.  I will attach a Quickstart as well.'}"
wicket,bugs-dot-jar_WICKET-5072_381b90fd,"{'BugID': 'WICKET-5072', 'Summary': 'Cookies#isEqual(Cookie, Cookie) may fail with NullPointerException', 'Description': ""If c1.getPath == null but c2.getPath != null then a NPE will occur.\nSame is valid for the 'domain' property.""}"
wicket,bugs-dot-jar_WICKET-5082_217fbb3b,"{'BugID': 'WICKET-5082', 'Summary': 'Ajax update renders parent/child JS in different order than initial Page render', 'Description': ""See attached quickstart.  On initial page load, the child Javascripts are rendered and executed first, followed by the parent's JS - in this case a Datatables.net JS. Everything works fine.\n\nHowever, if you click on a link in the DefaultDataTable, we trigger a DDT refresh via Ajax, and then you can see that the parent's JS is executed first, before the child JS - this causes a problem since the parent JS modifies the visible rows in the table and Wicket can no longer find some of the child rows.\n\nI expected the order of JS contributions to be the same for initial page render and any Ajax updates.""}"
wicket,bugs-dot-jar_WICKET-5085_581c7306,"{'BugID': 'WICKET-5085', 'Summary': 'InlineEnclosure are piling up on each render', 'Description': 'InlineEnclosureHandler#resolve() uses an auto-incremented id for its resolved InlineEnclosure, \n\nOn the next render, a new instance will be resolved, since the id of the already resolved InlineEnclosure does not match the id in the markup.\n\nBut InlineEnclosures are not removed after render as other auto-components, thus all instances pile up in the owning container of the markup.\n'}"
wicket,bugs-dot-jar_WICKET-5086_ba516f02,"{'BugID': 'WICKET-5086', 'Summary': 'FormTester throws an exception when a Palette component is added to a Form associated with a compound property model', 'Description': 'FormTester throws an exception when a Palette component is added to a Form associated with a compound property model:\norg.apache.wicket.WicketRuntimeException: No get method defined for class ... expression: choices\n\nIt worked fine in Wicket 6.5.0, and works fine if the form is not associated with a compound property model.'}"
wicket,bugs-dot-jar_WICKET-5094_74e77676,"{'BugID': 'WICKET-5094', 'Summary': 'ISecuritySettings#getEnforceMounts(true) prevents access to *all* non-mounted bookmarkable pages', 'Description': 'ISecuritySettings#setEnforceMounts(true) is meant to be used to prevent access to mounted-pages via BookmarkableMapper, e.g. when Page1.class is mounted:\n\n   http://localhost:8080/niceurl/a/nice/path/to/the/first/page\n\n... then the following url will not be accepted:\n\n   http://localhost:8080/niceurl/wicket/bookmarkable/org.apache.wicket.examples.niceurl.Page1\n\nBut starting with Wicket 1.5.x access to *all* non-mounted pages via BookmarkableMapper is prevented, i.e. no url ""http://localhost:8080/niceurl/wicket/bookmarkable/*"" is matched.'}"
wicket,bugs-dot-jar_WICKET-5102_d110e307,"{'BugID': 'WICKET-5102', 'Summary': 'wicket-bean-validation: Bean validation PropertyValidator only works with direct field access', 'Description': ""There's a substring indexing bug in the wicket-bean-validation module in org.apache.wicket.bean.validation.DefaultPropertyResolver that causes it to only work with direct field access and fail when field is missing and getter method should be used.\n\nThe problem is on this line:\n\n    String name = getter.getName().substring(3, 1).toLowerCase() + getter.getName().substring(4);\n\nWhich should be:\n\n    String name = getter.getName().substring(3, 4).toLowerCase() + getter.getName().substring(4);\n\n(or simply a single character access)""}"
wicket,bugs-dot-jar_WICKET-5112_ed780cc7,"{'BugID': 'WICKET-5112', 'Summary': 'Parantheses problem with UrlValidator', 'Description': 'One of our users got an error message when trying to add a new URL:\n\n\'http://en.wikipedia.org/wiki/Genus_(mathematics)\' is not a valid URL\n\nI just created very quickly a junit test and it fails:\n\nString[] schemes = {""http""};\nUrlValidator urlValidator = new UrlValidator(schemes);\nassertTrue(urlValidator.isValid(""http://en.wikipedia.org/wiki/Genus_(mathematics)""));'}"
wicket,bugs-dot-jar_WICKET-5114_518c933b,"{'BugID': 'WICKET-5114', 'Summary': 'Url#toString(StringMode.FULL) throws exception if a segment contains two dots', 'Description': 'When invoking toString(StringMode.FULL) for a URL like\n/mountPoint/whatever.../\nan IllegalStateException is thrown with message: Cannot render this url in FULL mode because it has a `..` segment: /mountPoint/whatever.../\n\nThe method does not actually check for `..` segments but rather checks whether path.contains("".."")\n\n'}"
wicket,bugs-dot-jar_WICKET-5131_4b7367ef,"{'BugID': 'WICKET-5131', 'Summary': 'Problems with cookies disabled when using 301/302 and also 303 (even with cookies)', 'Description': 'As mentioned in the mailing list by Martin, i open this as a bug...\n\nIts not possible to use 303 as redirect (SC_SEE_OTHER) because thats not supported, only 302 and 301 are supported but this is defined in RFC HTTP 1.1 from 1997. \n\n301 will add the Location header - which works as expected when disabling cookies. But a 302 (which is what i prefer) will redirect to the same page because the Location header is missing. When i enable cookies, its working.\n\nExample can be found here: https://github.com/olze/WicketRedirect'}"
wicket,bugs-dot-jar_WICKET-5138_e8dab4a0,"{'BugID': 'WICKET-5138', 'Summary': 'Wicket does not correctly handle http OPTIONS requests', 'Description': 'currently these requests cause regular processing (page rendering), when in fact they should have a special response.\n\nrendering the page in OPTIONS causes renderCount to be incremented and this messes with the subsequent request to the same url via a GET or POST'}"
wicket,bugs-dot-jar_WICKET-5147_184e51e9,"{'BugID': 'WICKET-5147', 'Summary': 'WicketTester MockHttpRequest.getCookies very slow / OutOfMemory', 'Description': ""\nWe have an extensive set of WicketTester tests. Recently, the wicket RELEASE in the maven repository changed to 6.7.0. After the new version, our tests got very slow.\n\nWhen profiling, I discovered that the MockHttpRequest.getCookies() was taking up a lot of time. Also, tests failed because of OutOfMemory exceptions. My guess is that somehow a lot of objects are created at such speeds that the GC cannot clean them\n\nI will investigate further, but switching back to 6.6.0 solved the issue. \n\n[Edit]\nThe tests are run with TestNG and using 'mvn test'""}"
wicket,bugs-dot-jar_WICKET-5157_961f2477,"{'BugID': 'WICKET-5157', 'Summary': 'URL query parameter values containing equals sign get cut off', 'Description': ""When calling a page with a query parameter like 'param1=val1=val2' the value of 'param1' obtained from PageParameters will be 'val1'. Everything after the equals sign inside the parameter value gets cut off.""}"
wicket,bugs-dot-jar_WICKET-5165_0d4d1df7,"{'BugID': 'WICKET-5165', 'Summary': 'Session should be bound when adding messages to it', 'Description': 'When using the Sessions info(), error() and success() methods, and the session is temporary, the messages can be dropped silently. This happens when on stateless pages and a redirect happens in the same request during which a session message is added.\n\nThe fix for this could be to make sure the session is bound and call Session#bind() automatically when a session message is added.\n\nEmail thread:\nhttp://wicket-users.markmail.org/thread/zd72s4gwnlp5d7ch'}"
wicket,bugs-dot-jar_WICKET-5176_34634266,"{'BugID': 'WICKET-5176', 'Summary': ""StringResourceModel doesn't detach model in some cases"", 'Description': 'We have come across an issue with StringResourceModel not detaching the model it holds under a certain condition.  The problem is the case where the StringResourceModel is created but it is not used - for example when it is on a tab that is not displayed.\n\nStringResourceModel is a subclass of LoadableDetachableModel and it simply implements onDetach(), letting the superclass decide whether it is attached or not. The problem is that when StringResourceModel is created, LoadableDetachableModel.attached will be false.  If the StringResourceModel is never read (i.e. getObject() is not called) the LoadableDetachableModel will not be marked as attached and when detach() is called, onDetach() will not be called.  Therefore StringResourceModel will not call detach() on the model that it holds.'}"
wicket,bugs-dot-jar_WICKET-5203_2293764f,"{'BugID': 'WICKET-5203', 'Summary': 'Base url is incorrect for error dispatched pages', 'Description': 'The fix for https://issues.apache.org/jira/browse/WICKET-4387 includes the following code in org.apache.wicket.protocol.http.servlet.ServletWebRequest#ServletWebRequest(HttpServletRequest httpServletRequest, String filterPrefix, Url url):\n\nif (forwardAttributes != null || errorAttributes != null)\n\t\t{\n\t\t\tif (LOG.isDebugEnabled())\n\t\t\t{\n\t\t\t\tLOG.debug(""Setting filterPrefix(\'{}\') to \'\' because there is either an error or a forward. {}, {}"",\n\t\t\t\t\t\tnew Object[] {filterPrefix, forwardAttributes, errorAttributes});\n\t\t\t}\n\t\t\t// the filter prefix is not needed when the current request is internal\n\t\t\t// see WICKET-4387\n\t\t\tthis.filterPrefix = """";\n\nThe filterPrefix is actually needed later when a request is made due to an error (e.g. 404):\n\npublic Url getClientUrl()\n\t{\n\t\tif (errorAttributes != null && !Strings.isEmpty(errorAttributes.getRequestUri()))\n\t\t{\n\t\t\tString problematicURI = Url.parse(errorAttributes.getRequestUri(), getCharset())\n\t\t\t\t.toString();\n\t\t\treturn getContextRelativeUrl(problematicURI, filterPrefix);\n\nWith filterPrefix=="""" the urls for any resources in the error page are wrong.\n'}"
wicket,bugs-dot-jar_WICKET-5204_9e6efa61,"{'BugID': 'WICKET-5204', 'Summary': 'The DateTimeField.onBeforeRender() method does not format the fields correctly.', 'Description': ""The current implementation relies on the org.joda.time.MutableDateTime instance to format the date, hours, amOrPm, and minutes fields. Unfortunately, the MutableDateTime constructor is not provided with the client's TimeZone value (assuming it is set). As a result, the joda library uses the JVM's default timezone. If the defaul timezone differs from the client's timezone, the formatted fields may turn out to be incorrect.""}"
wicket,bugs-dot-jar_WICKET-5209_55eb5212,"{'BugID': 'WICKET-5209', 'Summary': 'NPE when using ComponentRenderer.renderComponent on a panel with <wicket:enclosure>', 'Description': 'Hi,\n\nConsider this example:\n<wicket:panel>\n\t<wicket:enclosure child=""externalLink"">\n\t\t<a wicket:id=""externalLink"">Link</a>\n\t</wicket:enclosure>\n</wicket:panel>\n\nWhen trying to render such a panel with ComponentRenderer.renderComponent, a NPE is thrown because Wicket try to render Enclosure without initializing it.\n\nRoot cause:\njava.lang.NullPointerException\n\tat org.apache.wicket.markup.html.internal.Enclosure.isVisible(Enclosure.java:143)\n\tat org.apache.wicket.Component.determineVisibility(Component.java:4363)\n\tat org.apache.wicket.Component.internalBeforeRender(Component.java:916)\n\tat org.apache.wicket.Component.beforeRender(Component.java:991)\n\tat org.apache.wicket.Component.internalPrepareForRender(Component.java:2214)\n\tat org.apache.wicket.Component.render(Component.java:2303)\n\tat org.apache.wicket.MarkupContainer.renderNext(MarkupContainer.java:1390)\n\tat org.apache.wicket.MarkupContainer.renderAll(MarkupContainer.java:1554)\n\tat org.apache.wicket.MarkupContainer.renderComponentTagBody(MarkupContainer.java:1529)\n\tat org.apache.wicket.MarkupContainer.renderAssociatedMarkup(MarkupContainer.java:689)\n\tat org.apache.wicket.markup.html.panel.AssociatedMarkupSourcingStrategy.renderAssociatedMarkup(AssociatedMarkupSourcingStrategy.java:76)\n\tat org.apache.wicket.markup.html.panel.PanelMarkupSourcingStrategy.onComponentTagBody(PanelMarkupSourcingStrategy.java:112)\n\tat org.apache.wicket.Component.internalRenderComponent(Component.java:2549)\n\t... 29 more\n\nSee the attached quickstart.\n\nI\'ve looked a little into it, and it seems that RenderPage (used by ComponentRenderer to render components) is never initialized.\nTherefore the panel\'s children are never initialized too (see MarkupContainer l.930), and this causes Enclosure to have a null childComponent.\n\nThanks.'}"
wicket,bugs-dot-jar_WICKET-5226_8e518d88,"{'BugID': 'WICKET-5226', 'Summary': 'CDI integration fails in Glassfish 4.0 with WELD-000070', 'Description': 'When CDI is configured in the Application and a page has a non-static inner class the page throws exception, regardless of whether there are any injected fields.\n\nCaused by: org.jboss.weld.exceptions.DefinitionException: WELD-000070 Simple bean [EnhancedAnnotatedTypeImpl] private  class com.inversebit.HomePage$AForm cannot be a non-static inner class\n\tat org.jboss.weld.injection.producer.BasicInjectionTarget.checkType(BasicInjectionTarget.java:81)\n\tat org.jboss.weld.injection.producer.BasicInjectionTarget.<init>(BasicInjectionTarget.java:69)\n\tat org.jboss.weld.injection.producer.BeanInjectionTarget.<init>(BeanInjectionTarget.java:52)\n\tat org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:95)\n\tat org.jboss.weld.manager.InjectionTargetFactoryImpl.createInjectionTarget(InjectionTargetFactoryImpl.java:78)\n\t... 65 more\n'}"
wicket,bugs-dot-jar_WICKET-5230_9c8f658a,"{'BugID': 'WICKET-5230', 'Summary': 'AjaxFormChoiceComponentUpdatingBehavior fails for choices containing other invalid FormComponents', 'Description': ""If a TextField inside a RadioGroup has a ValidationError, processing of AjaxFormChoiceComponentUpdatingBehavior will erroneously update the group's model:\n\n- RadioGroup#validate() does not convert the input, because #isValid() returns false (since the nested textfield has an error message)\n- the behavior tests #hasErrorMessage() on the group, which returns false (since the group itself doesn't have an error message)\n- the behavior continues processing with a null value""}"
wicket,bugs-dot-jar_WICKET-5237_b61fe92c,"{'BugID': 'WICKET-5237', 'Summary': 'Wicket generates invalid HTML by expanding col tags', 'Description': 'hi,\n\nI just noticed that wicket expands col tags, even though the (x)html specifications forbids it.\n\ntake this markup as an example:\n\n<table>\n    <colgroup>\n        <col width=""20%"" />\n        <col width=""80%"" />\n    </colgroup>\n    <tbody>\n        <tr>\n            <td>I take a fifth of the available space</td>\n            <td>I take four fifth of the available space</td>\n        </tr>\n    </tbody>\n</table>\n\nInstead of return this as-is, it get\'s converted to:\n\n<table>\n    <colgroup>\n        <col width=""20%""></col>\n        <col width=""80%""></col>\n    </colgroup>\n    <tbody>\n        <tr>\n            <td>I take a fifth of the available space</td>\n            <td>I take four fifth of the available space</td>\n        </tr>\n    </tbody>\n</table>\n\nBut the specifications mention that col tags must not have end tags. This may be related to WICKET-2765, as this seems to be the point when col was added to the OpenCloseTagExpander class. Note that it is ok to have a non closing col tag in html (self-closing in xhtml). It\'s all about generating a separated end tag.\n\nThis happens in wicket 6.8, but I guess it\'s relevant to all versions down to wicket 1.4.\n\nSpecs for reference:\n\nhttp://www.w3.org/TR/1999/REC-html401-19991224/struct/tables.html#edef-COL\nhttp://www.w3.org/TR/html-markup/col.html\n\nKind regards,\n\nKonrad'}"
wicket,bugs-dot-jar_WICKET-5247_44a4132f,"{'BugID': 'WICKET-5247', 'Summary': 'Broken Link in Tomcat because of Page Mount', 'Description': 'I post this message on the user mailing List (http://apache-wicket.1842946.n4.nabble.com/Broken-Link-in-Tomcat-because-of-Page-Mount-tt4659663.html) and Martin Grigorov asked me, to create a ticket on Jira.\n\nBroken Link in Tomcat because of Page Mount\n\nFollowing situation:\n-I have a Wicket Application(6.8.0) which runs under the context ""webapp"" on a Tomcat 7.0.41\n-I mount a Page with two parameters (this is important) in the WicketApplication.\n\tmountPage(""/mount/${parameter1}/${parameter2}"", MountedPage.class);\n-The mounted Page(MountedPage.class) has only a simple Link\n-There are two links on the HomePage to the mounted Page.\n They are declared as follows:\n \n\tadd(new Link<Void>(""link"") {\n\t\t\t@Override\n\t\t\tpublic void onClick() {\n\t\t\t\tsetResponsePage(MountedPage.class, linkParameters);\n\t\t\t}\n\t});\n\n\tadd(new Link<Void>(""brokenLink"") {\n\t\t\t@Override\n\t\t\tpublic void onClick() {\n\t\t\t\tsetResponsePage(new MountedPage(linkParameters));\n\t\t\t}\n\t});\n\t\nI deploy this Application as a war file on a Tomcat under the context ""webapp"".\nWhen I call the first Link on the HomePage and then the Link on the mounted Page, everything works fine.\n\nBut if I call the second Link and then the Link on the mounted Page, the link is broken.\nThe context is missing in the generated link\n\thttp://localhost:8080/wicket/bookmarkable/com.mycompany.LinkedPage\n\nDoes anyone have an idea, why the second link does not work on Tomcat?\n\nI add a Quickstart and the war file as attachment.\n\nPs: Both links works fine in Jetty. \nPss:If I remove the mount command, both links will work in  Tomcat too.'}"
wicket,bugs-dot-jar_WICKET-5250_6122df49,"{'BugID': 'WICKET-5250', 'Summary': 'Minified css/js gets compressed', 'Description': 'Given an application with a resource reference to a minified script, i.e. html5.js and html5.min.js.\n\nWhen the ResourceRequestHandler responds \nit will set compress to false, if the resource reference was PackageResourceReference\nbut it will not change compression if the resource reference was JavaScriptResourceReference.\n\n\nPackageResourceReference handles minified resources more or less correctly (if they are minified, they should not be further compressed), but this behavior is overwritten in its subclasses.\n'}"
wicket,bugs-dot-jar_WICKET-5251_3d2393c7,"{'BugID': 'WICKET-5251', 'Summary': 'Minified name resolves incorrectly if default resource reference is used', 'Description': ""In PackageResourceReference.\n\nWhen a default reference to a minified resource is used (i.e. the resource wasn't mounted) the resource reference name includes '.min'. \n\nWhen trying to resolve the minified name, another '.min' is appended, resulting in the minified name resolving to 'html5.min.min.js'. \n\nAs a result, the PackageResourceReference concludes that the resource was not minified, and adds compression.""}"
wicket,bugs-dot-jar_WICKET-5251_6ce34ccf,"{'BugID': 'WICKET-5251', 'Summary': 'Minified name resolves incorrectly if default resource reference is used', 'Description': ""In PackageResourceReference.\n\nWhen a default reference to a minified resource is used (i.e. the resource wasn't mounted) the resource reference name includes '.min'. \n\nWhen trying to resolve the minified name, another '.min' is appended, resulting in the minified name resolving to 'html5.min.min.js'. \n\nAs a result, the PackageResourceReference concludes that the resource was not minified, and adds compression.""}"
wicket,bugs-dot-jar_WICKET-5259_a9e56e1e,"{'BugID': 'WICKET-5259', 'Summary': ""Url can't parse urls with username and password"", 'Description': 'Url tries to parse the password as the portnumber, because it\'s after the :, resulting in the following exception:\njava.lang.NumberFormatException: For input string: ""23dc429c-4ffa-4e99-8e24-984571f4c3b6@digdag-rest-dev2.topicusonderwijs.nl""\n\tjava.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tjava.lang.Integer.parseInt(Integer.java:492)\n\tjava.lang.Integer.parseInt(Integer.java:527)\n\torg.apache.wicket.request.Url.parse(Url.java:276)\n\torg.apache.wicket.request.Url.parse(Url.java:192)\n\torg.apache.wicket.protocol.http.servlet.ServletWebResponse.encodeRedirectURL(ServletWebResponse.java:212)\n\torg.apache.wicket.protocol.http.servlet.ServletWebResponse.sendRedirect(ServletWebResponse.java:236)\n\torg.apache.wicket.protocol.http.BufferedWebResponse$SendRedirectAction.invoke(BufferedWebResponse.java:400)\n\torg.apache.wicket.protocol.http.BufferedWebResponse.writeTo(BufferedWebResponse.java:588)\n\torg.apache.wicket.protocol.http.HeaderBufferingWebResponse.stopBuffering(HeaderBufferingWebResponse.java:60)\n\torg.apache.wicket.protocol.http.HeaderBufferingWebResponse.flush(HeaderBufferingWebResponse.java:97)\n\torg.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:269)\n\torg.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201)\n\torg.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282)\n'}"
wicket,bugs-dot-jar_WICKET-5265_0eb596df,"{'BugID': 'WICKET-5265', 'Summary': ""FencedFeedbackPanel is broken with RefreshingView(and it's implementations)"", 'Description': ""FencedFeedbackPanel doesn't work correctly if inner form(s) are in RefreshingView(or it's implementations)..\nin this case outerform feedbackpanel just starts including messages meant for inner feedbackpanel.\nwith ListView FencedFeedbackPanel works correctly..\nactually one user(Mike Dundee) created this issue in quickview https://github.com/vineetsemwal/quickview/issues/19\n so in that link he has described his problem and pasted the code you can use to reproduce ...\nthere i have also explained why it's broken with RefreshingView and it's implementations currently(it's a little complex so i am trying to avoid explaining all again ,also english is not my first language :-) ) \n\nthank you !\n""}"
wicket,bugs-dot-jar_WICKET-5319_c863b032,"{'BugID': 'WICKET-5319', 'Summary': 'CryptoMapper encrypts external URLs in ResourceReferences making the resources inaccessible', 'Description': 'Short Description: \n\nCryptoMapper encrypts links to resources with URLs of the form:\n - http://domain/path/script.js\n - /local/absolute/path/script.js\n\nAdditionally there might be some inconsistencies in handling URLs in instances of ResourceReference.\n\nThe problem occurs when JavaScript resources are included in the following way:\n\n@Override\npublic void renderHead(IHeaderResponse response)\n{\n\tsuper.renderHead(response);\n\t\n\tUrlResourceReference reference = new UrlResourceReference(Url.parse(""http://domain/path/script.js""));\n\tresponse.render(reference);\n}\n\nThe resulting JavaScript links can\'t be loaded (404 is returned) when CryptoMapper is used.\n\nThis is a minor problem, because the following always works for JavaScript files not served by Wicket (""external JavaScript files""):\n\nresponse.render(new StringHeaderItem(""<script type=\\""text/javascript\\"" src=\\""//domain/myPath/manual.js\\""></script>"");\n\n\nWays to reproduce: \n\n  A code example for wicket-examples is attached (example.zip)\n  Local URLs:\n     http://localhost:8080/enc/index\n     http://localhost:8080/unenc/index\n\n\nPossible fix: \n\n - disable encryption for URLs beginning with \'/\', \'<schema>://\' and \'//\' and not served/filtered by Wicket\n\n (\n - define different reference classes for external files and files served/filtered by Wicket, issue warnings when a wrong URL type is supplied by the user or treat URLs beginning with \'/\', \'<schema>://\' and \'//\' differently\n )\n\nThank you\n'}"
wicket,bugs-dot-jar_WICKET-5326_ded3c583,"{'BugID': 'WICKET-5326', 'Summary': ""Wicket doesn't encrypt links and Ajax URLs for mounted pages when CryptoMapper is used"", 'Description': 'URL encryption does not work in Wicket links and Ajax URLs.\n\nFor links the URL appears unencrypted in the href attribute value and is only later forwarded to the encrypted URL using a 302 response.\n\nI am uploading a quickstart.'}"
wicket,bugs-dot-jar_WICKET-5345_3fc7234e,"{'BugID': 'WICKET-5345', 'Summary': 'Url.canonical() breaks when there are two consecutive ""parent"" segments followed by a normal segment', 'Description': 'assertEquals(""a/d"", Url.parse(""a/b/c/../../d"").canonical().getPath()); \n\nbreaks with :\nExpected :a/d\nActual   :a/b/../d'}"
wicket,bugs-dot-jar_WICKET-5359_61122bab,"{'BugID': 'WICKET-5359', 'Summary': 'org.apache.wicket.util.string.StringValue#equals broken', 'Description': 'The #equals implementation for org.apache.wicket.util.string.StringValue is broken. The following throws an exception instead of just printing \'false\':\n\nStringValue val = StringValue.valueOf(""bla"", Locale.FRANCE);\nStringValue val2 = StringValue.valueOf(""bla"", Locale.CANADA);\nSystem.out.println(val.equals(val2));\n\n\nThis part of #equals\nObjects.isEqual(locale, stringValue.locale)\n\nshould probably be replaced with something like\n(locale == stringValue.locale || (locale != null && locale.equals(stringValue.locale))\n\n-> Objects.isEqual is not suitable to determine equality of Locale\n'}"
wicket,bugs-dot-jar_WICKET-5398_19e7c1cd,"{'BugID': 'WICKET-5398', 'Summary': 'XmlPullParser fails to properly parse from String with encoding declaration', 'Description': 'When parsing from a string, XmlPullParser fails if the encoding from the XML declaration is different than the system\'s file encoding.\n\nExamples:\n   -Dfile.encoding=ISO-8859-1\n   parser.parse(""<?xml encoding=\'UTF-8\' ?><span id=\'umlaut-äöü\'></span>"");\n\n   -Dfile.encoding=UTF-8\n   parser.parse(""<?xml encoding=\'ISO-8859-1\' ?><span id=\'umlaut-äöü\'></span>"");\n\nBoth fail because the string is read with the system\'s file encoding while the parser expects the stream to be encoded in the declarated encoding.'}"
wicket,bugs-dot-jar_WICKET-5400_6cefb9f8,"{'BugID': 'WICKET-5400', 'Summary': 'Behaviors#internalAdd(Behavior) erroneously gets id for stateless behaviors', 'Description': 'see http://markmail.org/thread/jtd4zn527r343jbm'}"
wicket,bugs-dot-jar_WICKET-5416_87fa630f,"{'BugID': 'WICKET-5416', 'Summary': 'BOM in UTF markup file breaks encoding detection', 'Description': 'I have project with internationalization and experienced this problem with one of the pages with non-english content. Page had UTF-8 encoding, but my JVM encoding is different. I always use ""<?xml encoding ... ?>"" to specify encoding for markup pages (and ""MarkupSettings.defaultMarkupEncoding"" is not set).\n\nUnexpectedly I got problem with bad encoding on page. After several hours of debugging I found what source of this issue was UTF BOM (Byte order mark) at the beggining of file and inability of ""XmlReader"" to process it. ""XmlReader.getXmlDeclaration"" tries to match xml declaration with regular expression, but fails because of BOM. After that encoding defaults to JVM encoding.\n\nIt\'s possible to use ""org.apache.commons.io.input.BOMInputStream"" to handle BOM or you could handle it manually inside ""XmlReader"".\n\nPS: issue found with Wicket 1.5.10 and I see same code in 6.12.0 without BOM handling, so I added it to ""Affects Version/s"", but no proof-in-code available from me at this moment.'}"
wicket,bugs-dot-jar_WICKET-5418_e350f19e,"{'BugID': 'WICKET-5418', 'Summary': 'PropertyValidator ignoring groups with the @NotNull annotation only', 'Description': 'When using groups in your JSR303 compliant classes, Wicket does not honor the groups for the @NotNull annotation.  '}"
wicket,bugs-dot-jar_WICKET-5426_fb45a781,"{'BugID': 'WICKET-5426', 'Summary': 'Page not recognized as stateless although stateful component is hidden in #onConfigure()', 'Description': 'Page#stateless gets cached. If Page#isStateless() is called before rendering, a page might not be considered stateless although in #onConfigure() all stateful components are hidden.'}"
wicket,bugs-dot-jar_WICKET-5441_8ccb1f6d,"{'BugID': 'WICKET-5441', 'Summary': 'IResourceCachingStrategy implementations should only set caching if version matches', 'Description': 'Implementations of IResourceCachingStrategy (FilenameWithVersionResourceCachingStrategy and QueryStringWithVersionResourceCachingStrategy) should only set cache duration to maximum if the version matches. Currently, if a user requests a resource with an arbitrary version, the version will be cached for one year (WebResponse.MAX_CACHE_DURATION). So people could polute proxy caches with potentially upcoming version.'}"
